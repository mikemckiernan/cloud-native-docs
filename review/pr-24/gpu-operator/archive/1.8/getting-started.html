<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Getting Started &mdash; NVIDIA Cloud Native Technologies  documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../../../_static/nvidia.ico"/>
    <link rel="canonical" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/archive/1.8/getting-started.html"/>
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/sphinx_highlight.js"></script>
        <script src="../../../_static/clipboard.min.js"></script>
        <script src="../../../_static/copybutton.js"></script>
        <script src="../../../_static/js/google-analytics/google-analytics-tracker.js"></script>
        <script src="../../../_static/js/google-analytics/google-analytics-write.js"></script>
        <script src="//assets.adobedtm.com/b92787824f2e0e9b68dc2e993f9bd995339fe417/satelliteLib-7ba51e58dc61bcb0e9311aadd02a0108ab24cc6c.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Platform Support" href="platform-support.html" />
    <link rel="prev" title="Overview" href="overview.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" > 
            <a href="../../../contents.html">
            <img src="../../../_static/NVLogo_H_B&W.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }

    .wy-nav-content {
      max-width: 1000px;
    }
  </style>
  
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">NVIDIA Container Toolkit:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/concepts.html">Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/arch-overview.html">Architecture Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/install-guide.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/user-guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/archive.html">Archive</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">NVIDIA GPU Operator:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../platform-support.html">Platform Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gpu-driver-upgrades.html">GPU Driver Upgrades</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install-gpu-operator-vgpu.html">NVIDIA vGPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install-gpu-operator-nvaie.html">NVIDIA AI Enterprise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../openshift/contents.html">GPU Operator on OpenShift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gpu-operator-mig.html">GPU Operator with MIG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gpu-sharing.html">Time-Slicing GPUs in Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gpu-operator-rdma.html">GPUDirect RDMA and GPUDirect Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gpu-operator-kubevirt.html">GPU Operator with KubeVirt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../appendix.html">Advanced Configurations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../self-validated/index.html">Self-Validated Configurations</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../archive.html">Archive</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id1">22.9.2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id2">22.9.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id3">22.9.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id4">1.11.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id5">1.11.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id6">1.10.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id7">1.9.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id8">1.9.0</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../archive.html#id9">1.8</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Kubernetes with GPUs:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../kubernetes/install-k8s.html">Install Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../kubernetes/mig-k8s.html">MIG Support in Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../kubernetes/anthos-guide.html">NVIDIA GPUs with Google Cloud’s Anthos</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Telemetry:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../gpu-telemetry/dcgm-exporter.html">DCGM-Exporter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../gpu-telemetry/dcgm-exporter.html#integrating-gpu-telemetry-into-kubernetes">Integrating GPU Telemetry into Kubernetes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multi-Instance GPU:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../mig/mig.html">Multi-Instance GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mig/mig-k8s.html">MIG Support in Kubernetes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Driver Containers:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../driver-containers/overview.html">Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Playground:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../playground/dind.html">Docker-in-Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../playground/x-arch.html">Running Cross-Architecture Containers</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../contents.html">NVIDIA Cloud Native Technologies</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../contents.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../archive.html">Archive</a> &raquo;</li>
      <li>Getting Started</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="getting-started">
<span id="operator-install-guide-1-8"></span><h1>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this heading"></a></h1>
<p>This document provides instructions, including pre-requisites for getting started with the NVIDIA GPU Operator.</p>
<hr class="docutils" />
<section id="red-hat-openshift-4">
<h2>Red Hat OpenShift 4<a class="headerlink" href="#red-hat-openshift-4" title="Permalink to this heading"></a></h2>
<p>For installing the GPU Operator on clusters with Red Hat OpenShift using RHCOS worker nodes,
follow the <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/openshift/introduction.html">user guide</a>.</p>
</section>
<hr class="docutils" />
<section id="google-cloud-anthos">
<h2>Google Cloud Anthos<a class="headerlink" href="#google-cloud-anthos" title="Permalink to this heading"></a></h2>
<p>For getting started with NVIDIA GPUs for Google Cloud Anthos, follow the getting started
<a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/kubernetes/anthos-guide.html">document</a>.</p>
</section>
<hr class="docutils" />
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this heading"></a></h2>
<p>Before installing the GPU Operator, you should ensure that the Kubernetes cluster meets some prerequisites.</p>
<ol class="arabic simple">
<li><p>Nodes must be configured with a container engine such as Docker CE/EE, <code class="docutils literal notranslate"><span class="pre">cri-o</span></code>, or <code class="docutils literal notranslate"><span class="pre">containerd</span></code>. For <strong>docker</strong>, follow the official install
<a class="reference external" href="https://docs.docker.com/engine/install/">instructions</a>.</p></li>
<li><p>If the HWE kernel (e.g. kernel 5.x) is used with Ubuntu 18.04 LTS or Ubuntu 20.04 LTS, then the <code class="docutils literal notranslate"><span class="pre">nouveau</span></code> driver for NVIDIA GPUs must be blacklisted
before starting the GPU Operator. Follow the steps in the CUDA installation <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#runfile-nouveau-ubuntu">guide</a>
to disable the nouveau driver and update <code class="docutils literal notranslate"><span class="pre">initramfs</span></code>.</p></li>
<li><p>Node Feature Discovery (NFD) is a dependency for the Operator on each node. By default, NFD master and worker are automatically deployed by the Operator.
If NFD is already running in the cluster prior to the deployment of the operator, then the Operator can be configured to not to install NFD.</p></li>
<li><p>For monitoring in Kubernetes 1.13 and 1.14, enable the kubelet <code class="docutils literal notranslate"><span class="pre">KubeletPodResources</span></code> <a class="reference external" href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature</a>
gate. From Kubernetes 1.15 onwards, its enabled by default.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To enable the <code class="docutils literal notranslate"><span class="pre">KubeletPodResources</span></code> feature gate, run the following command: <code class="docutils literal notranslate"><span class="pre">echo</span> <span class="pre">-e</span> <span class="pre">&quot;KUBELET_EXTRA_ARGS=--feature-gates=KubeletPodResources=true&quot;</span> <span class="pre">|</span> <span class="pre">sudo</span> <span class="pre">tee</span> <span class="pre">/etc/default/kubelet</span></code></p>
</div>
<p>Before installing the GPU Operator on NVIDIA vGPU, ensure the following.</p>
<ol class="arabic simple">
<li><p>The NVIDIA vGPU Host Driver version 12.0 (or later) is pre-installed on all hypervisors hosting NVIDIA vGPU accelerated Kubernetes worker node virtual machines. Please refer to <a class="reference external" href="https://docs.nvidia.com/grid/12.0/index.html">NVIDIA vGPU Documentation</a> for details.</p></li>
<li><p>A NVIDIA vGPU License Server is installed and reachable from all Kubernetes worker node virtual machines.</p></li>
<li><p>A private registry is available to upload the NVIDIA vGPU specific driver container image.</p></li>
<li><p>Each Kubernetes worker node in the cluster has access to the private registry. Private registry access is usually managed through imagePullSecrets. See the Kubernetes Documentation for more information. The user is required to provide these secrets to the NVIDIA GPU-Operator in the driver section of the values.yaml file.</p></li>
<li><p>Git and Docker/Podman are required to build the vGPU driver image from source repository and push to local registry.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Uploading the NVIDIA vGPU driver to a publicly available repository or otherwise publicly sharing the driver is a violation of the NVIDIA vGPU EULA.</p>
</div>
<p>The rest of this document includes instructions for installing the GPU Operator on supported Linux distributions.</p>
</section>
<section id="install-kubernetes">
<h2>Install Kubernetes<a class="headerlink" href="#install-kubernetes" title="Permalink to this heading"></a></h2>
<p>Refer to <a class="reference internal" href="../../../kubernetes/install-k8s.html#install-k8s"><span class="std std-ref">Install Kubernetes</span></a> for getting started with setting up a Kubernetes cluster.</p>
</section>
<section id="install-nvidia-gpu-operator">
<span id="install-gpu-operator-1-8"></span><h2>Install NVIDIA GPU Operator<a class="headerlink" href="#install-nvidia-gpu-operator" title="Permalink to this heading"></a></h2>
<section id="install-helm">
<h3>Install Helm<a class="headerlink" href="#install-helm" title="Permalink to this heading"></a></h3>
<p>The preferred method to deploy the GPU Operator is using <code class="docutils literal notranslate"><span class="pre">helm</span></code>.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>curl<span class="w"> </span>-fsSL<span class="w"> </span>-o<span class="w"> </span>get_helm.sh<span class="w"> </span>https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>chmod<span class="w"> </span><span class="m">700</span><span class="w"> </span>get_helm.sh<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>./get_helm.sh
</pre></div>
</div>
<p>Now, add the NVIDIA Helm repository:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>repo<span class="w"> </span>add<span class="w"> </span>nvidia<span class="w"> </span>https://nvidia.github.io/gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>helm<span class="w"> </span>repo<span class="w"> </span>update
</pre></div>
</div>
</section>
<section id="install-the-gpu-operator">
<h3>Install the GPU Operator<a class="headerlink" href="#install-the-gpu-operator" title="Permalink to this heading"></a></h3>
<p>The GPU Operator Helm chart offers a number of customizable options that can be configured depending on your environment.</p>
<div class="align-default"><img height="120" src="../../../_images/blockdiag-9ab121b0af6e54db8e70aef1d4d49cd964af7cd7.png" width="640" /></div>
<section id="chart-customization-options">
<h4>Chart Customization Options<a class="headerlink" href="#chart-customization-options" title="Permalink to this heading"></a></h4>
<p>The following options are available when using the Helm chart. These options can be used with <code class="docutils literal notranslate"><span class="pre">--set</span></code> when installing via Helm.</p>
<table class="colwidths-auto docutils align-center">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">nfd.enabled</span></code></p></td>
<td><p>Deploys Node Feature Discovery plugin as a daemonset.
Set this variable to <code class="docutils literal notranslate"><span class="pre">false</span></code> if NFD is already running in the cluster.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">true</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">operator.defaultRuntime</span></code></p></td>
<td><p>By default, the operator assumes your Kubernetes deployment is running with
<code class="docutils literal notranslate"><span class="pre">docker</span></code> as its container runtime. Other values are either <code class="docutils literal notranslate"><span class="pre">crio</span></code>
(for CRI-O) or <code class="docutils literal notranslate"><span class="pre">containerd</span></code> (for <strong>containerd</strong>).</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">docker</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">mig.strategy</span></code></p></td>
<td><p>Controls the strategy to be used with MIG on supported NVIDIA GPUs. Options
are either <code class="docutils literal notranslate"><span class="pre">mixed</span></code> or <code class="docutils literal notranslate"><span class="pre">single</span></code>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">single</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">psp.enabled</span></code></p></td>
<td><p>The GPU operator deploys <code class="docutils literal notranslate"><span class="pre">PodSecurityPolicies</span></code> if enabled.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">driver.enabled</span></code></p></td>
<td><p>By default, the Operator deploys NVIDIA drivers as a container on the system.
Set this value to <code class="docutils literal notranslate"><span class="pre">false</span></code> when using the Operator on systems with pre-installed drivers.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">true</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">driver.repository</span></code></p></td>
<td><p>The images are downloaded from NGC. Specify another image repository when using
custom driver images.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nvcr.io/nvidia</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">driver.version</span></code></p></td>
<td><p>Version of the NVIDIA datacenter driver supported by the Operator.</p></td>
<td><p>Depends on the version of the Operator. See the Component Matrix
for more information on supported drivers.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">driver.rdma.enabled</span></code></p></td>
<td><p>Controls whether the driver daemonset should build and load the <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> kernel module.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">toolkit.enabled</span></code></p></td>
<td><p>By default, the Operator deploys the NVIDIA Container Toolkit (<code class="docutils literal notranslate"><span class="pre">nvidia-docker2</span></code> stack)
as a container on the system. Set this value to <code class="docutils literal notranslate"><span class="pre">false</span></code> when using the Operator on systems
with pre-installed NVIDIA runtimes.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">true</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">migManager.enabled</span></code></p></td>
<td><p>The MIG manager watches for changes to the MIG geometry and applies reconfiguration as needed. By
default, the MIG manager only runs on nodes with GPUs that support MIG (for e.g. A100).</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">true</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="common-deployment-scenarios">
<h4>Common Deployment Scenarios<a class="headerlink" href="#common-deployment-scenarios" title="Permalink to this heading"></a></h4>
<p>In this section, we present some common deployment recipes when using the Helm chart to install the GPU Operator.</p>
<section id="bare-metal-passthrough-with-default-configurations-on-ubuntu">
<h5>Bare-metal/Passthrough with default configurations on Ubuntu<a class="headerlink" href="#bare-metal-passthrough-with-default-configurations-on-ubuntu" title="Permalink to this heading"></a></h5>
<p>In this scenario, the default configuration options are used:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>nvidia/gpu-operator
</pre></div>
</div>
</section>
<section id="bare-metal-passthrough-with-default-configurations-on-centos">
<h5>Bare-metal/Passthrough with default configurations on CentOS<a class="headerlink" href="#bare-metal-passthrough-with-default-configurations-on-centos" title="Permalink to this heading"></a></h5>
<p>In this scenario, the CentOS toolkit image is used:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span>--set<span class="w"> </span>toolkit.version<span class="o">=</span><span class="m">1</span>.7.1-centos7<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>nvidia/gpu-operator
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>For CentOS 8 systems, use <cite>toolkit.version=1.7.1-centos8</cite>.</p></li>
<li><p>Replace <cite>1.7.1</cite> toolkit version used here with the latest one available <a class="reference external" href="https://ngc.nvidia.com/catalog/containers/nvidia:k8s:container-toolkit/tags">here</a>.</p></li>
</ul>
</div>
</section>
<hr class="docutils" />
<section id="nvidia-vgpu">
<h5>NVIDIA vGPU<a class="headerlink" href="#nvidia-vgpu" title="Permalink to this heading"></a></h5>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The GPU Operator with NVIDIA vGPUs requires additional steps to build a private driver image prior to install.
Refer to the document <a class="reference internal" href="install-gpu-operator-vgpu.html#install-gpu-operator-1-8-vgpu"><span class="std std-ref">NVIDIA vGPU</span></a> for detailed instructions on the workflow and required values of
the variables used in this command.</p>
</div>
<p>The command below will install the GPU Operator with its default configuration for vGPU:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>nvidia/gpu-operator<span class="w"> </span>--set<span class="w"> </span>driver.repository<span class="o">=</span><span class="nv">$PRIVATE_REGISTRY</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>driver.version<span class="o">=</span><span class="nv">$VERSION</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>driver.imagePullSecrets<span class="o">={</span><span class="nv">$REGISTRY_SECRET_NAME</span><span class="o">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>driver.licensingConfig.configMapName<span class="o">=</span>licensing-config
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="nvidia-ai-enterprise">
<h5>NVIDIA AI Enterprise<a class="headerlink" href="#nvidia-ai-enterprise" title="Permalink to this heading"></a></h5>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The GPU Operator with NVIDIA AI Enterprise requires some tasks to be completed
prior to installation. Refer to the document <a class="reference internal" href="install-gpu-operator-nvaie.html#install-gpu-operator-1-8-nvaie"><span class="std std-ref">NVIDIA AI Enterprise</span></a> for instructions
prior to running the below commands.</p>
</div>
<p>Add the NVIDIA AI Enterprise Helm repository, where <code class="docutils literal notranslate"><span class="pre">api-key</span></code> is the NGC API key for accessing
the NVIDIA Enterprise Collection that you generated:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>repo<span class="w"> </span>add<span class="w"> </span>nvaie<span class="w"> </span>https://helm.ngc.nvidia.com/nvaie<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--username<span class="o">=</span><span class="s1">&#39;$oauthtoken&#39;</span><span class="w"> </span>--password<span class="o">=</span>api-key<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">&amp;&amp;</span><span class="w"> </span>helm<span class="w"> </span>repo<span class="w"> </span>update
</pre></div>
</div>
<p>Install the NVIDIA GPU Operator:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span>nvaie/gpu-operator<span class="w"> </span>-n<span class="w"> </span>gpu-operator-resources
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="bare-metal-passthrough-with-pre-installed-nvidia-drivers">
<h5>Bare-metal/Passthrough with pre-installed NVIDIA drivers<a class="headerlink" href="#bare-metal-passthrough-with-pre-installed-nvidia-drivers" title="Permalink to this heading"></a></h5>
<p>In this example, the user has already pre-installed NVIDIA drivers as part of the system image:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>nvidia/gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>driver.enabled<span class="o">=</span><span class="nb">false</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="bare-metal-passthrough-with-pre-installed-drivers-and-nvidia-container-toolkit">
<h5>Bare-metal/Passthrough with pre-installed drivers and NVIDIA Container Toolkit<a class="headerlink" href="#bare-metal-passthrough-with-pre-installed-drivers-and-nvidia-container-toolkit" title="Permalink to this heading"></a></h5>
<p>In this example, the user has already pre-installed the NVIDIA drivers and NVIDIA Container Toolkit (<code class="docutils literal notranslate"><span class="pre">nvidia-docker2</span></code>)
as part of the system image.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>These steps should be followed when using the GPU Operator v1.8+ on DGX systems such as DGX A100.</p>
</div>
<p>Before installing the operator, ensure that the following configurations are modified depending on the container runtime configured in your cluster.</p>
<p>Docker:</p>
<blockquote>
<div><ul>
<li><p>Update the Docker configuration to add <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> as the default runtime. The <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> runtime should
be setup as the default container runtime for Docker on GPU nodes. This can be done by adding the
<code class="docutils literal notranslate"><span class="pre">default-runtime</span></code> line into the Docker daemon config file, which is usually located on the system
at <code class="docutils literal notranslate"><span class="pre">/etc/docker/daemon.json</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go">    &quot;default-runtime&quot;: &quot;nvidia&quot;,</span>
<span class="go">    &quot;runtimes&quot;: {</span>
<span class="go">        &quot;nvidia&quot;: {</span>
<span class="go">            &quot;path&quot;: &quot;/usr/bin/nvidia-container-runtime&quot;,</span>
<span class="go">            &quot;runtimeArgs&quot;: []</span>
<span class="go">      }</span>
<span class="go">    }</span>
<span class="go">}</span>
</pre></div>
</div>
<p>Restart the Docker daemon to complete the installation after setting the default runtime:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>docker
</pre></div>
</div>
</li>
</ul>
</div></blockquote>
<p>Containerd:</p>
<blockquote>
<div><ul>
<li><p>Update <code class="docutils literal notranslate"><span class="pre">containerd</span></code> to use <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> as the default runtime and add <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> runtime configuration.
This can be done by adding below config to <code class="docutils literal notranslate"><span class="pre">/etc/containerd/config.toml</span></code> and restarting <code class="docutils literal notranslate"><span class="pre">containerd</span></code> service.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">version = 2</span>
<span class="go">[plugins]</span>
<span class="go">  [plugins.&quot;io.containerd.grpc.v1.cri&quot;]</span>
<span class="go">    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd]</span>
<span class="go">      default_runtime_name = &quot;nvidia&quot;</span>

<span class="go">      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes]</span>
<span class="go">        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.nvidia]</span>
<span class="go">          privileged_without_host_devices = false</span>
<span class="go">          runtime_engine = &quot;&quot;</span>
<span class="go">          runtime_root = &quot;&quot;</span>
<span class="go">          runtime_type = &quot;io.containerd.runc.v2&quot;</span>
<span class="go">          [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.nvidia.options]</span>
<span class="go">            BinaryName = &quot;/usr/bin/nvidia-container-runtime&quot;</span>
</pre></div>
</div>
<p>Restart the Containerd daemon to complete the installation after setting the default runtime:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>containerd
</pre></div>
</div>
</li>
</ul>
</div></blockquote>
<p>Install the GPU operator with the following options:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>nvidia/gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--set<span class="w"> </span>driver.enabled<span class="o">=</span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--set<span class="w"> </span>toolkit.enabled<span class="o">=</span><span class="nb">false</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="bare-metal-passthrough-with-pre-installed-nvidia-container-toolkit-but-no-drivers">
<h5>Bare-metal/Passthrough with pre-installed NVIDIA Container Toolkit (but no drivers)<a class="headerlink" href="#bare-metal-passthrough-with-pre-installed-nvidia-container-toolkit-but-no-drivers" title="Permalink to this heading"></a></h5>
<p>In this example, the user has already pre-installed the NVIDIA Container Toolkit (<code class="docutils literal notranslate"><span class="pre">nvidia-docker2</span></code>) as part of the system image.</p>
<p>Before installing the operator, ensure that the following configurations are modified depending on the container runtime configured in your cluster.</p>
<p>Docker:</p>
<blockquote>
<div><ul>
<li><p>Update the Docker configuration to add <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> as the default runtime. The <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> runtime should
be setup as the default container runtime for Docker on GPU nodes. This can be done by adding the
<code class="docutils literal notranslate"><span class="pre">default-runtime</span></code> line into the Docker daemon config file, which is usually located on the system
at <code class="docutils literal notranslate"><span class="pre">/etc/docker/daemon.json</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go">    &quot;default-runtime&quot;: &quot;nvidia&quot;,</span>
<span class="go">    &quot;runtimes&quot;: {</span>
<span class="go">        &quot;nvidia&quot;: {</span>
<span class="go">            &quot;path&quot;: &quot;/usr/bin/nvidia-container-runtime&quot;,</span>
<span class="go">            &quot;runtimeArgs&quot;: []</span>
<span class="go">      }</span>
<span class="go">    }</span>
<span class="go">}</span>
</pre></div>
</div>
<p>Restart the Docker daemon to complete the installation after setting the default runtime:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>docker
</pre></div>
</div>
</li>
</ul>
</div></blockquote>
<p>Containerd:</p>
<blockquote>
<div><ul>
<li><p>Update <code class="docutils literal notranslate"><span class="pre">containerd</span></code> to use <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> as the default runtime and add <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> runtime configuration.
This can be done by adding below config to <code class="docutils literal notranslate"><span class="pre">/etc/containerd/config.toml</span></code> and restarting <code class="docutils literal notranslate"><span class="pre">containerd</span></code> service.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">version = 2</span>
<span class="go">[plugins]</span>
<span class="go">  [plugins.&quot;io.containerd.grpc.v1.cri&quot;]</span>
<span class="go">    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd]</span>
<span class="go">      default_runtime_name = &quot;nvidia&quot;</span>

<span class="go">      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes]</span>
<span class="go">        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.nvidia]</span>
<span class="go">          privileged_without_host_devices = false</span>
<span class="go">          runtime_engine = &quot;&quot;</span>
<span class="go">          runtime_root = &quot;&quot;</span>
<span class="go">          runtime_type = &quot;io.containerd.runc.v2&quot;</span>
<span class="go">          [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.nvidia.options]</span>
<span class="go">            BinaryName = &quot;/usr/bin/nvidia-container-runtime&quot;</span>
</pre></div>
</div>
<p>Restart the Containerd daemon to complete the installation after setting the default runtime:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>containerd
</pre></div>
</div>
</li>
</ul>
</div></blockquote>
<p>Configure toolkit to use the <code class="docutils literal notranslate"><span class="pre">root</span></code> directory of the driver installation as <code class="docutils literal notranslate"><span class="pre">/run/nvidia/driver</span></code>, which is the path mounted by driver container.</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>sed<span class="w"> </span>-i<span class="w"> </span><span class="s1">&#39;s/^#root/root/&#39;</span><span class="w"> </span>/etc/nvidia-container-runtime/config.toml
</pre></div>
</div>
</div></blockquote>
<p>Once these steps are complete, now install the GPU operator with the following options (which will provision a driver):</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>nvidia/gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>toolkit.enabled<span class="o">=</span><span class="nb">false</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="custom-driver-image-based-off-a-specific-driver-version">
<h5>Custom driver image (based off a specific driver version)<a class="headerlink" href="#custom-driver-image-based-off-a-specific-driver-version" title="Permalink to this heading"></a></h5>
<p>If you want to use custom driver container images (for e.g. using 465.27), then
you would need to build a new driver container image. Follow these steps:</p>
<ul>
<li><p>Rebuild the driver container by specifying the <code class="docutils literal notranslate"><span class="pre">$DRIVER_VERSION</span></code> argument when building the Docker image. For
reference, the driver container Dockerfiles are available on the Git repo <a class="reference external" href="https://gitlab.com/nvidia/container-images/driver">here</a></p></li>
<li><p>Build the container using the appropriate Dockerfile. For example:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>build<span class="w"> </span>--pull<span class="w"> </span>-t<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--build-arg<span class="w"> </span><span class="nv">DRIVER_VERSION</span><span class="o">=</span><span class="m">455</span>.28<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>nvidia/driver:455.28-ubuntu20.04<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--file<span class="w"> </span>Dockerfile<span class="w"> </span>.
</pre></div>
</div>
<p>Ensure that the driver container is tagged as shown in the example by using the <code class="docutils literal notranslate"><span class="pre">driver:&lt;version&gt;-&lt;os&gt;</span></code> schema.</p>
</li>
<li><p>Specify the new driver image and repository by overriding the defaults in
the Helm install command. For example:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>nvidia/gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>driver.repository<span class="o">=</span>docker.io/nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>driver.version<span class="o">=</span><span class="s2">&quot;465.27&quot;</span>
</pre></div>
</div>
</li>
</ul>
<p>Note that these instructions are provided for reference and evaluation purposes.
Not using the standard releases of the GPU Operator from NVIDIA would mean limited
support for such custom configurations.</p>
</section>
<hr class="docutils" />
<section id="set-the-default-container-runtime-as-containerd">
<h5>Set the default container runtime as <code class="docutils literal notranslate"><span class="pre">containerd</span></code><a class="headerlink" href="#set-the-default-container-runtime-as-containerd" title="Permalink to this heading"></a></h5>
<p>In this example, we set the default container runtime to be used as <code class="docutils literal notranslate"><span class="pre">containerd</span></code>.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>nvidia/gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>operator.defaultRuntime<span class="o">=</span>containerd
</pre></div>
</div>
<p>When setting <cite>containerd</cite> as the <cite>defaultRuntime</cite> the following
options are also available:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">toolkit</span><span class="p">:</span>
<span class="w">   </span><span class="nt">env</span><span class="p">:</span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CONTAINERD_CONFIG</span>
<span class="w">   </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/etc/containerd/config.toml</span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CONTAINERD_SOCKET</span>
<span class="w">   </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/run/containerd/containerd.sock</span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CONTAINERD_RUNTIME_CLASS</span>
<span class="w">   </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia</span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CONTAINERD_SET_AS_DEFAULT</span>
<span class="w">   </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
<p>These options are defined as follows:</p>
<blockquote>
<div><ul>
<li><dl>
<dt><strong>CONTAINERD_CONFIG</strong><span class="classifier">The path on the host to the <code class="docutils literal notranslate"><span class="pre">containerd</span></code> config</span></dt><dd><p>you would like to have updated with support for the <code class="docutils literal notranslate"><span class="pre">nvidia-container-runtime</span></code>.
By default this will point to <code class="docutils literal notranslate"><span class="pre">/etc/containerd/config.toml</span></code> (the default
location for <code class="docutils literal notranslate"><span class="pre">containerd</span></code>). It should be customized if your <code class="docutils literal notranslate"><span class="pre">containerd</span></code>
installation is not in the default location.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>CONTAINERD_SOCKET</strong><span class="classifier">The path on the host to the socket file used to</span></dt><dd><p>communicate with <code class="docutils literal notranslate"><span class="pre">containerd</span></code>. The operator will use this to send a
<code class="docutils literal notranslate"><span class="pre">SIGHUP</span></code> signal to the <code class="docutils literal notranslate"><span class="pre">containerd</span></code> daemon to reload its config. By
default this will point to <code class="docutils literal notranslate"><span class="pre">/run/containerd/containerd.sock</span></code>
(the default location for <code class="docutils literal notranslate"><span class="pre">containerd</span></code>). It should be customized if
your <code class="docutils literal notranslate"><span class="pre">containerd</span></code> installation is not in the default location.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>CONTAINERD_RUNTIME_CLASS</strong><span class="classifier">The name of the</span></dt><dd><p><a class="reference external" href="https://kubernetes.io/docs/concepts/containers/runtime-class">Runtime Class</a>
you would like to associate with the <code class="docutils literal notranslate"><span class="pre">nvidia-container-runtime</span></code>.
Pods launched with a <code class="docutils literal notranslate"><span class="pre">runtimeClassName</span></code> equal to CONTAINERD_RUNTIME_CLASS
will always run with the <code class="docutils literal notranslate"><span class="pre">nvidia-container-runtime</span></code>. The default
CONTAINERD_RUNTIME_CLASS is <code class="docutils literal notranslate"><span class="pre">nvidia</span></code>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>CONTAINERD_SET_AS_DEFAULT</strong><span class="classifier">A flag indicating whether you want to set</span></dt><dd><p><code class="docutils literal notranslate"><span class="pre">nvidia-container-runtime</span></code> as the default runtime used to launch all
containers. When set to false, only containers in pods with a <code class="docutils literal notranslate"><span class="pre">runtimeClassName</span></code>
equal to CONTAINERD_RUNTIME_CLASS will be run with the <code class="docutils literal notranslate"><span class="pre">nvidia-container-runtime</span></code>.
The default value is <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</section>
<hr class="docutils" />
<section id="proxy-environments">
<h5>Proxy Environments<a class="headerlink" href="#proxy-environments" title="Permalink to this heading"></a></h5>
<p>Refer to the section <a class="reference internal" href="install-gpu-operator-proxy.html#install-gpu-operator-1-8-proxy"><span class="std std-ref">Install GPU Operator in Proxy Environments</span></a> for more information on how to install the Operator on clusters
behind a HTTP proxy.</p>
</section>
<hr class="docutils" />
<section id="air-gapped-environments">
<h5>Air-gapped Environments<a class="headerlink" href="#air-gapped-environments" title="Permalink to this heading"></a></h5>
<p>Refer to the section <a class="reference internal" href="install-gpu-operator-air-gapped.html#install-gpu-operator-1-8-air-gapped"><span class="std std-ref">Install GPU Operator in Air-gapped Environments</span></a> for more information on how to install the Operator
in air-gapped environments.</p>
</section>
<hr class="docutils" />
<section id="multi-instance-gpu-mig">
<h5>Multi-Instance GPU (MIG)<a class="headerlink" href="#multi-instance-gpu-mig" title="Permalink to this heading"></a></h5>
<p>Refer to the document <a class="reference internal" href="gpu-operator-mig.html#install-gpu-operator-1-8-mig"><span class="std std-ref">GPU Operator with MIG</span></a> for more information on how use the Operator with Multi-Instance GPU (MIG)
on NVIDIA Ampere products. For guidance on configuring MIG support for the <strong>NVIDIA GPU Operator</strong> in an OpenShift Container Platform cluster, see the <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/openshift/mig-ocp.html">user guide</a>.</p>
</section>
<hr class="docutils" />
<section id="outdated-kernels">
<h5>Outdated Kernels<a class="headerlink" href="#outdated-kernels" title="Permalink to this heading"></a></h5>
<p>Refer to the section <a class="reference internal" href="install-gpu-operator-outdated-kernels.html#install-gpu-operator-1-8-outdated-kernels"><span class="std std-ref">Considerations when Installing with Outdated Kernels in Cluster</span></a> for more information on how to install the Operator successfully
when nodes in the cluster are not running the latest kernel</p>
</section>
</section>
<hr class="docutils" />
<section id="verify-gpu-operator-install">
<h4>Verify GPU Operator Install<a class="headerlink" href="#verify-gpu-operator-install" title="Permalink to this heading"></a></h4>
<p>Once the Helm chart is installed, check the status of the pods to ensure all the containers are running and the validation is complete:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-A
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">NAMESPACE                NAME                                                          READY   STATUS      RESTARTS   AGE</span>
<span class="go">default                  gpu-operator-d6ccd4d8d-f7m57                                  1/1     Running     0          5m51s</span>
<span class="go">default                  gpu-operator-node-feature-discovery-master-867c4f7bfb-cbxck   1/1     Running     0          5m51s</span>
<span class="go">default                  gpu-operator-node-feature-discovery-worker-wv2rq              1/1     Running     0          5m51s</span>
<span class="go">gpu-operator-resources   gpu-feature-discovery-qmftl                                   1/1     Running     0          5m35s</span>
<span class="go">gpu-operator-resources   nvidia-container-toolkit-daemonset-tx4rd                      1/1     Running     0          5m35s</span>
<span class="go">gpu-operator-resources   nvidia-cuda-validator-ip-172-31-65-3                          0/1     Completed   0          2m29s</span>
<span class="go">gpu-operator-resources   nvidia-dcgm-exporter-99t8p                                    1/1     Running     0          5m35s</span>
<span class="go">gpu-operator-resources   nvidia-device-plugin-daemonset-nkbtz                          1/1     Running     0          5m35s</span>
<span class="go">gpu-operator-resources   nvidia-device-plugin-validator-ip-172-31-65-3                 0/1     Completed   0          103s</span>
<span class="go">gpu-operator-resources   nvidia-driver-daemonset-w97sh                                 1/1     Running     0          5m35s</span>
<span class="go">gpu-operator-resources   nvidia-operator-validator-2djn2                               1/1     Running     0          5m35s</span>
<span class="go">kube-system              calico-kube-controllers-b656ddcfc-4sgld                       1/1     Running     0          8m11s</span>
<span class="go">kube-system              calico-node-wzdbr                                             1/1     Running     0          8m11s</span>
<span class="go">kube-system              coredns-558bd4d5db-2w9tf                                      1/1     Running     0          8m11s</span>
<span class="go">kube-system              coredns-558bd4d5db-cv5md                                      1/1     Running     0          8m11s</span>
<span class="go">kube-system              etcd-ip-172-31-65-3                                           1/1     Running     0          8m25s</span>
<span class="go">kube-system              kube-apiserver-ip-172-31-65-3                                 1/1     Running     0          8m25s</span>
<span class="go">kube-system              kube-controller-manager-ip-172-31-65-3                        1/1     Running     0          8m25s</span>
<span class="go">kube-system              kube-proxy-gpqc5                                              1/1     Running     0          8m11s</span>
<span class="go">kube-system              kube-scheduler-ip-172-31-65-3                                 1/1     Running     0          8m25s</span>
</pre></div>
</div>
<p>We can now proceed to running some sample GPU workloads to verify that the Operator (and its components) are working correctly.</p>
</section>
</section>
</section>
<section id="running-sample-gpu-applications">
<h2>Running Sample GPU Applications<a class="headerlink" href="#running-sample-gpu-applications" title="Permalink to this heading"></a></h2>
<section id="cuda-vectoradd">
<h3>CUDA VectorAdd<a class="headerlink" href="#cuda-vectoradd" title="Permalink to this heading"></a></h3>
<p>In the first example, let’s run a simple CUDA sample, which adds two vectors together:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat<span class="w"> </span>&lt;&lt;<span class="w"> </span>EOF<span class="w"> </span><span class="p">|</span><span class="w"> </span>kubectl<span class="w"> </span>create<span class="w"> </span>-f<span class="w"> </span>-
<span class="go">apiVersion: v1</span>
<span class="go">kind: Pod</span>
<span class="go">metadata:</span>
<span class="go">  name: cuda-vectoradd</span>
<span class="go">spec:</span>
<span class="go">  restartPolicy: OnFailure</span>
<span class="go">  containers:</span>
<span class="go">  - name: cuda-vectoradd</span>
<span class="go">    image: &quot;nvidia/samples:vectoradd-cuda11.2.1&quot;</span>
<span class="go">    resources:</span>
<span class="go">      limits:</span>
<span class="go">         nvidia.com/gpu: 1</span>
<span class="go">EOF</span>
</pre></div>
</div>
<p>The sample should run fairly quickly. If you view the logs of the container:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">[Vector addition of 50000 elements]</span>
<span class="go">Copy input data from the host memory to the CUDA device</span>
<span class="go">CUDA kernel launch with 196 blocks of 256 threads</span>
<span class="go">Copy output data from the CUDA device to the host memory</span>
<span class="go">Test PASSED</span>
<span class="go">Done</span>
</pre></div>
</div>
</section>
<section id="cuda-fp16-matrix-multiply">
<h3>CUDA FP16 Matrix multiply<a class="headerlink" href="#cuda-fp16-matrix-multiply" title="Permalink to this heading"></a></h3>
<p>In the second example, let’s try running a CUDA load generator, which does an FP16 matrix-multiply on the GPU using
the Tensor Cores when available:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat<span class="w"> </span>&lt;&lt;<span class="w"> </span>EOF<span class="w"> </span><span class="p">|</span><span class="w"> </span>kubectl<span class="w"> </span>create<span class="w"> </span>-f<span class="w"> </span>-
<span class="go">apiVersion: v1</span>
<span class="go">kind: Pod</span>
<span class="go">metadata:</span>
<span class="go">   name: dcgmproftester</span>
<span class="go">spec:</span>
<span class="go">   restartPolicy: OnFailure</span>
<span class="go">   containers:</span>
<span class="go">   - name: dcgmproftester11</span>
<span class="go">     image: nvidia/samples:dcgmproftester-2.0.10-cuda11.0-ubuntu18.04</span>
<span class="go">     args: [&quot;--no-dcgm-validation&quot;, &quot;-t 1004&quot;, &quot;-d 30&quot;]</span>
<span class="go">     resources:</span>
<span class="go">       limits:</span>
<span class="go">          nvidia.com/gpu: 1</span>
<span class="go">     securityContext:</span>
<span class="go">       capabilities:</span>
<span class="go">         add: [&quot;SYS_ADMIN&quot;]</span>

<span class="go">EOF</span>
</pre></div>
</div>
<p>and then view the logs of the <code class="docutils literal notranslate"><span class="pre">dcgmproftester</span></code> pod:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>logs<span class="w"> </span>-f<span class="w"> </span>dcgmproftester
</pre></div>
</div>
<p>You should see the FP16 GEMM being run on the GPU:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">Skipping CreateDcgmGroups() since DCGM validation is disabled</span>
<span class="go">CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_MULTIPROCESSOR: 1024</span>
<span class="go">CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT: 40</span>
<span class="go">CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR: 65536</span>
<span class="go">CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR: 7</span>
<span class="go">CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR: 5</span>
<span class="go">CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH: 256</span>
<span class="go">CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE: 5001000</span>
<span class="go">Max Memory bandwidth: 320064000000 bytes (320.06 GiB)</span>
<span class="go">CudaInit completed successfully.</span>

<span class="go">Skipping WatchFields() since DCGM validation is disabled</span>
<span class="go">TensorEngineActive: generated ???, dcgm 0.000 (26096.4 gflops)</span>
<span class="go">TensorEngineActive: generated ???, dcgm 0.000 (26344.4 gflops)</span>
<span class="go">TensorEngineActive: generated ???, dcgm 0.000 (26351.2 gflops)</span>
<span class="go">TensorEngineActive: generated ???, dcgm 0.000 (26359.9 gflops)</span>
<span class="go">TensorEngineActive: generated ???, dcgm 0.000 (26750.7 gflops)</span>
<span class="go">TensorEngineActive: generated ???, dcgm 0.000 (25378.8 gflops)</span>
</pre></div>
</div>
<p>You will observe that on an NVIDIA T4, this has resulted in ~26 TFLOPS of FP16 GEMM performance.</p>
</section>
<section id="jupyter-notebook">
<h3>Jupyter Notebook<a class="headerlink" href="#jupyter-notebook" title="Permalink to this heading"></a></h3>
<p>In the next example, let’s try running a TensorFlow Jupyter notebook.</p>
<p>First, deploy the pods:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>https://nvidia.github.io/gpu-operator/notebook-example.yml
</pre></div>
</div>
<p>Check to determine if the pod has successfully started:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>pod<span class="w"> </span>tf-notebook
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">NAMESPACE                NAME                                                              READY   STATUS      RESTARTS   AGE</span>
<span class="go">default                  tf-notebook                                                       1/1     Running     0          3m45s</span>
</pre></div>
</div>
<p>Since the example also includes a service, let’s obtain the external port at which the notebook is accessible:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>svc<span class="w"> </span>-A
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">NAMESPACE                NAME                                                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE</span>
<span class="go">default                  tf-notebook                                             NodePort    10.106.229.20   &lt;none&gt;        80:30001/TCP             4m41s</span>
<span class="go">..</span>
</pre></div>
</div>
<p>And the token for the Jupyter notebook:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>logs<span class="w"> </span>tf-notebook
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">[I 21:50:23.188 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret</span>
<span class="go">[I 21:50:23.390 NotebookApp] Serving notebooks from local directory: /tf</span>
<span class="go">[I 21:50:23.391 NotebookApp] The Jupyter Notebook is running at:</span>
<span class="go">[I 21:50:23.391 NotebookApp] http://tf-notebook:8888/?token=3660c9ee9b225458faaf853200bc512ff2206f635ab2b1d9</span>
<span class="go">[I 21:50:23.391 NotebookApp]  or http://127.0.0.1:8888/?token=3660c9ee9b225458faaf853200bc512ff2206f635ab2b1d9</span>
<span class="go">[I 21:50:23.391 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).</span>
<span class="go">[C 21:50:23.394 NotebookApp]</span>

<span class="go">   To access the notebook, open this file in a browser:</span>
<span class="go">      file:///root/.local/share/jupyter/runtime/nbserver-1-open.html</span>
<span class="go">   Or copy and paste one of these URLs:</span>
<span class="go">      http://tf-notebook:8888/?token=3660c9ee9b225458faaf853200bc512ff2206f635ab2b1d9</span>
<span class="go">   or http://127.0.0.1:8888/?token=3660c9ee9b225458faaf853200bc512ff2206f635ab2b1d9</span>
</pre></div>
</div>
<p>The notebook should now be accessible from your browser at this URL: <code class="docutils literal notranslate"><span class="pre">http:://&lt;your-machine-ip&gt;:30001/?token=3660c9ee9b225458faaf853200bc512ff2206f635ab2b1d9</span></code></p>
</section>
</section>
<section id="demo">
<h2>Demo<a class="headerlink" href="#demo" title="Permalink to this heading"></a></h2>
<p>Check out the demo below where we scale GPU nodes in a K8s cluster using the GPU Operator:</p>
<a class="reference internal image-reference" href="../../../_images/gpu-operator-demo3.gif"><img alt="../../../_images/gpu-operator-demo3.gif" src="../../../_images/gpu-operator-demo3.gif" style="width: 1440px;" /></a>
</section>
<section id="gpu-telemetry">
<h2>GPU Telemetry<a class="headerlink" href="#gpu-telemetry" title="Permalink to this heading"></a></h2>
<p>To gather GPU telemetry in Kubernetes, the GPU Operator deploys the <code class="docutils literal notranslate"><span class="pre">dcgm-exporter</span></code>. <code class="docutils literal notranslate"><span class="pre">dcgm-exporter</span></code>, based
on <a class="reference external" href="https://developer.nvidia.com/dcgm">DCGM</a> exposes GPU metrics for Prometheus and can be visualized using Grafana. <code class="docutils literal notranslate"><span class="pre">dcgm-exporter</span></code> is architected to take advantage of
<code class="docutils literal notranslate"><span class="pre">KubeletPodResources</span></code> <a class="reference external" href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">API</a> and exposes GPU metrics in a format that can be
scraped by Prometheus.</p>
<p>With GPU Operator users can customize the metrics to be collected by <code class="docutils literal notranslate"><span class="pre">dcgm-exporter</span></code>. Below are the steps for this</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Fetch the metrics file and save as dcgm-metrics.csv</p></li>
</ol>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>curl<span class="w"> </span>https://raw.githubusercontent.com/NVIDIA/dcgm-exporter/main/etc/dcp-metrics-included.csv<span class="w"> </span>&gt;<span class="w"> </span>dcgm-metrics.csv
</pre></div>
</div>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>Edit the metrics file as required to add/remove any metrics to be collected.</p></li>
<li><p>Create a Namespace <code class="docutils literal notranslate"><span class="pre">gpu-operator-resources</span></code> if one is already not present.</p></li>
</ol>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>create<span class="w"> </span>namespace<span class="w"> </span>gpu-operator-resources
</pre></div>
</div>
</div></blockquote>
<ol class="arabic simple" start="4">
<li><p>Create a ConfigMap using the file edited above.</p></li>
</ol>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>create<span class="w"> </span>configmap<span class="w"> </span>metrics-config<span class="w"> </span>-n<span class="w"> </span>gpu-operator-resources<span class="w"> </span>--from-file<span class="o">=</span>dcgm-metrics.csv
</pre></div>
</div>
</div></blockquote>
<ol class="arabic simple" start="5">
<li><p>Install GPU Operator with additional options <code class="docutils literal notranslate"><span class="pre">--set</span> <span class="pre">dcgmExporter.config.name=metrics-config</span></code> and</p></li>
</ol>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">--set</span> <span class="pre">dcgmExporter.env[0].name=DCGM_EXPORTER_COLLECTORS</span> <span class="pre">--set</span> <span class="pre">dcgmExporter.env[0].value=/etc/dcgm-exporter/dcgm-metrics.csv</span></code></p>
</div></blockquote>
</div></blockquote>
<p>The rest of this section walks through how to setup Prometheus, Grafana using Operators and using Prometheus with <code class="docutils literal notranslate"><span class="pre">dcgm-exporter</span></code>.</p>
<section id="setting-up-prometheus">
<h3>Setting up Prometheus<a class="headerlink" href="#setting-up-prometheus" title="Permalink to this heading"></a></h3>
<p>Implementing a Prometheus stack can be complicated but can be managed by taking advantage of the <code class="docutils literal notranslate"><span class="pre">Helm</span></code> package manager and
the <a class="reference external" href="https://github.com/coreos/prometheus-operator">Prometheus Operator</a> and <a class="reference external" href="https://github.com/coreos/kube-prometheus">kube-prometheus</a> projects.
The Operator uses standard configurations and dashboards for Prometheus and Grafana and the Helm <a class="reference external" href="https://github.com/helm/charts/tree/master/stable/prometheus-operator">prometheus-operator</a>
chart allows you to get a full cluster monitoring solution up and running by installing Prometheus Operator and the rest of the components listed above.</p>
<p>First, add the <code class="docutils literal notranslate"><span class="pre">helm</span></code> repo:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>repo<span class="w"> </span>add<span class="w"> </span>prometheus-community<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>https://prometheus-community.github.io/helm-charts
</pre></div>
</div>
<p>Now, search for the available <code class="docutils literal notranslate"><span class="pre">prometheus</span></code> charts:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>search<span class="w"> </span>repo<span class="w"> </span>kube-prometheus
</pre></div>
</div>
<p>Once you’ve located which the version of the chart to use, inspect the chart so we can modify the settings:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>inspect<span class="w"> </span>values<span class="w"> </span>prometheus-community/kube-prometheus-stack<span class="w"> </span>&gt;<span class="w"> </span>/tmp/kube-prometheus-stack.values
</pre></div>
</div>
<p>Next, we’ll need to edit the values file to change the port at which the Prometheus server service is available. In the <code class="docutils literal notranslate"><span class="pre">prometheus</span></code> instance
section of the chart, change the service type from <code class="docutils literal notranslate"><span class="pre">ClusterIP</span></code> to <code class="docutils literal notranslate"><span class="pre">NodePort</span></code>. This will allow the Prometheus server to be accessible at your
machine ip address at port 30090 as <code class="docutils literal notranslate"><span class="pre">http://&lt;machine-ip&gt;:30090/</span></code></p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">From:</span>
<span class="gp"> #</span><span class="c1"># Port to expose on each node</span>
<span class="gp"> #</span><span class="c1"># Only used if service.type is &#39;NodePort&#39;</span>
<span class="gp"> #</span><span class="c1">#</span>
<span class="go"> nodePort: 30090</span>

<span class="gp"> #</span><span class="c1"># Loadbalancer IP</span>
<span class="gp"> #</span><span class="c1"># Only use if service.type is &quot;loadbalancer&quot;</span>
<span class="go"> loadBalancerIP: &quot;&quot;</span>
<span class="go"> loadBalancerSourceRanges: []</span>
<span class="gp"> #</span><span class="c1"># Service type</span>
<span class="gp"> #</span><span class="c1">#</span>
<span class="go"> type: ClusterIP</span>

<span class="go">To:</span>
<span class="gp"> #</span><span class="c1"># Port to expose on each node</span>
<span class="gp"> #</span><span class="c1"># Only used if service.type is &#39;NodePort&#39;</span>
<span class="gp"> #</span><span class="c1">#</span>
<span class="go"> nodePort: 30090</span>

<span class="gp"> #</span><span class="c1"># Loadbalancer IP</span>
<span class="gp"> #</span><span class="c1"># Only use if service.type is &quot;loadbalancer&quot;</span>
<span class="go"> loadBalancerIP: &quot;&quot;</span>
<span class="go"> loadBalancerSourceRanges: []</span>
<span class="gp"> #</span><span class="c1"># Service type</span>
<span class="gp"> #</span><span class="c1">#</span>
<span class="go"> type: NodePort</span>
</pre></div>
</div>
<p>Also, modify the <code class="docutils literal notranslate"><span class="pre">prometheusSpec.serviceMonitorSelectorNilUsesHelmValues</span></code> settings to <code class="docutils literal notranslate"><span class="pre">false</span></code> below:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span><span class="c1"># If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the</span>
<span class="gp">#</span><span class="c1"># prometheus resource to be created with selectors based on values in the helm deployment,</span>
<span class="gp">#</span><span class="c1"># which will also match the servicemonitors created</span>
<span class="gp">#</span><span class="c1">#</span>
<span class="go">serviceMonitorSelectorNilUsesHelmValues: false</span>
</pre></div>
</div>
<p>Add the following <code class="docutils literal notranslate"><span class="pre">configMap</span></code> to the section on <code class="docutils literal notranslate"><span class="pre">additionalScrapeConfigs</span></code> in the Helm chart.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span><span class="c1"># AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations</span>
<span class="gp">#</span><span class="c1"># are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form</span>
<span class="gp">#</span><span class="c1"># as specified in the official Prometheus documentation:</span>
<span class="gp">#</span><span class="c1"># https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are</span>
<span class="gp">#</span><span class="c1"># appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility</span>
<span class="gp">#</span><span class="c1"># to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible</span>
<span class="gp">#</span><span class="c1"># scrape configs are going to break Prometheus after the upgrade.</span>
<span class="gp">#</span><span class="c1">#</span>
<span class="gp">#</span><span class="c1"># The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the</span>
<span class="gp">#</span><span class="c1"># port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes</span>
<span class="gp">#</span><span class="c1">#</span>
<span class="go">additionalScrapeConfigs:</span>
<span class="go">- job_name: gpu-metrics</span>
<span class="go">  scrape_interval: 1s</span>
<span class="go">  metrics_path: /metrics</span>
<span class="go">  scheme: http</span>
<span class="go">  kubernetes_sd_configs:</span>
<span class="go">  - role: endpoints</span>
<span class="go">    namespaces:</span>
<span class="go">      names:</span>
<span class="go">      - gpu-operator</span>
<span class="go">  relabel_configs:</span>
<span class="go">  - source_labels: [__meta_kubernetes_pod_node_name]</span>
<span class="go">    action: replace</span>
<span class="go">    target_label: kubernetes_node</span>
</pre></div>
</div>
<p>Finally, we can deploy the Prometheus and Grafana pods using the <code class="docutils literal notranslate"><span class="pre">kube-prometheus-stack</span></code> via Helm:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>prometheus-community/kube-prometheus-stack<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--create-namespace<span class="w"> </span>--namespace<span class="w"> </span>prometheus<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--values<span class="w"> </span>/tmp/kube-prometheus-stack.values
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can also override values in the Prometheus chart directly on the Helm command line:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>prometheus-community/kube-prometheus-stack<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--create-namespace<span class="w"> </span>--namespace<span class="w"> </span>prometheus<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--set<span class="w"> </span>prometheus.service.type<span class="o">=</span>NodePort<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--set<span class="w"> </span>prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues<span class="o">=</span><span class="nb">false</span>
</pre></div>
</div>
</div>
<p>You should see a console output as below:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">NAME: kube-prometheus-stack-1637791640</span>
<span class="go">LAST DEPLOYED: Wed Nov 24 22:07:22 2021</span>
<span class="go">NAMESPACE: prometheus</span>
<span class="go">STATUS: deployed</span>
<span class="go">REVISION: 1</span>
<span class="go">NOTES:</span>
<span class="go">kube-prometheus-stack has been installed. Check its status by running:</span>
<span class="go">  kubectl --namespace prometheus get pods -l &quot;release=kube-prometheus-stack-1637791640&quot;</span>

<span class="go">Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create &amp; configure Alertmanager and Prometheus instances using the Operator.</span>
</pre></div>
</div>
<p>Now you can see the Prometheus and Grafana pods:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-A
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">NAMESPACE                NAME                                                              READY   STATUS      RESTARTS   AGE</span>
<span class="go">default                  gpu-operator-1597965115-node-feature-discovery-master-fbf9rczx5   1/1     Running     1          6h57m</span>
<span class="go">default                  gpu-operator-1597965115-node-feature-discovery-worker-n58pm       1/1     Running     1          6h57m</span>
<span class="go">default                  gpu-operator-774ff7994c-xh62d                                     1/1     Running     1          6h57m</span>
<span class="go">default                  gpu-operator-test                                                 0/1     Completed   0          8h</span>
<span class="go">gpu-operator-resources   nvidia-container-toolkit-daemonset-grnnd                          1/1     Running     1          6h57m</span>
<span class="go">gpu-operator-resources   nvidia-dcgm-exporter-nv5z7                                        1/1     Running     7          6h57m</span>
<span class="go">gpu-operator-resources   nvidia-device-plugin-daemonset-qq6lq                              1/1     Running     7          6h57m</span>
<span class="go">gpu-operator-resources   nvidia-device-plugin-validation                                   0/1     Completed   0          6h57m</span>
<span class="go">gpu-operator-resources   nvidia-driver-daemonset-vwzvq                                     1/1     Running     1          6h57m</span>
<span class="go">gpu-operator-resources   nvidia-driver-validation                                          0/1     Completed   3          6h57m</span>
<span class="go">kube-system              calico-kube-controllers-578894d4cd-pv5kw                          1/1     Running     1          10h</span>
<span class="go">kube-system              calico-node-ffhdd                                                 1/1     Running     1          10h</span>
<span class="go">kube-system              coredns-66bff467f8-nwdrx                                          1/1     Running     1          10h</span>
<span class="go">kube-system              coredns-66bff467f8-srg8d                                          1/1     Running     1          10h</span>
<span class="go">kube-system              etcd-ip-172-31-80-124                                             1/1     Running     1          10h</span>
<span class="go">kube-system              kube-apiserver-ip-172-31-80-124                                   1/1     Running     1          10h</span>
<span class="go">kube-system              kube-controller-manager-ip-172-31-80-124                          1/1     Running     1          10h</span>
<span class="go">kube-system              kube-proxy-kj5qb                                                  1/1     Running     1          10h</span>
<span class="go">kube-system              kube-scheduler-ip-172-31-80-124                                   1/1     Running     1          10h</span>
<span class="go">prometheus               alertmanager-prometheus-operator-159799-alertmanager-0            2/2     Running     0          12s</span>
<span class="go">prometheus               prometheus-operator-159799-operator-78f95fccbd-hcl76              2/2     Running     0          16s</span>
<span class="go">prometheus               prometheus-operator-1597990146-grafana-5c7db4f7d4-qcjbj           2/2     Running     0          16s</span>
<span class="go">prometheus               prometheus-operator-1597990146-kube-state-metrics-645c57c8x28nv   1/1     Running     0          16s</span>
<span class="go">prometheus               prometheus-operator-1597990146-prometheus-node-exporter-6lchc     1/1     Running     0          16s</span>
<span class="go">prometheus               prometheus-prometheus-operator-159799-prometheus-0                2/3     Running     0          2s</span>
</pre></div>
</div>
<p>You can view the services setup as part of the operator and <code class="docutils literal notranslate"><span class="pre">dcgm-exporter</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>svc<span class="w"> </span>-A
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">NAMESPACE                NAME                                                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                        AGE</span>
<span class="go">default                  gpu-operator-1597965115-node-feature-discovery-master     ClusterIP   10.110.46.7      &lt;none&gt;        8080/TCP                       6h57m</span>
<span class="go">default                  kubernetes                                                ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP                        10h</span>
<span class="go">default                  tf-notebook                                               NodePort    10.106.229.20    &lt;none&gt;        80:30001/TCP                   8h</span>
<span class="go">gpu-operator-resources   nvidia-dcgm-exporter                                      ClusterIP   10.99.250.100    &lt;none&gt;        9400/TCP                       6h57m</span>
<span class="go">kube-system              kube-dns                                                  ClusterIP   10.96.0.10       &lt;none&gt;        53/UDP,53/TCP,9153/TCP         10h</span>
<span class="go">kube-system              prometheus-operator-159797-kubelet                        ClusterIP   None             &lt;none&gt;        10250/TCP,10255/TCP,4194/TCP   4h50m</span>
<span class="go">kube-system              prometheus-operator-159799-coredns                        ClusterIP   None             &lt;none&gt;        9153/TCP                       32s</span>
<span class="go">kube-system              prometheus-operator-159799-kube-controller-manager        ClusterIP   None             &lt;none&gt;        10252/TCP                      32s</span>
<span class="go">kube-system              prometheus-operator-159799-kube-etcd                      ClusterIP   None             &lt;none&gt;        2379/TCP                       32s</span>
<span class="go">kube-system              prometheus-operator-159799-kube-proxy                     ClusterIP   None             &lt;none&gt;        10249/TCP                      32s</span>
<span class="go">kube-system              prometheus-operator-159799-kube-scheduler                 ClusterIP   None             &lt;none&gt;        10251/TCP                      32s</span>
<span class="go">kube-system              prometheus-operator-159799-kubelet                        ClusterIP   None             &lt;none&gt;        10250/TCP,10255/TCP,4194/TCP   18s</span>
<span class="go">prometheus               alertmanager-operated                                     ClusterIP   None             &lt;none&gt;        9093/TCP,9094/TCP,9094/UDP     28s</span>
<span class="go">prometheus               prometheus-operated                                       ClusterIP   None             &lt;none&gt;        9090/TCP                       18s</span>
<span class="go">prometheus               prometheus-operator-159799-alertmanager                   ClusterIP   10.106.93.161    &lt;none&gt;        9093/TCP                       32s</span>
<span class="go">prometheus               prometheus-operator-159799-operator                       ClusterIP   10.100.116.170   &lt;none&gt;        8080/TCP,443/TCP               32s</span>
<span class="go">prometheus               prometheus-operator-159799-prometheus                     NodePort    10.102.169.42    &lt;none&gt;        9090:30090/TCP                 32s</span>
<span class="go">prometheus               prometheus-operator-1597990146-grafana                    ClusterIP   10.104.40.69     &lt;none&gt;        80/TCP                         32s</span>
<span class="go">prometheus               prometheus-operator-1597990146-kube-state-metrics         ClusterIP   10.100.204.91    &lt;none&gt;        8080/TCP                       32s</span>
<span class="go">prometheus               prometheus-operator-1597990146-prometheus-node-exporter   ClusterIP   10.97.64.60      &lt;none&gt;        9100/TCP                       32s</span>
</pre></div>
</div>
<p>You can observe that the Prometheus server is available at port 30090 on the node’s IP address. Open your browser to <code class="docutils literal notranslate"><span class="pre">http://&lt;machine-ip-address&gt;:30090</span></code>.
It may take a few minutes for DCGM to start publishing the metrics to Prometheus. The metrics availability can be verified by typing <code class="docutils literal notranslate"><span class="pre">DCGM_FI_DEV_GPU_UTIL</span></code>
in the event bar to determine if the GPU metrics are visible:</p>
<a class="reference internal image-reference" href="gpu-operator/archive/kubernetes/graphics/dcgm-e2e/001-dcgm-e2e-prom-screenshot.png"><img alt="gpu-operator/archive/kubernetes/graphics/dcgm-e2e/001-dcgm-e2e-prom-screenshot.png" src="gpu-operator/archive/kubernetes/graphics/dcgm-e2e/001-dcgm-e2e-prom-screenshot.png" style="width: 800px;" /></a>
</section>
<section id="using-grafana">
<h3>Using Grafana<a class="headerlink" href="#using-grafana" title="Permalink to this heading"></a></h3>
<p>You can also launch the Grafana tools for visualizing the GPU metrics.</p>
<p>There are two mechanisms for dealing with the ports on which Grafana is available - the service can be patched or port-forwarding can be used to reach the home page.
Either option can be chosen based on preference.</p>
<section id="patching-the-grafana-service">
<h4>Patching the Grafana Service<a class="headerlink" href="#patching-the-grafana-service" title="Permalink to this heading"></a></h4>
<p>By default, Grafana uses a <code class="docutils literal notranslate"><span class="pre">ClusterIP</span></code> to expose the ports on which the service is accessible. This can be changed to a <code class="docutils literal notranslate"><span class="pre">NodePort</span></code> instead, so the page is accessible
from the browser, similar to the Prometheus dashboard.</p>
<p>You can use <a class="reference external" href="https://kubernetes.io/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/">kubectl patch</a> to update the service API
object to expose a <code class="docutils literal notranslate"><span class="pre">NodePort</span></code> instead.</p>
<p>First, modify the spec to change the service type:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat<span class="w"> </span>&lt;&lt;<span class="w"> </span>EOF<span class="w"> </span><span class="p">|</span><span class="w"> </span>tee<span class="w"> </span>grafana-patch.yaml
<span class="go">spec:</span>
<span class="go">  type: NodePort</span>
<span class="go">  nodePort: 32322</span>
<span class="go">EOF</span>
</pre></div>
</div>
<p>And now use <code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">patch</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>patch<span class="w"> </span>svc<span class="w"> </span>prometheus-operator-1597990146-grafana<span class="w"> </span>-n<span class="w"> </span>prometheus<span class="w"> </span>--patch<span class="w"> </span><span class="s2">&quot;</span><span class="k">$(</span>cat<span class="w"> </span>grafana-patch.yaml<span class="k">)</span><span class="s2">&quot;</span>
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">service/prometheus-operator-1597990146-grafana patched</span>
</pre></div>
</div>
<p>You can verify that the service is now exposed at an externally accessible port:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>svc<span class="w"> </span>-A
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">NAMESPACE     NAME                                                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                        AGE</span>
<span class="go">&lt;snip&gt;</span>
<span class="go">prometheus    prometheus-operator-1597990146-grafana                    NodePort    10.108.187.141   &lt;none&gt;        80:32258/TCP                   17h</span>
</pre></div>
</div>
<p>Open your browser to <code class="docutils literal notranslate"><span class="pre">http://&lt;machine-ip-address&gt;:32258</span></code> and view the Grafana login page. Access Grafana home using the <code class="docutils literal notranslate"><span class="pre">admin</span></code> username.
The password credentials for the login are available in the <code class="docutils literal notranslate"><span class="pre">prometheus.values</span></code> file we edited in the earlier section of the doc:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span><span class="c1"># Deploy default dashboards.</span>
<span class="gp">#</span><span class="c1">#</span>
<span class="go">defaultDashboardsEnabled: true</span>

<span class="go">adminPassword: prom-operator</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="gpu-operator/archive/kubernetes/graphics/dcgm-e2e/002-dcgm-e2e-grafana-screenshot.png"><img alt="gpu-operator/archive/kubernetes/graphics/dcgm-e2e/002-dcgm-e2e-grafana-screenshot.png" src="gpu-operator/archive/kubernetes/graphics/dcgm-e2e/002-dcgm-e2e-grafana-screenshot.png" style="width: 800px;" /></a>
</section>
</section>
</section>
<section id="upgrade">
<span id="operator-upgrades-1-8"></span><h2>Upgrade<a class="headerlink" href="#upgrade" title="Permalink to this heading"></a></h2>
<section id="using-helm">
<h3>Using Helm<a class="headerlink" href="#using-helm" title="Permalink to this heading"></a></h3>
<p>Starting with GPU Operator v1.8.0, the GPU Operator supports dynamic updates to existing resources. This allows
the GPU Operator to ensure settings from the <cite>ClusterPolicy</cite> Spec are always applied and current.</p>
<p>Since Helm doesn’t support auto upgrade of existing CRDs, the user needs to follow a two step process to
upgrade the GPU Operator chart:</p>
<div class="align-default"><img height="120" src="../../../_images/blockdiag-369eda0a7f3f131de010bca25d186c7a6a920e0e.png" width="448" /></div>
<p>With this workflow, all existing GPU operator resources are updated inline and the <cite>ClusterPolicy</cite> resource is patched with updates from <code class="docutils literal notranslate"><span class="pre">values.yaml</span></code>.</p>
<p>Download the CRD from the specific <cite>&lt;release-tag&gt;</cite> from the Git repo. For example:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>wget<span class="w"> </span>https://gitlab.com/nvidia/kubernetes/gpu-operator/-/raw/&lt;release-tag&gt;/deployments/gpu-operator/crds/nvidia.com_clusterpolicies_crd.yaml
</pre></div>
</div>
<p>Apply the CRD using the file downloaded above:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$  </span>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>nvidia.com_clusterpolicies_crd.yaml
</pre></div>
</div>
<p>Fetch latest values from the chart (replace the <code class="docutils literal notranslate"><span class="pre">.x</span></code> below with the desired version)</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>show<span class="w"> </span>values<span class="w"> </span>nvidia/gpu-operator<span class="w"> </span>--version<span class="o">=</span><span class="m">1</span>.8.x<span class="w"> </span>&gt;<span class="w"> </span>values-1.8.x.yaml
</pre></div>
</div>
<p>Update the values file as needed.</p>
<p>And upgrade via Helm:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>upgrade<span class="w"> </span>gpu-operator<span class="w"> </span>-n<span class="w"> </span>gpu-operator-resources<span class="w"> </span>-f<span class="w"> </span>values-1.8.x.yaml
</pre></div>
</div>
<section id="cluster-policy-updates">
<h4>Cluster Policy Updates<a class="headerlink" href="#cluster-policy-updates" title="Permalink to this heading"></a></h4>
<p>The GPU Operator also supports dynamic updates to the <code class="docutils literal notranslate"><span class="pre">ClusterPolicy</span></code> CustomResource using <code class="docutils literal notranslate"><span class="pre">kubectl</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>edit<span class="w"> </span>clusterpolicy
</pre></div>
</div>
<p>After the edits are complete, Kubernetes will automatically apply the updates to cluster.</p>
</section>
<section id="additional-controls-for-driver-upgrades">
<h4>Additional Controls for Driver Upgrades<a class="headerlink" href="#additional-controls-for-driver-upgrades" title="Permalink to this heading"></a></h4>
<p>While most of the GPU Operator managed daemonset pods can be updated without dependencies, the NVIDIA driver daemonset needs special handling.
This is due to the fact that the driver kernel modules have to be unloaded and loaded again on each driver container restart.
In turn this has certain dependencies:</p>
<ol class="arabic simple">
<li><p>All clients to the GPU driver have to be disabled</p></li>
<li><p>The current GPU driver kernel modules have to be unloaded</p></li>
<li><p>The updated driver pods need to start</p></li>
<li><p>The GPU driver has to be installed and new kernel modules loaded</p></li>
<li><p>The driver clients disabled initially have to be enabled again</p></li>
</ol>
<p>In order to achieve this, a new component called <cite>k8s-driver-manager</cite> is added which will ensure that, all
existing GPU driver clients are disabled and current modules are unloaded. This component is added as an <cite>initContainer</cite>
within the driver daemonset.</p>
<p>Since the <cite>k8s-driver-manager</cite> evicts pods from the node to complete the driver upgrade, users can control the node drain
behavior using environment variables as specified in the GPU Operator Helm chart (see the <code class="docutils literal notranslate"><span class="pre">driver.manager.env</span></code> variables):</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">IfNotPresent</span>
<span class="nt">env</span><span class="p">:</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ENABLE_AUTO_DRAIN</span>
<span class="w">   </span><span class="l l-Scalar l-Scalar-Plain">value</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;true&quot;</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">DRAIN_USE_FORCE</span>
<span class="w">   </span><span class="l l-Scalar l-Scalar-Plain">value</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;false&quot;</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">DRAIN_POD_SELECTOR_LABEL</span>
<span class="w">   </span><span class="l l-Scalar l-Scalar-Plain">value</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;&quot;</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">DRAIN_TIMEOUT_SECONDS</span>
<span class="w">   </span><span class="l l-Scalar l-Scalar-Plain">value</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;0s&quot;</span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">DRAIN_DELETE_EMPTYDIR_DATA</span>
<span class="w">   </span><span class="l l-Scalar l-Scalar-Plain">value</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="s">&quot;false&quot;</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The <em>DRAIN_POD_SELECTOR_LABEL</em> env can be used to let <cite>k8s-driver-manager</cite> only evict GPU pods with matching labels from the node.
This way, CPU only pods will not be affected during driver upgrades.</p></li>
<li><p><em>DRAIN_USE_FORCE</em> needs to be enabled for evicting GPU pods that are not managed by any of the replication controllers (Deployment, Daemonset, StatefulSet, ReplicaSet).</p></li>
</ul>
</section>
</section>
<section id="using-olm-in-openshift">
<h3>Using OLM in OpenShift<a class="headerlink" href="#using-olm-in-openshift" title="Permalink to this heading"></a></h3>
<p>For upgrading the GPU Operator when running in OpenShift, refer to the official documentation on upgrading installed operators:
<a class="reference external" href="https://docs.openshift.com/container-platform/4.8/operators/admin/olm-upgrading-operators.html">https://docs.openshift.com/container-platform/4.8/operators/admin/olm-upgrading-operators.html</a></p>
</section>
</section>
<section id="uninstall">
<h2>Uninstall<a class="headerlink" href="#uninstall" title="Permalink to this heading"></a></h2>
<p>To uninstall the operator:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>delete<span class="w"> </span><span class="k">$(</span>helm<span class="w"> </span>list<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>gpu-operator<span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;{print $1}&#39;</span><span class="k">)</span>
</pre></div>
</div>
<p>You should now see all the pods being deleted:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>gpu-operator-resources
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">No resources found.</span>
</pre></div>
</div>
<p>Also, ensure that CRDs created during the operator install have been removed:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>crds<span class="w"> </span>-A<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>-i<span class="w"> </span>clusterpolicies.nvidia.com
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>After un-install of GPU Operator, the NVIDIA driver modules might still be loaded.
Either reboot the node or unload them using the following command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>rmmod<span class="w"> </span>nvidia_modeset<span class="w"> </span>nvidia_uvm<span class="w"> </span>nvidia
</pre></div>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="overview.html" class="btn btn-neutral float-left" title="Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="platform-support.html" class="btn btn-neutral float-right" title="Platform Support" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018-2023, NVIDIA Corporation.
      <span class="lastupdated">Last updated on 2023-04-10.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>  

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  .rst-content dl:not(.docutils) dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }
  </style>
  

</body>
</html>