<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Appendix &mdash; NVIDIA Cloud Native Technologies  documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../../../_static/nvidia.ico"/>
    <link rel="canonical" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/archive/1.11.0/appendix.html"/>
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/sphinx_highlight.js"></script>
        <script src="../../../_static/clipboard.min.js"></script>
        <script src="../../../_static/copybutton.js"></script>
        <script src="../../../_static/js/google-analytics/google-analytics-tracker.js"></script>
        <script src="../../../_static/js/google-analytics/google-analytics-write.js"></script>
        <script src="//assets.adobedtm.com/b92787824f2e0e9b68dc2e993f9bd995339fe417/satelliteLib-7ba51e58dc61bcb0e9311aadd02a0108ab24cc6c.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Overview" href="../1.10.1/overview.html" />
    <link rel="prev" title="Running KubeVirt VMs with the GPU Operator" href="gpu-operator-kubevirt.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" > 
            <a href="../../../contents.html">
            <img src="../../../_static/NVLogo_H_B&W.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }

    .wy-nav-content {
      max-width: 1000px;
    }
  </style>
  
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">NVIDIA Container Toolkit</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/concepts.html">Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/arch-overview.html">Architecture Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/install-guide.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/user-guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/archive.html">Archive</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">NVIDIA GPU Operator</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../about.html">About the Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../platform-support.html">Platform Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting-started.html#install-nvidia-gpu-operator">Install NVIDIA GPU Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gpu-driver-upgrades.html">GPU Driver Upgrades</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced-configurations.html">Advanced Configurations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../openshift/contents.html">GPU Operator on OpenShift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../appendix.html">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../license.html">Licenses and Contributing</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../archive.html">Archive</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id1">22.9.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id2">22.9.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id3">1.11.1</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../archive.html#id4">1.11.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id5">1.10.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id6">1.9.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id7">1.9.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id8">1.8</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Kubernetes with GPUs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../kubernetes/install-k8s.html">Install Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../kubernetes/mig-k8s.html">MIG Support in Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../kubernetes/anthos-guide.html">NVIDIA GPUs with Google Cloud’s Anthos</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Telemetry</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../gpu-telemetry/dcgm-exporter.html">DCGM-Exporter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../gpu-telemetry/dcgm-exporter.html#integrating-gpu-telemetry-into-kubernetes">Integrating GPU Telemetry into Kubernetes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multi-Instance GPU</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../mig/mig.html">Multi-Instance GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mig/mig-k8s.html">MIG Support in Kubernetes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Driver Containers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../driver-containers/overview.html">Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Playground</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../playground/dind.html">Docker-in-Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../playground/x-arch.html">Running Cross-Architecture Containers</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../contents.html">NVIDIA Cloud Native Technologies</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../contents.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../archive.html">Archive</a> &raquo;</li>
      <li>Appendix</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="appendix">
<span id="operator-appendix-1-11-0"></span><h1>Appendix<a class="headerlink" href="#appendix" title="Permalink to this heading"></a></h1>
<section id="install-gpu-operator-in-proxy-environments">
<span id="install-gpu-operator-1-11-0-proxy"></span><h2>Install GPU Operator in Proxy Environments<a class="headerlink" href="#install-gpu-operator-in-proxy-environments" title="Permalink to this heading"></a></h2>
<section id="introduction">
<h3>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading"></a></h3>
<p>This page describes how to successfully deploy the GPU Operator in clusters behind a HTTP Proxy.
By default, the GPU Operator requires internet access for the following reasons:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Container images need to be pulled during GPU Operator installation.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">driver</span></code> container needs to download several OS packages prior to driver installation.</p></li>
</ol>
</div></blockquote>
<p>To address these requirements, all Kubernetes nodes as well as the <code class="docutils literal notranslate"><span class="pre">driver</span></code> container need proper configuration
in order to direct traffic through the proxy.</p>
<p>This document demonstrates how to configure the GPU Operator so that the <code class="docutils literal notranslate"><span class="pre">driver</span></code> container can successfully
download packages behind a HTTP proxy. Since configuring Kubernetes/container runtime components to use
a proxy is not specific to the GPU Operator, we do not include those instructions here.</p>
<p>The instructions for Openshift are different, so skip the section titled <a class="reference internal" href="install-gpu-operator-proxy.html#proxy-config-openshift-1-11-0"><span class="std std-ref">HTTP Proxy Configuration for Openshift</span></a> if you are not running Openshift.</p>
</section>
<section id="prerequisites">
<h3>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>Kubernetes cluster is configured with HTTP proxy settings (container runtime should be enabled with HTTP proxy)</p></li>
</ul>
</section>
<section id="http-proxy-configuration-for-openshift">
<span id="proxy-config-openshift-1-11-0"></span><h3>HTTP Proxy Configuration for Openshift<a class="headerlink" href="#http-proxy-configuration-for-openshift" title="Permalink to this heading"></a></h3>
<p>For Openshift, it is recommended to use the cluster-wide Proxy object to provide proxy information for the cluster.
Please follow the procedure described in <a class="reference external" href="https://docs.openshift.com/container-platform/4.8/networking/enable-cluster-wide-proxy.html">Configuring the cluster-wide proxy</a>
from Red Hat Openshift public documentation. The GPU Operator will automatically inject proxy related ENV into the <code class="docutils literal notranslate"><span class="pre">driver</span></code> container
based on information present in the cluster-wide Proxy object.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>GPU Operator v1.8.0 does not work well on RedHat OpenShift when a cluster-wide Proxy object is configured and causes constant restarts of <code class="docutils literal notranslate"><span class="pre">driver</span></code> container. This will be fixed in an upcoming patch release v1.8.2.</p></li>
</ul>
</div>
</section>
<section id="http-proxy-configuration">
<h3>HTTP Proxy Configuration<a class="headerlink" href="#http-proxy-configuration" title="Permalink to this heading"></a></h3>
<p>First, get the <code class="docutils literal notranslate"><span class="pre">values.yaml</span></code> file used for GPU Operator configuration:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>curl<span class="w"> </span>-sO<span class="w"> </span>https://raw.githubusercontent.com/NVIDIA/gpu-operator/v1.7.0/deployments/gpu-operator/values.yaml
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Replace <code class="docutils literal notranslate"><span class="pre">v1.7.0</span></code> in the above command with the version you want to use.</p>
</div>
<p>Specify <code class="docutils literal notranslate"><span class="pre">driver.env</span></code> in <code class="docutils literal notranslate"><span class="pre">values.yaml</span></code> with appropriate HTTP_PROXY, HTTPS_PROXY, and NO_PROXY environment variables
(in both uppercase and lowercase).</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">driver</span><span class="p">:</span>
<span class="w">   </span><span class="nt">env</span><span class="p">:</span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">HTTPS_PROXY</span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">http://&lt;example.proxy.com:port&gt;</span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">HTTP_PROXY</span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">http://&lt;example.proxy.com:port&gt;</span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NO_PROXY</span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;example.com&gt;</span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">https_proxy</span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">http://&lt;example.proxy.com:port&gt;</span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">http_proxy</span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">http://&lt;example.proxy.com:port&gt;</span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">no_proxy</span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;example.com&gt;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Proxy related ENV are automatically injected by GPU Operator into the <code class="docutils literal notranslate"><span class="pre">driver</span></code> container to indicate proxy information used when downloading necessary packages.</p></li>
<li><p>If HTTPS Proxy server is setup then change the values of HTTPS_PROXY and https_proxy to use <code class="docutils literal notranslate"><span class="pre">https</span></code> instead.</p></li>
</ul>
</div>
</section>
<section id="deploy-gpu-operator">
<h3>Deploy GPU Operator<a class="headerlink" href="#deploy-gpu-operator" title="Permalink to this heading"></a></h3>
<p>Download and deploy GPU Operator Helm Chart with the updated <code class="docutils literal notranslate"><span class="pre">values.yaml</span></code>.</p>
<p>Fetch the chart from NGC repository. <code class="docutils literal notranslate"><span class="pre">v1.10.0</span></code> is used as an example in the command below:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>fetch<span class="w"> </span>https://helm.ngc.nvidia.com/nvidia/charts/gpu-operator-v1.10.0.tgz
</pre></div>
</div>
<p>Install the GPU Operator with updated <code class="docutils literal notranslate"><span class="pre">values.yaml</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--create-namespace<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>gpu-operator-v1.10.0.tgz<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-f<span class="w"> </span>values.yaml
</pre></div>
</div>
<p>Check the status of the pods to ensure all the containers are running:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>gpu-operator
</pre></div>
</div>
</section>
</section>
<section id="install-gpu-operator-in-air-gapped-environments">
<span id="install-gpu-operator-1-11-0-air-gapped"></span><h2>Install GPU Operator in Air-gapped Environments<a class="headerlink" href="#install-gpu-operator-in-air-gapped-environments" title="Permalink to this heading"></a></h2>
<section id="id1">
<h3>Introduction<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h3>
<p>This page describes how to successfully deploy the GPU Operator in clusters with restricted internet access.
By default, The GPU Operator requires internet access for the following reasons:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Container images need to be pulled during GPU Operator installation.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">driver</span></code> container needs to download several OS packages prior to driver installation.</p></li>
</ol>
</div></blockquote>
<p>To address these requirements, it may be necessary to create a local image registry and/or a local package repository
so that the necessary images and packages are available for your cluster. In subsequent sections, we detail how to
configure the GPU Operator to use local image registries and local package repositories. If your cluster is behind
a proxy, also follow the steps from <a class="reference internal" href="install-gpu-operator-proxy.html#install-gpu-operator-1-11-0-proxy"><span class="std std-ref">Install GPU Operator in Proxy Environments</span></a>.</p>
<p>Different steps are required for different environments with varying levels of internet connectivity.
The supported use cases/environments are listed in the below table:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 12%" />
<col style="width: 26%" />
<col style="width: 31%" />
<col style="width: 31%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head" colspan="2"></th>
<th class="head" colspan="2"><p>Network Flow</p></th>
</tr>
<tr class="row-even"><th class="head" colspan="2"><p>Use Case</p></th>
<th class="head"><p>Pulling Images</p></th>
<th class="head"><p>Pulling Packages</p></th>
</tr>
</thead>
<tbody>
<tr class="row-odd"><td><p><strong>1</strong></p></td>
<td><p>HTTP Proxy with
full Internet
access</p></td>
<td><p>K8s node –&gt; HTTP
Proxy –&gt; Internet
Image Registry</p></td>
<td><p>Driver container
–&gt; HTTP Proxy –&gt;
Internet Package
Repository</p></td>
</tr>
<tr class="row-even"><td><p><strong>2</strong></p></td>
<td><p>HTTP Proxy with
limited Internet
access</p></td>
<td><p>K8s node –&gt; HTTP
Proxy –&gt; Internet
Image Registry</p></td>
<td><p>Driver container
–&gt; HTTP Proxy –&gt;
Local Package
Repository</p></td>
</tr>
<tr class="row-odd"><td><p><strong>3a</strong></p></td>
<td><p>Full Air-Gapped
(w/ HTTP Proxy)</p></td>
<td><p>K8s node –&gt; Local
Image Registry</p></td>
<td><p>Driver container
–&gt; HTTP Proxy –&gt;
Local Package
Repository</p></td>
</tr>
<tr class="row-even"><td><p><strong>3b</strong></p></td>
<td><p>Full Air-Gapped
(w/o HTTP Proxy)</p></td>
<td><p>K8s node –&gt; Local
Image Registry</p></td>
<td><p>Driver container–&gt;
Local Package
Repository</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For Red Hat Openshift deployments in air-gapped environments (use cases 2, 3a and 3b), see the documentation <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/openshift/mirror-gpu-ocp-disconnected.html">here</a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Ensure that Kubernetes nodes can successfully reach the local DNS server(s).
Public name resolution for image registry and package repositories are
mandatory for use cases 1 and 2.</p>
</div>
<p>Before proceeding to the next sections, get the <code class="docutils literal notranslate"><span class="pre">values.yaml</span></code> file used for GPU Operator configuration.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>curl<span class="w"> </span>-sO<span class="w"> </span>https://raw.githubusercontent.com/NVIDIA/gpu-operator/v1.7.0/deployments/gpu-operator/values.yaml
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Replace <code class="docutils literal notranslate"><span class="pre">v1.7.0</span></code> in the above command with the version you want to use.</p>
</div>
</section>
<section id="local-image-registry">
<h3>Local Image Registry<a class="headerlink" href="#local-image-registry" title="Permalink to this heading"></a></h3>
<p>Without internet access, the GPU Operator requires all images to be hosted in a local image registry that is accessible
to all nodes in the cluster. To allow the GPU Operator to work with a local registry, users can specify local
repository, image, tag along with pull secrets in <code class="docutils literal notranslate"><span class="pre">values.yaml</span></code>.</p>
<section id="pulling-and-pushing-container-images-to-local-registry">
<h4>Pulling and pushing container images to local registry<a class="headerlink" href="#pulling-and-pushing-container-images-to-local-registry" title="Permalink to this heading"></a></h4>
<p>To pull the correct images from the NVIDIA registry, you can leverage the fields <code class="docutils literal notranslate"><span class="pre">repository</span></code>, <code class="docutils literal notranslate"><span class="pre">image</span></code> and <code class="docutils literal notranslate"><span class="pre">version</span></code>
specified in the file <code class="docutils literal notranslate"><span class="pre">values.yaml</span></code>.</p>
<p>The general syntax for the container image is <code class="docutils literal notranslate"><span class="pre">&lt;repository&gt;/&lt;image&gt;:&lt;version&gt;</span></code>.</p>
<p>If the version is not specified, you can retrieve the information from the NVIDIA NGC catalog (<a class="reference external" href="https://ngc.nvidia.com/catalog">https://ngc.nvidia.com/catalog</a>)
by checking the available tags for an image.</p>
<p>An example is shown below with the gpu-operator container image:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">operator</span><span class="p">:</span>
<span class="w">    </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvcr.io/nvidia</span>
<span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpu-operator</span>
<span class="w">    </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;v1.9.0&quot;</span>
</pre></div>
</div>
<p>For instance, to pull the gpu-operator image version v1.9.0, use the following instruction:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>pull<span class="w"> </span>nvcr.io/nvidia/gpu-operator:v1.9.0
</pre></div>
</div>
<p>There is one caveat with regards to the driver image. The version field must be appended by the OS name running on the worker node.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">driver</span><span class="p">:</span>
<span class="w">    </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvcr.io/nvidia</span>
<span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">driver</span>
<span class="w">    </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;470.82.01&quot;</span>
</pre></div>
</div>
<p>To pull the driver image for Ubuntu 20.04:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>pull<span class="w"> </span>nvcr.io/nvidia/driver:470.82.01-ubuntu20.04
</pre></div>
</div>
<p>To pull the driver image for CentOS 8:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>pull<span class="w"> </span>nvcr.io/nvidia/driver:470.82.01-centos8
</pre></div>
</div>
<p>To push the images to the local registry, simply tag the pulled images by prefixing the image with the image registry information.</p>
<p>Using the above examples, this will result in:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>tag<span class="w"> </span>nvcr.io/nvidia/gpu-operator:v1.9.0<span class="w"> </span>&lt;local-registry&gt;/&lt;local-path&gt;/gpu-operator:v1.9.0
<span class="gp">$ </span>docker<span class="w"> </span>tag<span class="w"> </span>nvcr.io/nvidia/driver:470.82.01-ubuntu20.04<span class="w"> </span>&lt;local-registry&gt;/&lt;local-path&gt;/driver:470.82.01-ubuntu20.04
</pre></div>
</div>
<p>Finally, push the images to the local registry:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>push<span class="w">  </span>&lt;local-registry&gt;/&lt;local-path&gt;/gpu-operator:v1.9.0
<span class="gp">$ </span>docker<span class="w"> </span>push<span class="w"> </span>&lt;local-registry&gt;/&lt;local-path&gt;/driver:470.82.01-ubuntu20.04
</pre></div>
</div>
<p>Update <code class="docutils literal notranslate"><span class="pre">values.yaml</span></code> with local registry information in the repository field.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>replace &lt;repo.example.com:port&gt; below with your local image registry url and port</p>
</div>
<p>Sample of <code class="docutils literal notranslate"><span class="pre">values.yaml</span></code> for GPU Operator v1.9.0:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">operator</span><span class="p">:</span>
<span class="w">  </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;repo.example.com:port&gt;</span>
<span class="w">  </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpu-operator</span>
<span class="w">  </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.9.0</span>
<span class="w">  </span><span class="nt">imagePullSecrets</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>
<span class="w">  </span><span class="nt">initContainer</span><span class="p">:</span>
<span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cuda</span>
<span class="w">    </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;repo.example.com:port&gt;</span>
<span class="w">    </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">11.4.2-base-ubi8</span>

<span class="w w-Error"> </span><span class="nt">validator</span><span class="p">:</span>
<span class="w">   </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpu-operator-validator</span>
<span class="w">   </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;repo.example.com:port&gt;</span>
<span class="w">   </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.9.0</span>
<span class="w">   </span><span class="nt">imagePullSecrets</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>

<span class="w"> </span><span class="nt">driver</span><span class="p">:</span>
<span class="w">   </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;repo.example.com:port&gt;</span>
<span class="w">   </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">driver</span>
<span class="w">   </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;470.82.01&quot;</span>
<span class="w">   </span><span class="nt">imagePullSecrets</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>
<span class="w">   </span><span class="nt">manager</span><span class="p">:</span>
<span class="w">     </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">k8s-driver-manager</span>
<span class="w">     </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;repo.example.com:port&gt;</span>
<span class="w">     </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v0.2.0</span>

<span class="w"> </span><span class="nt">toolkit</span><span class="p">:</span>
<span class="w">   </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;repo.example.com:port&gt;</span>
<span class="w">   </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">container-toolkit</span>
<span class="w">   </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.7.2-ubuntu18.04</span>
<span class="w">   </span><span class="nt">imagePullSecrets</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>

<span class="w"> </span><span class="nt">devicePlugin</span><span class="p">:</span>
<span class="w">   </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;repo.example.com:port&gt;</span>
<span class="w">   </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">k8s-device-plugin</span>
<span class="w">   </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v0.10.0-ubi8</span>
<span class="w">   </span><span class="nt">imagePullSecrets</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>

<span class="w"> </span><span class="nt">dcgmExporter</span><span class="p">:</span>
<span class="w">   </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;repo.example.com:port&gt;</span>
<span class="w">   </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">dcgm-exporter</span>
<span class="w">   </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2.3.1-2.6.0-ubuntu20.04</span>
<span class="w">   </span><span class="nt">imagePullSecrets</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>

<span class="w"> </span><span class="nt">gfd</span><span class="p">:</span>
<span class="w">   </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;repo.example.com:port&gt;</span>
<span class="w">   </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpu-feature-discovery</span>
<span class="w">   </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v0.4.1</span>
<span class="w">   </span><span class="nt">imagePullSecrets</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>

<span class="w"> </span><span class="nt">nodeStatusExporter</span><span class="p">:</span>
<span class="w">   </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">   </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;repo.example.com:port&gt;</span>
<span class="w">   </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpu-operator-validator</span>
<span class="w">   </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;1.9.0&quot;</span>

<span class="w"> </span><span class="nt">migManager</span><span class="p">:</span>
<span class="w">   </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">   </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;repo.example.com:port&gt;</span>
<span class="w">   </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">k8s-mig-manager</span>
<span class="w">   </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v0.2.0-ubuntu20.04</span>
</pre></div>
</div>
</section>
</section>
<section id="local-package-repository">
<h3>Local Package Repository<a class="headerlink" href="#local-package-repository" title="Permalink to this heading"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">driver</span></code> container deployed as part of the GPU operator requires certain packages to be available as part of the
driver installation. In restricted internet access or air-gapped installations, users are required to create a
local mirror repository for their OS distribution and make the following packages available:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>KERNEL_VERSION is the underlying running kernel version on the GPU node
GCC_VERSION is the gcc version matching the one used for building underlying kernel</p>
</div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">ubuntu</span><span class="p">:</span>
<span class="w">   </span><span class="l l-Scalar l-Scalar-Plain">linux-headers-${KERNEL_VERSION}</span>
<span class="w">   </span><span class="l l-Scalar l-Scalar-Plain">linux-image-${KERNEL_VERSION}</span>
<span class="w">   </span><span class="l l-Scalar l-Scalar-Plain">linux-modules-${KERNEL_VERSION}</span>

<span class="nt">centos</span><span class="p">:</span>
<span class="w">   </span><span class="l l-Scalar l-Scalar-Plain">elfutils-libelf.x86_64</span>
<span class="w">   </span><span class="l l-Scalar l-Scalar-Plain">elfutils-libelf-devel.x86_64</span>
<span class="w">   </span><span class="l l-Scalar l-Scalar-Plain">kernel-headers-${KERNEL_VERSION}</span>
<span class="w">   </span><span class="l l-Scalar l-Scalar-Plain">kernel-devel-${KERNEL_VERSION}</span>
<span class="w">   </span><span class="l l-Scalar l-Scalar-Plain">kernel-core-${KERNEL_VERSION}</span>
<span class="w">   </span><span class="l l-Scalar l-Scalar-Plain">gcc-${GCC_VERSION}</span>

<span class="nt">rhel/rhcos</span><span class="p">:</span>
<span class="w">   </span><span class="l l-Scalar l-Scalar-Plain">kernel-headers-${KERNEL_VERSION}</span>
<span class="w">   </span><span class="l l-Scalar l-Scalar-Plain">kernel-devel-${KERNEL_VERSION}</span>
<span class="w">   </span><span class="l l-Scalar l-Scalar-Plain">kernel-core-${KERNEL_VERSION}</span>
<span class="w">   </span><span class="l l-Scalar l-Scalar-Plain">gcc-${GCC_VERSION}</span>
</pre></div>
</div>
<p>For example, for Ubuntu these packages can be found at <code class="docutils literal notranslate"><span class="pre">archive.ubuntu.com</span></code> so this would be the mirror that
needs to be replicated locally for your cluster. Using <code class="docutils literal notranslate"><span class="pre">apt-mirror</span></code>, these packages will be automatically mirrored
to your local package repository server.</p>
<p>For CentOS, <code class="docutils literal notranslate"><span class="pre">reposync</span></code> can be used to create the local mirror.</p>
<p>Once all above required packages are mirrored to the local repository, repo lists need to be created following
distribution specific documentation. A <code class="docutils literal notranslate"><span class="pre">ConfigMap</span></code> containing the repo list file needs to be created in
the namespace where the GPU Operator gets deployed.</p>
<p>An example of repo list is shown below for Ubuntu 20.04 (access to local package repository via HTTP):</p>
<p><code class="docutils literal notranslate"><span class="pre">custom-repo.list</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">deb</span> <span class="p">[</span><span class="n">arch</span><span class="o">=</span><span class="n">amd64</span><span class="p">]</span> <span class="n">http</span><span class="p">:</span><span class="o">//&lt;</span><span class="n">local</span> <span class="n">pkg</span> <span class="n">repository</span><span class="o">&gt;/</span><span class="n">ubuntu</span><span class="o">/</span><span class="n">mirror</span><span class="o">/</span><span class="n">archive</span><span class="o">.</span><span class="n">ubuntu</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">ubuntu</span> <span class="n">focal</span> <span class="n">main</span> <span class="n">universe</span>
<span class="n">deb</span> <span class="p">[</span><span class="n">arch</span><span class="o">=</span><span class="n">amd64</span><span class="p">]</span> <span class="n">http</span><span class="p">:</span><span class="o">//&lt;</span><span class="n">local</span> <span class="n">pkg</span> <span class="n">repository</span><span class="o">&gt;/</span><span class="n">ubuntu</span><span class="o">/</span><span class="n">mirror</span><span class="o">/</span><span class="n">archive</span><span class="o">.</span><span class="n">ubuntu</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">ubuntu</span> <span class="n">focal</span><span class="o">-</span><span class="n">updates</span> <span class="n">main</span> <span class="n">universe</span>
<span class="n">deb</span> <span class="p">[</span><span class="n">arch</span><span class="o">=</span><span class="n">amd64</span><span class="p">]</span> <span class="n">http</span><span class="p">:</span><span class="o">//&lt;</span><span class="n">local</span> <span class="n">pkg</span> <span class="n">repository</span><span class="o">&gt;/</span><span class="n">ubuntu</span><span class="o">/</span><span class="n">mirror</span><span class="o">/</span><span class="n">archive</span><span class="o">.</span><span class="n">ubuntu</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">ubuntu</span> <span class="n">focal</span><span class="o">-</span><span class="n">security</span> <span class="n">main</span> <span class="n">universe</span>
</pre></div>
</div>
<p>An example of repo list is shown below for CentOS 8 (access to local package repository via HTTP):</p>
<p><code class="docutils literal notranslate"><span class="pre">custom-repo.repo</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[baseos]
name=CentOS Linux $releasever - BaseOS
baseurl=http://&lt;local pkg repository&gt;/repos/centos/$releasever/$basearch/os/baseos/
gpgcheck=0
enabled=1

[appstream]
name=CentOS Linux $releasever - AppStream
baseurl=http://&lt;local pkg repository&gt;/repos/centos/$releasever/$basearch/os/appstream/
gpgcheck=0
enabled=1

[extras]
name=CentOS Linux $releasever - Extras
baseurl=http://&lt;local pkg repository&gt;/repos/centos/$releasever/$basearch/os/extras/
gpgcheck=0
enabled=1
</pre></div>
</div>
<p>Create the <code class="docutils literal notranslate"><span class="pre">ConfigMap</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>create<span class="w"> </span>configmap<span class="w"> </span>repo-config<span class="w"> </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--from-file<span class="o">=</span>&lt;path-to-repo-list-file&gt;
</pre></div>
</div>
<p>Once the ConfigMap is created using the above command, update <code class="docutils literal notranslate"><span class="pre">values.yaml</span></code> with this information, to let the GPU Operator mount the repo configuration
within the <code class="docutils literal notranslate"><span class="pre">driver</span></code> container to pull required packages. Based on the OS distribution the GPU Operator will automatically mount this ConfigMap into the appropriate directory.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">driver</span><span class="p">:</span>
<span class="w">   </span><span class="nt">repoConfig</span><span class="p">:</span>
<span class="w">      </span><span class="nt">configMapName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">repo-config</span>
</pre></div>
</div>
<p>If self-signed certificates are used for an HTTPS based internal repository then a ConfigMap needs to be created for those certs and provide that during the GPU Operator
install. Based on the OS distribution the GPU Operator will automatically mount this ConfigMap into the appropriate directory.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>create<span class="w"> </span>configmap<span class="w"> </span>cert-config<span class="w"> </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--from-file<span class="o">=</span>&lt;path-to-pem-file1&gt;<span class="w"> </span>--from-file<span class="o">=</span>&lt;path-to-pem-file2&gt;
</pre></div>
</div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">driver</span><span class="p">:</span>
<span class="w">   </span><span class="nt">certConfig</span><span class="p">:</span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cert-config</span>
</pre></div>
</div>
</section>
<section id="id2">
<h3>Deploy GPU Operator<a class="headerlink" href="#id2" title="Permalink to this heading"></a></h3>
<p>Download and deploy GPU Operator Helm Chart with the updated <code class="docutils literal notranslate"><span class="pre">values.yaml</span></code>.</p>
<p>Fetch the chart from NGC repository. <code class="docutils literal notranslate"><span class="pre">v1.9.0</span></code> is used in the command below:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>fetch<span class="w"> </span>https://helm.ngc.nvidia.com/nvidia/charts/gpu-operator-v1.9.0.tgz
</pre></div>
</div>
<p>Install the GPU Operator with updated <code class="docutils literal notranslate"><span class="pre">values.yaml</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--create-namespace<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>gpu-operator-v1.9.0.tgz<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-f<span class="w"> </span>values.yaml
</pre></div>
</div>
<p>Check the status of the pods to ensure all the containers are running:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>gpu-operator
</pre></div>
</div>
</section>
</section>
<section id="considerations-when-installing-with-outdated-kernels-in-cluster">
<span id="install-gpu-operator-1-11-0-outdated-kernels"></span><h2>Considerations when Installing with Outdated Kernels in Cluster<a class="headerlink" href="#considerations-when-installing-with-outdated-kernels-in-cluster" title="Permalink to this heading"></a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">driver</span></code> container deployed as part of the GPU Operator requires certain packages to be available as part of the driver installation.
On GPU nodes where the running kernel is not the latest, the <code class="docutils literal notranslate"><span class="pre">driver</span></code> container may fail to find the right version of these packages
(e.g. kernel-headers, kernel-devel) that correspond to the running kernel version. In the <code class="docutils literal notranslate"><span class="pre">driver</span></code> container logs, you will most likely
see the following error message: <code class="docutils literal notranslate"><span class="pre">Could</span> <span class="pre">not</span> <span class="pre">resolve</span> <span class="pre">Linux</span> <span class="pre">kernel</span> <span class="pre">version</span></code>.</p>
<p>In general, upgrading your system to the latest kernel should fix this issue. But if this is not an option, the following is a
workaround to successfully deploy the GPU operator when GPU nodes in your cluster may not be running the latest kernel.</p>
<section id="add-archived-package-repositories">
<h3>Add Archived Package Repositories<a class="headerlink" href="#add-archived-package-repositories" title="Permalink to this heading"></a></h3>
<p>The workaround is to find the package archive containing packages for your outdated kernel and to add this repository to the package
manager running inside the <code class="docutils literal notranslate"><span class="pre">driver</span></code> container. To achieve this, we can simply mount a repository list file into the <code class="docutils literal notranslate"><span class="pre">driver</span></code> container using a <code class="docutils literal notranslate"><span class="pre">ConfigMap</span></code>.
The <code class="docutils literal notranslate"><span class="pre">ConfigMap</span></code> containing the repository list file needs to be created in the <code class="docutils literal notranslate"><span class="pre">gpu-operator</span></code> namespace.</p>
<p>Let us demonstrate this workaround via an example. The system used in this example is running CentOS 7 with an outdated kernel:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>uname<span class="w"> </span>-r
<span class="go">3.10.0-1062.12.1.el7.x86_64</span>
</pre></div>
</div>
<p>The official archive for older CentOS packages is <a class="reference external" href="https://vault.centos.org/">https://vault.centos.org/</a>. Typically, most archived CentOS repositories
are found in <code class="docutils literal notranslate"><span class="pre">/etc/yum.repos.d/CentOS-Vault.repo</span></code> but they are disabled by default. If the appropriate archive repository
was enabled, then the <code class="docutils literal notranslate"><span class="pre">driver</span></code> container would resolve the kernel version and be able to install the correct versions
of the prerequisite packages.</p>
<p>We can simply drop in a replacement of <code class="docutils literal notranslate"><span class="pre">/etc/yum.repos.d/CentOS-Vault.repo</span></code> to ensure the appropriate CentOS archive is enabled.
For the kernel running in this example, the <code class="docutils literal notranslate"><span class="pre">CentOS-7.7.1908</span></code> archive contains the kernel-headers version we are looking for.
Here is our example drop-in replacement file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[C7.7.1908-base]
name=CentOS-7.7.1908 - Base
baseurl=http://vault.centos.org/7.7.1908/os/$basearch/
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7
enabled=1

[C7.7.1908-updates]
name=CentOS-7.7.1908 - Updates
baseurl=http://vault.centos.org/7.7.1908/updates/$basearch/
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7
enabled=1
</pre></div>
</div>
<p>Once the repo list file is created, we can create a <code class="docutils literal notranslate"><span class="pre">ConfigMap</span></code> for it:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>create<span class="w"> </span>configmap<span class="w"> </span>repo-config<span class="w"> </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--from-file<span class="o">=</span>&lt;path-to-repo-list-file&gt;
</pre></div>
</div>
<p>Once the <code class="docutils literal notranslate"><span class="pre">ConfigMap</span></code> is created using the above command, update <code class="docutils literal notranslate"><span class="pre">values.yaml</span></code> with this information, to let the GPU Operator mount the repo configuration
within the <code class="docutils literal notranslate"><span class="pre">driver</span></code> container to pull required packages.</p>
<p>For Ubuntu:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">driver</span><span class="p">:</span>
<span class="w">   </span><span class="nt">repoConfig</span><span class="p">:</span>
<span class="w">      </span><span class="nt">configMapName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">repo-config</span>
<span class="w">      </span><span class="nt">destinationDir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/etc/apt/sources.list.d</span>
</pre></div>
</div>
<p>For RHEL/Centos/RHCOS:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">driver</span><span class="p">:</span>
<span class="w">   </span><span class="nt">repoConfig</span><span class="p">:</span>
<span class="w">      </span><span class="nt">configMapName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">repo-config</span>
<span class="w">      </span><span class="nt">destinationDir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/etc/yum.repos.d</span>
</pre></div>
</div>
<p>Deploy GPU Operator with updated <code class="docutils literal notranslate"><span class="pre">values.yaml</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--create-namespace<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>nvidia/gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-f<span class="w"> </span>values.yaml
</pre></div>
</div>
<p>Check the status of the pods to ensure all the containers are running:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>gpu-operator
</pre></div>
</div>
</section>
</section>
<section id="installing-precompiled-and-canonical-signed-drivers-on-ubuntu20-04">
<span id="install-precompiled-signed-drivers-1-11-0"></span><h2>Installing Precompiled and Canonical Signed Drivers on Ubuntu20.04<a class="headerlink" href="#installing-precompiled-and-canonical-signed-drivers-on-ubuntu20-04" title="Permalink to this heading"></a></h2>
<p>GPU Operator supports deploying NVIDIA precompiled and signed drivers from Canonical on Ubuntu20.04. This is required
when nodes are enabled with Secure Boot. In order to use these, GPU Operator needs to be installed with options <code class="docutils literal notranslate"><span class="pre">--set</span> <span class="pre">driver.version=&lt;DRIVER_BRANCH&gt;-signed</span></code>.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--create-namespace<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>nvidia/gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>driver.version<span class="o">=</span>&lt;DRIVER_BRANCH&gt;-signed
</pre></div>
</div>
<p>supported DRIVER_BRANCH value currently are <code class="docutils literal notranslate"><span class="pre">470</span></code> and <code class="docutils literal notranslate"><span class="pre">510</span></code> which will install latest drivers available on that branch for current running
kernel version.</p>
<p>Following are the packages used in this case by the driver container.</p>
<ul class="simple">
<li><p>linux-objects-nvidia-${DRIVER_BRANCH}-server-${KERNEL_VERSION} - Linux kernel nvidia modules.</p></li>
<li><p>linux-signatures-nvidia-${KERNEL_VERSION} - Linux kernel signatures for nvidia modules.</p></li>
<li><p>linux-modules-nvidia-${DRIVER_BRANCH}-server-${KERNEL_VERSION} - Meta package for nvidia driver modules, signatures and kernel interfaces.</p></li>
<li><p>nvidia-utils-${DRIVER_BRANCH}-server - NVIDIA driver support binaries.</p></li>
<li><p>nvidia-compute-utils-${DRIVER_BRANCH}-server - NVIDIA compute utilities (includes nvidia-persistenced).</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Before upgrading kernel on the worker nodes please ensure that above packages are available for that kernel version, else upgrade will
cause driver installation failures.</p></li>
</ul>
</div>
</section>
<section id="running-kubevirt-vms-with-the-gpu-operator">
<span id="gpu-operator-kubevirt-1-11-0"></span><h2>Running KubeVirt VMs with the GPU Operator<a class="headerlink" href="#running-kubevirt-vms-with-the-gpu-operator" title="Permalink to this heading"></a></h2>
<section id="gpu-operator-kubevirt-1-11-0-introduction">
<span id="id3"></span><h3>Introduction<a class="headerlink" href="#gpu-operator-kubevirt-1-11-0-introduction" title="Permalink to this heading"></a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This feature is introduced as a technical preview with the GPU Operator 1.11.0 release. This is not ready for production use. Please submit feedback and bug reports <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues">here</a>. We encourage contributions in our <a class="reference external" href="https://gitlab.com/nvidia/kubernetes/gpu-operator">Gitlab repository</a>.</p>
</div>
<p><a class="reference external" href="https://kubevirt.io/">KubeVirt</a> is a virtual machine management add-on to Kubernetes that allows you to run and manage VMs in a Kubernetes cluster. It eliminates the need to manage separate clusters for VM and container workloads, as both can now coexist in a single Kubernetes cluster.</p>
<p>Up until this point, the GPU Operator only provisioned worker nodes for running GPU-accelerated containers. Now, the GPU Operator can also be used to provision worker nodes for running GPU-accelerated VMs.</p>
<p>The prerequisites needed for running containers and VMs with GPU(s) differs, with the primary difference being the drivers required. For example, the datacenter driver is needed for containers, the vfio-pci driver is needed for GPU passthrough, and the <a class="reference external" href="https://docs.nvidia.com/grid/latest/grid-vgpu-user-guide/index.html#installing-configuring-grid-vgpu">NVIDIA vGPU Manager</a> is needed for creating vGPU devices.</p>
<p>The GPU Operator can now be configured to deploy different software components on worker nodes depending on what GPU workload is configured to run on those nodes. Consider the following example.</p>
<div class="line-block">
<div class="line">Node A is configured to run containers.</div>
<div class="line">Node B is configured to run VMs with Passthrough GPU.</div>
<div class="line">Node C is configured to run VMs with vGPU.</div>
</div>
<p>Node A receives the following software components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Datacenter</span> <span class="pre">Driver</span></code> - to install the driver</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Container</span> <span class="pre">Toolkit</span></code> - to ensure containers can properly access GPUs</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Kubernetes</span> <span class="pre">Device</span> <span class="pre">Plugin</span></code> - to discover and advertise GPU resources to kubelet</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">DCGM</span> <span class="pre">and</span> <span class="pre">DCGM</span> <span class="pre">Exporter</span></code> - to monitor the GPU(s)</p></li>
</ul>
<p>Node B receives the following software components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">VFIO</span> <span class="pre">Manager</span></code> - to load <cite>vfio-pci</cite> and bind it to all GPUs on the node</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sandbox</span> <span class="pre">Device</span> <span class="pre">Plugin</span></code> - to discover and advertise the passthrough GPUs to kubelet</p></li>
</ul>
<p>Node C receives the following software components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">vGPU</span> <span class="pre">Manager</span></code> - to install the driver</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">vGPU</span> <span class="pre">Device</span> <span class="pre">Manager</span></code> - to create vGPU devices on the node</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sandbox</span> <span class="pre">Device</span> <span class="pre">Plugin</span></code> - to discover and advertise the vGPU devices to kubelet</p></li>
</ul>
</section>
<section id="limitations">
<span id="gpu-operator-kubevirt-1-11-0-limitations"></span><h3>Limitations<a class="headerlink" href="#limitations" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>This feature is a Technical Preview and is not ready for production use.</p></li>
<li><p>Trying out this feature requires a fresh install of GPU Operator 1.11 with necessary fields set in ClusterPolicy as detailed in this document. The instructions in this document are not valid if upgrading from 1.10 to 1.11.</p></li>
<li><p>Enabling / disabling this feature post-install is not supported.</p></li>
<li><p>MIG-backed vGPUs are not supported.</p></li>
<li><p>A GPU worker node can run GPU workloads of a particular type - containers, VMs with GPU Passthrough, and VMs with vGPU - but not a combination of any of them.</p></li>
</ul>
</section>
<section id="install-the-gpu-operator">
<h3>Install the GPU Operator<a class="headerlink" href="#install-the-gpu-operator" title="Permalink to this heading"></a></h3>
<p>To enable this functionality, install the GPU Operator and set the following parameter in <code class="docutils literal notranslate"><span class="pre">ClusterPolicy</span></code>: <code class="docutils literal notranslate"><span class="pre">sandboxWorkloads.enabled=true</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The term <code class="docutils literal notranslate"><span class="pre">sandboxing</span></code> refers to running software in a separate isolated environment, typically for added security (i.e. a virtual machine). We use the term <code class="docutils literal notranslate"><span class="pre">sandbox</span> <span class="pre">workloads</span></code> to signify workloads that run in a virtual machine, irrespective of the virtualization technology used.</p>
</div>
</section>
<section id="partition-cluster-based-the-gpu-workload">
<h3>Partition Cluster based the GPU Workload<a class="headerlink" href="#partition-cluster-based-the-gpu-workload" title="Permalink to this heading"></a></h3>
<p>When sandbox workloads are enabled (<code class="docutils literal notranslate"><span class="pre">sandboxWorkloads.enabled=true</span></code>), a worker node can run GPU workloads of a particular type – containers, VMs with GPU passthrough, or VMs with vGPU –  but not a combination of any of them. As illustrated in the <a class="reference internal" href="gpu-operator-kubevirt.html#gpu-operator-kubevirt-1-11-0-introduction"><span class="std std-ref">Introduction</span></a>, the GPU Operator will deploy a specific set of operands on a worker node depending on the workload type configured. For example, a node which is configured to run containers will receive the <code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Datacenter</span> <span class="pre">Driver</span></code>, while a node which is configured to run VMs with vGPU will receive the <code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">vGPU</span> <span class="pre">Manager</span></code>.</p>
<p>To set the GPU workload configuration for a worker node, apply the node label <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.workload.config=&lt;config&gt;</span></code>, where the valid config values are <code class="docutils literal notranslate"><span class="pre">container</span></code>, <code class="docutils literal notranslate"><span class="pre">vm-passthrough</span></code>, and <code class="docutils literal notranslate"><span class="pre">vm-vgpu</span></code>.</p>
<p>If the node label <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.workload.config</span></code> does not exist on the node, the GPU Operator will assume the default GPU workload configuration, <code class="docutils literal notranslate"><span class="pre">container</span></code>. To override the default GPU workload configuration, set the following value in <code class="docutils literal notranslate"><span class="pre">ClusterPolicy</span></code> during install: <code class="docutils literal notranslate"><span class="pre">sandboxWorkloads.defaultWorkload=&lt;config&gt;</span></code>.</p>
<p>Consider the following example:</p>
<p>GPU Operator is installed with the following options: <code class="docutils literal notranslate"><span class="pre">sandboxWorkloads.enabled=true</span> <span class="pre">sandboxWorkloads.defaultWorkload=container</span></code></p>
<div class="line-block">
<div class="line">Node A is <cite>not</cite> labeled with <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.workload.config</span></code>.</div>
<div class="line">Node B is labeled with <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.workload.config=vm-passthrough</span></code>.</div>
<div class="line">Node C is labeled with <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.workload.config=vm-vgpu</span></code>.</div>
</div>
<div class="line-block">
<div class="line">Node A gets provisioned for containers.</div>
<div class="line">Node B gets provisioned for GPU Passthrough.</div>
<div class="line">Node C gets provisioned for vGPU.</div>
</div>
</section>
<section id="deployment-scenarios">
<h3>Deployment Scenarios<a class="headerlink" href="#deployment-scenarios" title="Permalink to this heading"></a></h3>
<section id="running-vms-with-gpu-passthrough">
<h4>Running VMs with GPU Passthrough<a class="headerlink" href="#running-vms-with-gpu-passthrough" title="Permalink to this heading"></a></h4>
<p>This section runs through the deployment scenario of running VMs with GPU Passthrough. We will first deploy the GPU Operator, such that our worker node will be provisioned for GPU Passthrough, then we will deploy a KubeVirt VM which requests a GPU.</p>
<p>By default, to provision GPU Passthrough, the GPU Operator will deploy the following components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">VFIO</span> <span class="pre">Manager</span></code> - to load <code class="docutils literal notranslate"><span class="pre">vfio-pci</span></code> and bind it to all GPUs on the node</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sandbox</span> <span class="pre">Device</span> <span class="pre">Plugin</span></code> - to discover and advertise the passthrough GPUs to kubelet</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sandbox</span> <span class="pre">Validator</span></code> - to validate the other operands</p></li>
</ul>
<section id="id5">
<h5>Install the GPU Operator<a class="headerlink" href="#id5" title="Permalink to this heading"></a></h5>
<p>Follow the below steps.</p>
<p>Label the worker node explicitly for GPU passthrough workloads:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>label<span class="w"> </span>node<span class="w"> </span>&lt;node-name&gt;<span class="w"> </span>--overwrite<span class="w"> </span>nvidia.com/gpu.workload.config<span class="o">=</span>vm-passthrough
</pre></div>
</div>
<p>Install the GPU Operator with sandbox workloads enabled:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>gpu-operator<span class="w"> </span>nvidia/gpu-operator<span class="w"> </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>–set<span class="w"> </span>sandboxWorkloads.enabled<span class="o">=</span><span class="nb">true</span>
</pre></div>
</div>
<p>The following operands get deployed. Ensure all pods are in a running state and all validations succeed with the <code class="docutils literal notranslate"><span class="pre">sandbox-validator</span></code> component:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>gpu-operator
<span class="go">NAME                                                          READY   STATUS    RESTARTS   AGE</span>
<span class="go">...</span>
<span class="go">nvidia-sandbox-device-plugin-daemonset-4mxsc                  1/1     Running   0          40s</span>
<span class="go">nvidia-sandbox-validator-vxj7t                                1/1     Running   0          40s</span>
<span class="go">nvidia-vfio-manager-thfwf                                     1/1     Running   0          78s</span>
</pre></div>
</div>
<p>The vfio-manager pod will bind all GPUs on the node to the vfio-pci driver:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>lspci<span class="w"> </span>--nnk<span class="w"> </span>-d<span class="w"> </span>10de:
<span class="go">3b:00.0 3D controller [0302]: NVIDIA Corporation Device [10de:2236] (rev a1)</span>
<span class="go">       Subsystem: NVIDIA Corporation Device [10de:1482]</span>
<span class="go">       Kernel driver in use: vfio-pci</span>
<span class="go">       Kernel modules: nvidiafb, nouveau</span>
<span class="go">86:00.0 3D controller [0302]: NVIDIA Corporation Device [10de:2236] (rev a1)</span>
<span class="go">       Subsystem: NVIDIA Corporation Device [10de:1482]</span>
<span class="go">       Kernel driver in use: vfio-pci</span>
<span class="go">       Kernel modules: nvidiafb, nouveau</span>
</pre></div>
</div>
<p>The sandbox-device-plugin will discover and advertise these resources to kubelet. In this example, we have two A10 GPUs:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>describe<span class="w"> </span>node<span class="w"> </span>&lt;node-name&gt;
<span class="go">...</span>
<span class="go">Capacity:</span>
<span class="go">  ...</span>
<span class="go">  nvidia.com/GA102GL_A10:         2</span>
<span class="go">  ...</span>
<span class="go">Allocatable:</span>
<span class="go">  ...</span>
<span class="go">  nvidia.com/GA102GL_A10:         2</span>
<span class="go">...</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The resource name is currently constructed by joining the <cite>device</cite> and <cite>device_name</cite> columns from the <a class="reference external" href="https://pci-ids.ucw.cz/v2.2/pci.ids">PCI IDs database</a>. For example, the entry for A10 in the database reads <code class="docutils literal notranslate"><span class="pre">2236</span>&#160; <span class="pre">GA102GL</span> <span class="pre">[A10]</span></code>, which results in a resource name <code class="docutils literal notranslate"><span class="pre">nvidia.com/GA102GL_A10</span></code>.</p>
</div>
</section>
<section id="update-the-kubevirt-cr">
<h5>Update the KubeVirt CR<a class="headerlink" href="#update-the-kubevirt-cr" title="Permalink to this heading"></a></h5>
<p>Next, we will update the KubeVirt Custom Resource, as documented in the <a class="reference external" href="https://kubevirt.io/user-guide/virtual_machines/host-devices/#listing-permitted-devices">KubeVirt user guide</a>, so that the passthrough GPUs are permitted and can be requested by a KubeVirt VM. Note, replace the values for <code class="docutils literal notranslate"><span class="pre">pciVendorSelector</span></code> and <code class="docutils literal notranslate"><span class="pre">resourceName</span></code> to correspond to your GPU model. We set <code class="docutils literal notranslate"><span class="pre">externalResourceProvider=true</span></code> to indicate that this resource is being provided by an external device plugin, in this case the <code class="docutils literal notranslate"><span class="pre">sandbox-device-plugin</span></code> which is deployed by the Operator.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To find the device ID for a particular GPU, search by device name in the <a class="reference external" href="https://pci-ids.ucw.cz/v2.2/pci.ids">PCI IDs database</a>.</p>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>edit<span class="w"> </span>kubevirt<span class="w"> </span>-n<span class="w"> </span>kubevirt
<span class="go">  ...</span>
<span class="go">  spec:</span>
<span class="go">    configuration:</span>
<span class="go">    developerConfiguration:</span>
<span class="go">      featureGates:</span>
<span class="go">      - GPU</span>
<span class="go">    permittedHostDevices:</span>
<span class="go">      pciHostDevices:</span>
<span class="go">      - externalResourceProvider: true</span>
<span class="go">        pciVendorSelector: 10DE:2236</span>
<span class="go">        resourceName: nvidia.com/GA102GL_A10</span>
<span class="go">  ...</span>
</pre></div>
</div>
</section>
<section id="create-a-vm">
<h5>Create a VM<a class="headerlink" href="#create-a-vm" title="Permalink to this heading"></a></h5>
<p>We are now ready to create a VM. Let’s create a sample VM using a simple VMI spec which requests a nvidia.com/GA102GL_A10 resource:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">---</span>
<span class="go">apiVersion: kubevirt.io/v1alpha3</span>
<span class="go">kind: VirtualMachineInstance</span>
<span class="go">metadata:</span>
<span class="go">  labels:</span>
<span class="go">    special: vmi-gpu</span>
<span class="go">  name: vmi-gpu</span>
<span class="go">spec:</span>
<span class="go">  domain:</span>
<span class="go">    devices:</span>
<span class="go">      disks:</span>
<span class="go">      - disk:</span>
<span class="go">          bus: virtio</span>
<span class="go">        name: containerdisk</span>
<span class="go">      - disk:</span>
<span class="go">          bus: virtio</span>
<span class="go">        name: cloudinitdisk</span>
<span class="go">      gpus:</span>
<span class="go">      - deviceName: nvidia.com/GA102GL_A10</span>
<span class="go">        name: gpu1</span>
<span class="go">      rng: {}</span>
<span class="go">    machine:</span>
<span class="go">      type: &quot;&quot;</span>
<span class="go">    resources:</span>
<span class="go">      requests:</span>
<span class="go">        memory: 1024M</span>
<span class="go">  terminationGracePeriodSeconds: 0</span>
<span class="go">  volumes:</span>
<span class="go">  - containerDisk:</span>
<span class="go">      image: docker.io/kubevirt/fedora-cloud-container-disk-demo:devel</span>
<span class="go">    name: containerdisk</span>
<span class="go">  - cloudInitNoCloud:</span>
<span class="go">      userData: |-</span>
<span class="gp">        #</span>cloud-config
<span class="go">        password: fedora</span>
<span class="go">        chpasswd: { expire: False }</span>
<span class="go">    name: cloudinitdisk</span>
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>vmi-gpu.yaml
<span class="go">virtualmachineinstance.kubevirt.io/vmi-gpu created</span>

<span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>vmis
<span class="go">NAME      AGE   PHASE     IP               NODENAME       READY</span>
<span class="go">vmi-gpu   13s   Running   192.168.47.210   cnt-server-2   True</span>
</pre></div>
</div>
<p>Let’s console into the VM and verify we have a GPU. Refer <a class="reference external" href="https://kubevirt.io/user-guide/operations/virtctl_client_tool/">here</a> for installing virtctl.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>./virtctl<span class="w"> </span>console<span class="w"> </span>vmi-gpu
<span class="go">Successfully connected to vmi-gpu console. The escape sequence is ^]</span>

<span class="go">vmi-gpu login: fedora</span>
<span class="go">Password:</span>
<span class="gp">[fedora@vmi-gpu ~]$ </span>sudo<span class="w"> </span>yum<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>-q<span class="w"> </span>pciutils
<span class="gp">[fedora@vmi-gpu ~]$ </span>lspci<span class="w"> </span>-nnk<span class="w"> </span>-d<span class="w"> </span>10de:
<span class="go">06:00.0 3D controller [0302]: NVIDIA Corporation GA102GL [A10] [10de:2236] (rev a1)</span>
<span class="go">       Subsystem: NVIDIA Corporation Device [10de:1482]</span>
</pre></div>
</div>
</section>
</section>
<section id="running-vms-with-vgpu">
<h4>Running VMs with vGPU<a class="headerlink" href="#running-vms-with-vgpu" title="Permalink to this heading"></a></h4>
<p>This section runs through the deployment scenario of running VMs with vGPU. We will first deploy the GPU Operator, such that our worker node will be provisioned for vGPU, then we will deploy a KubeVirt VM which requests a vGPU.</p>
<p>By default, to provision vGPU, the GPU Operator will deploy the following components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">vGPU</span> <span class="pre">Manager</span></code> - installs vGPU Manager on the node</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">vGPU</span> <span class="pre">Device</span> <span class="pre">Manager</span></code> - creates vGPU devices on the node after vGPU Manager is installed</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sandbox</span> <span class="pre">Device</span> <span class="pre">Plugin</span></code> - to discover and advertise the vGPU devices to kubelet</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sandbox</span> <span class="pre">Validator</span></code> - to validate the other operands</p></li>
</ul>
<section id="build-the-vgpu-manager-image">
<h5>Build the vGPU Manager Image<a class="headerlink" href="#build-the-vgpu-manager-image" title="Permalink to this heading"></a></h5>
<p>Building the vGPU Manager container and pushing it to a private registry is a prerequisite. To fulfill this prerequisite, follow the below steps.</p>
<p>Download the vGPU Software from the <a class="reference external" href="https://nvid.nvidia.com/dashboard/#/dashboard">NVIDIA Licensing Portal</a>.</p>
<ul class="simple">
<li><p>Login to the NVIDIA Licensing Portal and navigate to the <cite>Software Downloads</cite> section.</p></li>
<li><p>The NVIDIA vGPU Software is located in the Software Downloads section of the NVIDIA Licensing Portal.</p></li>
<li><p>The vGPU Software bundle is packaged as a zip file. Download and unzip the bundle to obtain the NVIDIA vGPU Manager for Linux (<code class="docutils literal notranslate"><span class="pre">NVIDIA-Linux-x86_64-&lt;version&gt;-vgpu-kvm.run</span></code> file)</p></li>
</ul>
<p>Next, clone the driver container repository and build the driver image with the following steps.</p>
<p>Open a terminal and clone the driver container image repository.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>git<span class="w"> </span>clone<span class="w"> </span>https://gitlab.com/nvidia/container-images/driver
<span class="gp">$ </span><span class="nb">cd</span><span class="w"> </span>driver
</pre></div>
</div>
<p>Change to the vgpu-manager directory for your OS. We use Ubuntu 20.04 as an example.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nb">cd</span><span class="w"> </span>vgpu-manager/ubuntu20.04
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For RedHat OpenShift, run <code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">vgpu-manager/rhel</span></code> to use the <code class="docutils literal notranslate"><span class="pre">rhel</span></code> folder instead.</p>
</div>
<p>Copy the NVIDIA vGPU Manager from your extracted zip file</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cp<span class="w"> </span>&lt;local-driver-download-directory&gt;/*-vgpu-kvm.run<span class="w"> </span>./
</pre></div>
</div>
<div class="line-block">
<div class="line">Set the following environment variables:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">PRIVATE_REGISTRY</span></code> - name of private registry used to store driver image</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">VERSION</span></code> - NVIDIA vGPU Manager version downloaded from NVIDIA Software Portal</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">OS_TAG</span></code> - this must match the Guest OS version. In the below example <code class="docutils literal notranslate"><span class="pre">ubuntu20.04</span></code> is used. For RedHat OpenShift this should be set to <code class="docutils literal notranslate"><span class="pre">rhcos4.x</span></code> where x is the supported minor OCP version.</div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nb">export</span><span class="w"> </span><span class="nv">PRIVATE_REGISTRY</span><span class="o">=</span>my/private/registry<span class="w"> </span><span class="nv">VERSION</span><span class="o">=</span><span class="m">510</span>.73.06<span class="w"> </span><span class="nv">OS_TAG</span><span class="o">=</span>ubuntu20.04
</pre></div>
</div>
<p>Build the NVIDIA vGPU Manager image.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>build<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>–build-arg<span class="w"> </span><span class="nv">DRIVER_VERSION</span><span class="o">=</span><span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-t<span class="w"> </span><span class="si">${</span><span class="nv">PRIVATE_REGISTRY</span><span class="si">}</span>/vgpu-manager:<span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span>-<span class="si">${</span><span class="nv">OS_TAG</span><span class="si">}</span><span class="w"> </span>.
</pre></div>
</div>
<p>Push NVIDIA vGPU Manager image to your private registry.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>push<span class="w"> </span><span class="si">${</span><span class="nv">PRIVATE_REGISTRY</span><span class="si">}</span>/vgpu-manager:<span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span>-<span class="si">${</span><span class="nv">OS_TAG</span><span class="si">}</span>
</pre></div>
</div>
</section>
<section id="id8">
<h5>Install the GPU Operator<a class="headerlink" href="#id8" title="Permalink to this heading"></a></h5>
<p>Follow the below steps.</p>
<p>Label the worker node explicitly for vGPU workloads:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>label<span class="w"> </span>node<span class="w"> </span>&lt;node-name&gt;<span class="w"> </span>--overwrite<span class="w"> </span>nvidia.com/gpu.workload.config<span class="o">=</span>vm-vgpu
</pre></div>
</div>
<p>Create a configuration file named <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> for the vGPU Device Manager.  This file contains a list of vGPU device configurations. Each named configuration contains a list of desired vGPU types. The vGPU Device Manager reads the configuration file and applies a specific named configuration when creating vGPU devices on the node. Download the comprehensive example file as a starting point, and modify as needed:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>wget<span class="w"> </span>-O<span class="w"> </span>config.yaml<span class="w"> </span>https://raw.githubusercontent.com/NVIDIA/vgpu-device-manager/main/examples/config-example.yaml
</pre></div>
</div>
<p>Optionally, label the worker node explicitly with a vGPU devices config. More information on vGPU devices config is detailed in <a class="reference internal" href="gpu-operator-kubevirt.html#apply-new-vgpu-device-config-1-11-0"><span class="std std-ref">this section</span></a> below.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>label<span class="w"> </span>node<span class="w"> </span>&lt;node-name&gt;<span class="w"> </span>--overwrite<span class="w"> </span>nvidia.com/vgpu.config<span class="o">=</span>&lt;config-name&gt;
</pre></div>
</div>
<p>Create a namespace for GPU Operator:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>create<span class="w"> </span>namespace<span class="w"> </span>gpu-operator
</pre></div>
</div>
<p>Create a ConfigMap for the vGPU devices config:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>create<span class="w"> </span>cm<span class="w"> </span>vgpu-devices-config<span class="w"> </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>–from-file<span class="o">=</span>config.yaml
</pre></div>
</div>
<p>Install the GPU Operator with sandbox workloads enabled and specify the vGPU Manager image built previously:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>gpu-operator<span class="w"> </span>nvidia/gpu-operator<span class="w"> </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>–set<span class="w"> </span>sandboxWorkloads.enabled<span class="o">=</span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>–set<span class="w"> </span>vgpuManager.repository<span class="o">=</span>&lt;path<span class="w"> </span>to<span class="w"> </span>private<span class="w"> </span>repository&gt;
<span class="go">    –set vgpuManager.image=vgpu-manager</span>
<span class="go">    –set vgpuManager.version=&lt;driver version&gt;</span>
</pre></div>
</div>
<p>The following operands get deployed. Ensure all pods are in a running state and all validations succeed with the <code class="docutils literal notranslate"><span class="pre">sandbox-validator</span></code> component.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>gpu-operator
<span class="go">NAME                                                          READY   STATUS    RESTARTS   AGE</span>
<span class="go">...</span>
<span class="go">nvidia-sandbox-device-plugin-daemonset-kkdt9                  1/1     Running   0          9s</span>
<span class="go">nvidia-sandbox-validator-jcpgw                                1/1     Running   0          9s</span>
<span class="go">nvidia-vgpu-device-manager-8mgg8                              1/1     Running   0          89s</span>
<span class="go">nvidia-vgpu-manager-daemonset-fpplc                           1/1     Running   0          2m41s</span>
</pre></div>
</div>
<p>This worker node has two A10 GPUs. Assuming the node has not been labeled explicitly with <code class="docutils literal notranslate"><span class="pre">nvidia.com/vgpu.config=&lt;config-name&gt;</span></code>, the <code class="docutils literal notranslate"><span class="pre">default</span></code> configuration will be used. And since the <code class="docutils literal notranslate"><span class="pre">default</span></code> configuration in the vgpu-devices-config only lists the <strong>A10-24C</strong> vGPU type for the A10 GPU, the vgpu-device-manager should only create vGPU devices on this type.</p>
<p><strong>A10-24C</strong> is the largest vGPU type supported on the A10 GPU, and only one vGPU device can be created per physical GPU. We should see two vGPU devices created:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ls<span class="w"> </span>-l<span class="w"> </span>/sys/bus/mdev/devices
<span class="go">total 0</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:18 9adc60ea-98a7-41b6-b17b-9b3e0d210c7a -&gt; ../../../devices/pci0000:85/0000:85:02.0/0000:86:00.4/9adc60ea-98a7-41b6-b17b-9b3e0d210c7a</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:18 f9033b86-ccee-454b-8b20-dd7912d95bfd -&gt; ../../../devices/pci0000:3a/0000:3a:00.0/0000:3b:00.4/f9033b86-ccee-454b-8b20-dd7912d95bfd</span>
</pre></div>
</div>
<p>The sandbox-device-plugin will discover and advertise these resources to kubelet. In this example, we have two A10 GPUs and therefore two <strong>A10-24C</strong> vGPU devices.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>describe<span class="w"> </span>node
<span class="go">...</span>
<span class="go">Capacity:</span>
<span class="go">  ...</span>
<span class="go">  nvidia.com/NVIDIA_A10-24C:      2</span>
<span class="go">  ...</span>
<span class="go">Allocatable:</span>
<span class="go">  ...</span>
<span class="go">  nvidia.com/NVIDIA_A10-24C:      2</span>
<span class="go">...</span>
</pre></div>
</div>
</section>
<section id="id9">
<h5>Update the KubeVirt CR<a class="headerlink" href="#id9" title="Permalink to this heading"></a></h5>
<p>Next, we will update the KubeVirt Custom Resource, as documented in the <a class="reference external" href="https://kubevirt.io/user-guide/virtual_machines/host-devices/#listing-permitted-devices">KubeVirt user guide</a>, so that these vGPU devices are permitted and can be requested by a KubeVirt VM. Note, replace the values for <code class="docutils literal notranslate"><span class="pre">mdevNameSelector</span></code> and <code class="docutils literal notranslate"><span class="pre">resourceName</span></code> to correspond to your vGPU type. We set <code class="docutils literal notranslate"><span class="pre">externalResourceProvider=true</span></code> to indicate that this resource is being provided by an external device plugin, in this case the sandbox-device-plugin which is deployed by the Operator.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>edit<span class="w"> </span>kubevirt<span class="w"> </span>-n<span class="w"> </span>kubevirt
<span class="go">...</span>
<span class="go">spec:</span>
<span class="go">  certificateRotateStrategy: {}</span>
<span class="go">  configuration:</span>
<span class="go">    developerConfiguration:</span>
<span class="go">      featureGates:</span>
<span class="go">      - GPU</span>
<span class="go">    permittedHostDevices:</span>
<span class="go">      mediatedDevices:</span>
<span class="go">      - externalResourceProvider: true</span>
<span class="go">        mdevNameSelector: NVIDIA A10-24C</span>
<span class="go">        resourceName: nvidia.com/NVIDIA_A10-24C</span>
<span class="go">...</span>
</pre></div>
</div>
<p>We are now ready to create a VM. Let’s create a sample VM using a simple VMI spec which requests a <code class="docutils literal notranslate"><span class="pre">nvidia.com/NVIDIA_A10-24C</span></code> resource:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat<span class="w"> </span>vmi-vgpu.yaml
<span class="go">---</span>
<span class="go">apiVersion: kubevirt.io/v1alpha3</span>
<span class="go">kind: VirtualMachineInstance</span>
<span class="go">metadata:</span>
<span class="go">  labels:</span>
<span class="go">    special: vmi-vgpu</span>
<span class="go">  name: vmi-vgpu</span>
<span class="go">spec:</span>
<span class="go">  domain:</span>
<span class="go">    devices:</span>
<span class="go">      disks:</span>
<span class="go">      - disk:</span>
<span class="go">          bus: virtio</span>
<span class="go">        name: containerdisk</span>
<span class="go">      - disk:</span>
<span class="go">          bus: virtio</span>
<span class="go">        name: cloudinitdisk</span>
<span class="go">      gpus:</span>
<span class="go">      - deviceName: nvidia.com/NVIDIA_A10-24C</span>
<span class="go">        name: vgpu1</span>
<span class="go">      rng: {}</span>
<span class="go">    machine:</span>
<span class="go">      type: &quot;&quot;</span>
<span class="go">    resources:</span>
<span class="go">      requests:</span>
<span class="go">        memory: 1024M</span>
<span class="go">  terminationGracePeriodSeconds: 0</span>
<span class="go">  volumes:</span>
<span class="go">  - containerDisk:</span>
<span class="go">      image: docker.io/kubevirt/fedora-cloud-container-disk-demo:devel</span>
<span class="go">    name: containerdisk</span>
<span class="go">  - cloudInitNoCloud:</span>
<span class="go">      userData: |-</span>
<span class="gp">        #</span>cloud-config
<span class="go">        password: fedora</span>
<span class="go">        chpasswd: { expire: False }</span>
<span class="go">    name: cloudinitdisk</span>
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>vmi-vgpu.yaml
<span class="go">virtualmachineinstance.kubevirt.io/vmi-vgpu created</span>

<span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>vmis
<span class="go">NAME       AGE   PHASE     IP               NODENAME       READY</span>
<span class="go">vmi-vgpu   10s   Running   192.168.47.205   cnt-server-2   True</span>
</pre></div>
</div>
<p>Let’s console into the VM and verify we have a GPU. Refer <a class="reference external" href="https://docs.google.com/document/d/1mH08JNe8nj5SRKzg8llttzMJbJbDbDaLji07BG6P1c4/edit#heading=h.hwxorb7idly9">here</a> for installing virtctl.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>./virtctl<span class="w"> </span>console<span class="w"> </span>vmi-vgpu
<span class="go">Successfully connected to vmi-vgpu console. The escape sequence is ^]</span>

<span class="go">vmi-vgpu login: fedora</span>
<span class="go">Password:</span>
<span class="gp">[fedora@vmi-vgpu ~]$ </span>sudo<span class="w"> </span>yum<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>-q<span class="w"> </span>pciutils
<span class="gp">[fedora@vmi-vgpu ~]$ </span>lspci<span class="w"> </span>-nnk<span class="w"> </span>-d<span class="w"> </span>10de:
<span class="go">06:00.0 3D controller [0302]: NVIDIA Corporation GA102GL [A10] [10de:2236] (rev a1)</span>
<span class="go">       Subsystem: NVIDIA Corporation Device [10de:14d4]</span>
</pre></div>
</div>
</section>
<section id="apply-a-new-vgpu-device-configuration">
<span id="apply-new-vgpu-device-config-1-11-0"></span><h5>Apply a New vGPU Device Configuration<a class="headerlink" href="#apply-a-new-vgpu-device-configuration" title="Permalink to this heading"></a></h5>
<p>We can apply a specific vGPU device configuration on a per-node basis by setting the <code class="docutils literal notranslate"><span class="pre">nvidia.com/vgpu.config</span></code> node label. It is recommended to set this node label prior to installing the GPU Operator if you do not want the default configuration applied.</p>
<p>Switching vGPU device configuration assumes that no VMs with vGPU are currently running on the node. Any existing VMs will have to be shutdown/migrated first.</p>
<p>To apply a new configuration after GPU Operator install, simply update the node label:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>label<span class="w"> </span>node<span class="w"> </span>&lt;node-name&gt;<span class="w"> </span>--overwrite<span class="w"> </span>nvidia.com/vgpu.config<span class="o">=</span>A10-4C
</pre></div>
</div>
<p>After the vGPU Device Manager finishes applying the new configuration, all pods should return to the Running state.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>gpu-operator
<span class="go">NAME                                                          READY   STATUS    RESTARTS   AGE</span>
<span class="go">...</span>
<span class="go">nvidia-sandbox-device-plugin-daemonset-brtb6                  1/1     Running   0          10s</span>
<span class="go">nvidia-sandbox-validator-ljnwg                                1/1     Running   0          10s</span>
<span class="go">nvidia-vgpu-device-manager-8mgg8                              1/1     Running   0          30m</span>
<span class="go">nvidia-vgpu-manager-daemonset-fpplc                           1/1     Running   0          31m</span>
</pre></div>
</div>
<p>We now see 12 vGPU devices on the node, as 6 <strong>A10-4C</strong> devices can be created per A10 GPU.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ls<span class="w"> </span>-ltr<span class="w"> </span>/sys/bus/mdev/devices
<span class="go">total 0</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 87401d9a-545b-4506-b1be-d4d30f6f4a4b -&gt; ../../../devices/pci0000:3a/0000:3a:00.0/0000:3b:00.5/87401d9a-545b-4506-b1be-d4d30f6f4a4b</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 78597b11-282f-496c-a4d0-19220310039c -&gt; ../../../devices/pci0000:3a/0000:3a:00.0/0000:3b:00.4/78597b11-282f-496c-a4d0-19220310039c</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 0d093db4-2c57-40ce-a1f0-ef4d410c6db8 -&gt; ../../../devices/pci0000:3a/0000:3a:00.0/0000:3b:00.6/0d093db4-2c57-40ce-a1f0-ef4d410c6db8</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 f830dbb1-0eb5-4294-af32-c68028e2ae35 -&gt; ../../../devices/pci0000:3a/0000:3a:00.0/0000:3b:00.7/f830dbb1-0eb5-4294-af32-c68028e2ae35</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 a5a11713-e683-4372-bebf-82219c58ce24 -&gt; ../../../devices/pci0000:3a/0000:3a:00.0/0000:3b:01.1/a5a11713-e683-4372-bebf-82219c58ce24</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 1a48c902-07f1-4a19-b3b0-b89ce35ad025 -&gt; ../../../devices/pci0000:3a/0000:3a:00.0/0000:3b:01.0/1a48c902-07f1-4a19-b3b0-b89ce35ad025</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 b8de2bbe-a41a-440e-9276-f7b56dc35138 -&gt; ../../../devices/pci0000:85/0000:85:02.0/0000:86:01.1/b8de2bbe-a41a-440e-9276-f7b56dc35138</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 afd7a4bb-d638-4489-bb41-6e03fc5c75b5 -&gt; ../../../devices/pci0000:85/0000:85:02.0/0000:86:01.0/afd7a4bb-d638-4489-bb41-6e03fc5c75b5</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 98175f96-707b-4167-ada5-869110ead3ab -&gt; ../../../devices/pci0000:85/0000:85:02.0/0000:86:00.5/98175f96-707b-4167-ada5-869110ead3ab</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 6e93ea61-9068-4096-b20c-ea30a72c1238 -&gt; ../../../devices/pci0000:85/0000:85:02.0/0000:86:00.7/6e93ea61-9068-4096-b20c-ea30a72c1238</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 537ce645-32cc-46d0-b7f0-f90ead840957 -&gt; ../../../devices/pci0000:85/0000:85:02.0/0000:86:00.6/537ce645-32cc-46d0-b7f0-f90ead840957</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 4eb167bc-0e15-43f3-a218-d74cc9d162ff -&gt; ../../../devices/pci0000:85/0000:85:02.0/0000:86:00.4/4eb167bc-0e15-43f3-a218-d74cc9d162ff</span>
</pre></div>
</div>
<p>Check the new vGPU resources are advertised to kubelet:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>describe<span class="w"> </span>node
<span class="go">...</span>
<span class="go">Capacity:</span>
<span class="go">  ...</span>
<span class="go">  nvidia.com/NVIDIA_A10-4C:       12</span>
<span class="go">  ...</span>
<span class="go">Allocatable:</span>
<span class="go">  ...</span>
<span class="go">  nvidia.com/NVIDIA_A10-4C:       12</span>
<span class="go">...</span>
</pre></div>
</div>
<p>Following previous instructions, we can now create a VM with an <strong>A10-4C</strong> vGPU attached.</p>
</section>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="gpu-operator-kubevirt.html" class="btn btn-neutral float-left" title="Running KubeVirt VMs with the GPU Operator" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../1.10.1/overview.html" class="btn btn-neutral float-right" title="Overview" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018-2023, NVIDIA Corporation.
      <span class="lastupdated">Last updated on 2023-03-14.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>  

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  .rst-content dl:not(.docutils) dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }
  </style>
  

</body>
</html>