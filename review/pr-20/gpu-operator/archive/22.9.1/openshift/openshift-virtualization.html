<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>NVIDIA GPU Operator with OpenShift Virtualization &mdash; NVIDIA Cloud Native Technologies  documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../../../../_static/nvidia.ico"/>
    <link rel="canonical" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/archive/22.9.1/openshift/openshift-virtualization.html"/>
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/sphinx_highlight.js"></script>
        <script src="../../../../_static/clipboard.min.js"></script>
        <script src="../../../../_static/copybutton.js"></script>
        <script src="../../../../_static/js/google-analytics/google-analytics-tracker.js"></script>
        <script src="../../../../_static/js/google-analytics/google-analytics-write.js"></script>
        <script src="//assets.adobedtm.com/b92787824f2e0e9b68dc2e993f9bd995339fe417/satelliteLib-7ba51e58dc61bcb0e9311aadd02a0108ab24cc6c.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Troubleshooting" href="troubleshooting-gpu-ocp.html" />
    <link rel="prev" title="Time-slicing NVIDIA GPUs in OpenShift" href="time-slicing-gpus-in-openshift.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" > 
            <a href="../../../../contents.html">
            <img src="../../../../_static/NVLogo_H_B&W.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }

    .wy-nav-content {
      max-width: 1000px;
    }
  </style>
  
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">NVIDIA Container Toolkit:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../container-toolkit/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../container-toolkit/concepts.html">Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../container-toolkit/arch-overview.html">Architecture Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../container-toolkit/install-guide.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../container-toolkit/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../container-toolkit/user-guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../container-toolkit/release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../container-toolkit/archive.html">Archive</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">NVIDIA GPU Operator:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../platform-support.html">Platform Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../gpu-driver-upgrades.html">GPU Driver Upgrades</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install-gpu-operator-vgpu.html">NVIDIA vGPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install-gpu-operator-nvaie.html">NVIDIA AI Enterprise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../openshift/contents.html">GPU Operator on OpenShift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../gpu-operator-mig.html">GPU Operator with MIG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../gpu-sharing.html">Time-Slicing GPUs in Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../gpu-operator-rdma.html">GPUDirect RDMA and GPUDirect Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../gpu-operator-kubevirt.html">GPU Operator with KubeVirt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../appendix.html">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../csp.html">GPU Operator with Cloud Service Providers</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../archive.html">Archive</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../../archive.html#id1">22.9.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../archive.html#id2">22.9.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../archive.html#id3">1.11.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../archive.html#id4">1.11.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../archive.html#id5">1.10.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../archive.html#id6">1.9.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../archive.html#id7">1.9.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../archive.html#id8">1.8</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Kubernetes with GPUs:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../kubernetes/install-k8s.html">Install Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../kubernetes/mig-k8s.html">MIG Support in Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../kubernetes/anthos-guide.html">NVIDIA GPUs with Google Cloud’s Anthos</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Telemetry:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../gpu-telemetry/dcgm-exporter.html">DCGM-Exporter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../gpu-telemetry/dcgm-exporter.html#integrating-gpu-telemetry-into-kubernetes">Integrating GPU Telemetry into Kubernetes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multi-Instance GPU:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mig/mig.html">Multi-Instance GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mig/mig-k8s.html">MIG Support in Kubernetes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Driver Containers:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../driver-containers/overview.html">Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Playground:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../playground/dind.html">Docker-in-Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../playground/x-arch.html">Running Cross-Architecture Containers</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../contents.html">NVIDIA Cloud Native Technologies</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../contents.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../archive.html">Archive</a> &raquo;</li>
          <li><a href="contents.html">GPU Operator on OpenShift</a> &raquo;</li>
      <li>NVIDIA GPU Operator with OpenShift Virtualization</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="nvidia-gpu-operator-with-openshift-virtualization">
<h1>NVIDIA GPU Operator with OpenShift Virtualization<a class="headerlink" href="#nvidia-gpu-operator-with-openshift-virtualization" title="Permalink to this heading"></a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading"></a></h2>
<p>There is a growing demand among Red Hat customers to use virtual GPUs (NVIDIA vGPU)
with Red Hat OpenShift Virtualization. Red Hat OpenShift Virtualization is based on KubeVirt, a
virtual machine (VM) management add-on to Kubernetes that allows you to run and manage VMs in
a Kubernetes cluster. It eliminates the need to manage separate clusters for VM and container workloads,
as both can now coexist in a single Kubernetes cluster. Red Hat OpenShift Virtualization is an
OpenShift feature to run virtual machines (VMs) orchestrated by OpenShift (Kubernetes).</p>
<p>Up until this point, the GPU Operator only provisioned worker nodes for running GPU-accelerated containers.
Now, the GPU Operator can also be used to provision worker nodes for running GPU-accelerated VMs.</p>
<p>The prerequisites needed for running containers and VMs with GPU(s) differs, with the primary difference
being the drivers required. For example, the datacenter driver is needed for containers, the vfio-pci driver
is needed for GPU passthrough, and the NVIDIA vGPU Manager is needed for creating vGPU devices.</p>
<p>The GPU Operator can now be configured to deploy different software components on worker nodes depending
on what GPU workload is configured to run on those nodes. Consider the following example.</p>
<ul class="simple">
<li><p>Node A is configured to run containers.</p></li>
<li><p>Node B is configured to run VMs with Passthrough GPU.</p></li>
<li><p>Node C is configured to run VMs with vGPU.</p></li>
</ul>
<p>Node A receives the following software components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Datacenter</span> <span class="pre">Driver</span></code> - To install the driver.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Container</span> <span class="pre">Toolkit</span></code> - To ensure containers can properly access GPUs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Kubernetes</span> <span class="pre">Device</span> <span class="pre">Plugin</span></code> - To discover and advertise GPU resources to the kubelet.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">DCGM</span> <span class="pre">and</span> <span class="pre">DCGM</span> <span class="pre">Exporter</span></code> - To monitor the GPU(s).</p></li>
</ul>
<p>Node B receives the following software components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">VFIO</span> <span class="pre">Manager</span></code> - Optional. To load vfio-pci and bind it to all GPUs on the node.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sandbox</span> <span class="pre">Device</span> <span class="pre">Plugin</span></code> - Optional. To discover and advertise the passthrough GPUs to the kubelet.</p></li>
</ul>
<p>Node C receives the following software components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">vGPU</span> <span class="pre">Manager</span></code> - To install the driver.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">vGPU</span> <span class="pre">Device</span> <span class="pre">Manager</span></code> - To create vGPU devices on the node.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sandbox</span> <span class="pre">Device</span> <span class="pre">Plugin</span></code> -Optional. To discover and advertise the vGPU devices to kubelet.</p></li>
</ul>
</section>
<section id="assumptions-constraints-and-dependencies">
<h2>Assumptions, constraints, and dependencies<a class="headerlink" href="#assumptions-constraints-and-dependencies" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p>A worker node can run GPU-accelerated containers, or GPU accelerated VMs with GPU passthrough, or GPU accelerated-VMs with vGPU, but not a combination of any of them.</p></li>
<li><p>The cluster admin or developer has knowledge about their cluster ahead of time, and can properly label nodes to indicate what types of GPU workloads they will run.</p></li>
<li><p>Worker nodes running GPU accelerated VMs (with pGPU or vGPU) are assumed to be bare metal.</p></li>
<li><p>MIG-backed vGPUs are not supported.</p></li>
<li><p>The GPU Operator will not automate the installation of the vGPU guest driver inside KubeVirt VMs with vGPUs attached.</p></li>
<li><p>There are two separate device plugins – the NVIDIA k8s-device-plugin and the NVIDIA kubevirt-gpu-device-plugin.</p></li>
<li><p>KubeVirt/Openshift virtualization provides built-in device plugins. These are the default tested device plugins.</p></li>
</ul>
</section>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.openshift.com/container-platform/latest/virt/install/installing-virt-cli.html">Install the OpenShift Virtualization Operator</a>.</p></li>
<li><p><a class="reference external" href="https://docs.openshift.com/container-platform/latest/virt/install/virt-enabling-virtctl.html">Install the virtctl client</a>.</p></li>
</ul>
</section>
<section id="labeling-worker-nodes">
<h2>Labeling worker nodes<a class="headerlink" href="#labeling-worker-nodes" title="Permalink to this heading"></a></h2>
<p>Use the following command to add a label to a worker node:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc<span class="w"> </span>label<span class="w"> </span>node<span class="w"> </span>&lt;node-name&gt;<span class="w"> </span>--overwrite<span class="w"> </span>nvidia.com/gpu.workload.config<span class="o">=</span>vm-vgpu
</pre></div>
</div>
<p>You can assign the following values to the label - <code class="docutils literal notranslate"><span class="pre">container</span></code>, <code class="docutils literal notranslate"><span class="pre">vm-passthrough</span></code>, and <code class="docutils literal notranslate"><span class="pre">vm-vgpu</span></code>. The GPU Operator uses the value of this label when determining which operands to deploy.</p>
<p>If the node label <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.workload.config</span></code> does not exist on the node, the GPU Operator will assume the default GPU workload configuration, <code class="docutils literal notranslate"><span class="pre">container</span></code>, and will deploy the software components needed to support this workload type.
To change the default GPU workload configuration, set the following value in <code class="docutils literal notranslate"><span class="pre">ClusterPolicy</span></code>: <code class="docutils literal notranslate"><span class="pre">sandboxWorkloads.defaultWorkload=&lt;config&gt;</span></code>.</p>
</section>
<section id="building-the-vgpu-manager-image">
<h2>Building the vGPU Manager image<a class="headerlink" href="#building-the-vgpu-manager-image" title="Permalink to this heading"></a></h2>
<p>Use the following steps to build the vGPU Manager container and push it to a private registry.</p>
<ol class="arabic">
<li><p>Download the vGPU Software from the <a class="reference external" href="https://nvid.nvidia.com/dashboard/#/dashboard">NVIDIA Licensing Portal</a>.</p>
<ul class="simple">
<li><p>Login to the NVIDIA Licensing Portal and navigate to the Software Downloads section.</p></li>
<li><p>The NVIDIA vGPU Software is located in the Software Downloads section of the NVIDIA Licensing Portal.</p></li>
<li><p>The vGPU Software bundle is packaged as a zip file. Download and unzip the bundle to obtain the NVIDIA vGPU Manager for Linux (<code class="docutils literal notranslate"><span class="pre">NVIDIA-Linux-x86_64-&lt;version&gt;-vgpu-kvm.run</span></code> file)</p></li>
</ul>
<p>Use the following steps to clone the driver container repository and build the driver image.</p>
</li>
<li><p>Open a terminal and clone the driver container image repository:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>git<span class="w"> </span>clone<span class="w"> </span>https://gitlab.com/nvidia/container-images/driver
<span class="gp">$ </span><span class="nb">cd</span><span class="w"> </span>driver
</pre></div>
</div>
</li>
<li><p>Change to the <code class="docutils literal notranslate"><span class="pre">vgpu-manager</span></code> directory for your OS.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nb">cd</span><span class="w"> </span>vgpu-manager/rhel
</pre></div>
</div>
</li>
<li><p>Copy the NVIDIA vGPU Manager from your extracted zip file:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cp<span class="w"> </span>&lt;local-driver-download-directory&gt;/*-vgpu-kvm.run<span class="w"> </span>./
</pre></div>
</div>
</li>
<li><p>Set the following environment variables:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">PRIVATE_REGISTRY</span></code> - Name of the private registry used to store the driver image.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">VERSION</span></code> - The NVIDIA vGPU Manager version downloaded from the NVIDIA Software Portal.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">OS_TAG</span></code> - This must match the Guest OS version. For RedHat OpenShift this should be set to <code class="docutils literal notranslate"><span class="pre">rhcos4.x</span></code> where x is the supported minor OCP version.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CUDA_VERSION</span></code> - CUDA base image version to build the driver image with.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The recommended registry to use is the Integrated OpenShift Container Platform registry. For more information about the registry, see <a class="reference external" href="https://docs.openshift.com/container-platform/latest/registry/accessing-the-registry.html">Accessing the registry</a>.</p>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nb">export</span><span class="w"> </span><span class="nv">PRIVATE_REGISTRY</span><span class="o">=</span>my/private/registry<span class="w"> </span><span class="nv">VERSION</span><span class="o">=</span><span class="m">510</span>.73.06<span class="w"> </span><span class="nv">OS_TAG</span><span class="o">=</span>rhcos4.11<span class="w"> </span><span class="nv">CUDA_VERSION</span><span class="o">=</span><span class="m">11</span>.7.1
</pre></div>
</div>
</li>
<li><p>Build the NVIDIA vGPU Manager image:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>build<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--build-arg<span class="w"> </span><span class="nv">DRIVER_VERSION</span><span class="o">=</span><span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--build-arg<span class="w"> </span><span class="nv">CUDA_VERSION</span><span class="o">=</span><span class="si">${</span><span class="nv">CUDA_VERSION</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-t<span class="w"> </span><span class="si">${</span><span class="nv">PRIVATE_REGISTRY</span><span class="si">}</span>/vgpu-manager:<span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span>-<span class="si">${</span><span class="nv">OS_TAG</span><span class="si">}</span><span class="w"> </span>.
</pre></div>
</div>
</li>
<li><p>Push the NVIDIA vGPU Manager image to your private registry:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>push<span class="w"> </span><span class="si">${</span><span class="nv">PRIVATE_REGISTRY</span><span class="si">}</span>/vgpu-manager:<span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span>-<span class="si">${</span><span class="nv">OS_TAG</span><span class="si">}</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="installing-the-nvidia-gpu-operator-using-the-cli">
<h2>Installing the NVIDIA GPU Operator using the CLI<a class="headerlink" href="#installing-the-nvidia-gpu-operator-using-the-cli" title="Permalink to this heading"></a></h2>
<p>Install the <strong>NVIDIA GPU Operator</strong> using the guidance <a class="reference internal" href="install-gpu-ocp.html#install-nvidiagpu-22-9-1"><span class="std std-ref">Installing the NVIDIA GPU Operator</span></a>.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>When prompted to create a cluster policy follow the guidance <a class="reference internal" href="#install-cluster-policy-vgpu-22-9-1"><span class="std std-ref">Creating a ClusterPolicy for the GPU Operator</span></a>.</p>
</div>
</div></blockquote>
</section>
<section id="create-the-secret">
<h2>Create the secret<a class="headerlink" href="#create-the-secret" title="Permalink to this heading"></a></h2>
<p>OpenShift has a secret object type which provides a mechanism for holding sensitive information such as passwords and private source repository credentials. Next you will create a secret object for storing your registry API key (the mechanism used to authenticate your access to the
private container registry).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before you begin you will need to generate or use an existing API key for your private registry.</p>
</div>
<ol class="arabic">
<li><p>Navigate to <strong>Home</strong> &gt; <strong>Projects</strong> and ensure the <code class="docutils literal notranslate"><span class="pre">nvidia-gpu-operator</span></code> is selected.</p></li>
<li><p>In the OpenShift Container Platform web console, click <strong>Secrets</strong> from the Workloads drop down.</p></li>
<li><p>Click the <strong>Create</strong> Drop down.</p></li>
<li><p>Select Image Pull Secret.</p>
<img alt="../../../../_images/secrets4.png" src="../../../../_images/secrets4.png" />
</li>
<li><p>Enter the following into each field:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>Secret name</strong>: private-registry-secret</p></li>
<li><p><strong>Authentication type</strong>: Image registry credentials</p></li>
<li><p><strong>Registry server address</strong>: &lt;private-registry_address&gt;</p></li>
<li><p><strong>Username</strong>: $oauthtoken</p></li>
<li><p><strong>Password</strong>: &lt;API-KEY&gt;</p></li>
<li><p><strong>Email</strong>: &lt;YOUR-EMAIL&gt;</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Click <strong>Create</strong>.</p>
<p>A pull secret is created.</p>
</li>
</ol>
</section>
<section id="creating-a-clusterpolicy-for-the-gpu-operator">
<span id="install-cluster-policy-vgpu-22-9-1"></span><h2>Creating a ClusterPolicy for the GPU Operator<a class="headerlink" href="#creating-a-clusterpolicy-for-the-gpu-operator" title="Permalink to this heading"></a></h2>
<p>As a cluster administrator, you can create a ClusterPolicy using the OpenShift Container Platform CLI.
Create the cluster policy using the CLI:</p>
<ol class="arabic">
<li><p>Create the ClusterPolicy:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc<span class="w"> </span>get<span class="w"> </span>csv<span class="w"> </span>-n<span class="w"> </span>nvidia-gpu-operator<span class="w"> </span>gpu-operator-certified.v22.9.0<span class="w"> </span>-ojsonpath<span class="o">={</span>.metadata.annotations.alm-examples<span class="o">}</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>jq<span class="w"> </span>.<span class="o">[</span><span class="m">0</span><span class="o">]</span><span class="w"> </span>&gt;<span class="w"> </span>clusterpolicy.json
</pre></div>
</div>
</li>
<li><p>Modify the <code class="docutils literal notranslate"><span class="pre">clusterpolicy.json</span></code> file as follows:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">vgpuManager</span></code> options are only required if you want to use the NVIDIA vGPU. If you are only using GPU passthrough, these options should not be set.</p>
</div>
<ul class="simple">
<li><p>sandboxWorloads.enabled=true</p></li>
<li><p>vgpuManager.enabled=true</p></li>
<li><p>vgpuManager.repository=&lt;path to private repository&gt;</p></li>
<li><p>vgpuManager.image=vgpu-manager</p></li>
<li><p>vgpuManager.version=&lt;driver version&gt;</p></li>
<li><p>vgpuManager.imagePullSecrets={&lt;name of image pull secret&gt;}</p></li>
</ul>
</li>
<li><p>Apply the changes:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>clusterpolicy.json
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">clusterpolicy.nvidia.com/gpu-cluster-policy created</span>
</pre></div>
</div>
</li>
</ol>
<p>The vGPU Device Manager, deployed by the GPU Operator, automatically creates vGPU devices which can be assigned to KubeVirt VMs.
Without additional configuration, the GPU Operator creates a default set of devices on all GPUs.
To learn more about how the vGPU Device Manager and configure which types of vGPU devices get created in your cluster, refer to <a class="reference internal" href="#vgpu-device-configuration-22-9-1-22-9-1"><span class="std std-ref">vGPU Device Configuration</span></a>.</p>
</section>
<section id="enabling-the-iommu-driver-on-hosts">
<h2>Enabling the IOMMU driver on hosts<a class="headerlink" href="#enabling-the-iommu-driver-on-hosts" title="Permalink to this heading"></a></h2>
<p>To enable the IOMMU (Input-Output Memory Management Unit) driver in the kernel, create the <code class="docutils literal notranslate"><span class="pre">MachineConfig</span></code> object and add the kernel arguments.</p>
<section id="id1">
<h3>Prerequisites<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>Administrative privilege to a working OpenShift Container Platform cluster.</p></li>
<li><p>Intel or AMD CPU hardware.</p></li>
<li><p>Intel Virtualization Technology for Directed I/O extensions or AMD IOMMU in the BIOS (Basic Input/Output System) is enabled.</p></li>
</ul>
<ol class="arabic">
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">MachineConfig</span></code> object that identifies the kernel argument. The following example shows a kernel argument for an Intel CPU.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span>apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 100-worker-iommu
spec:
  config:
    ignition:
      version: 3.2.0
  kernelArguments:
      - intel_iommu=on
</pre></div>
</div>
</li>
<li><p>Create the new <code class="docutils literal notranslate"><span class="pre">MachineConfig</span></code> object:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc<span class="w"> </span>create<span class="w"> </span>-f<span class="w"> </span><span class="m">100</span>-worker-kernel-arg-iommu.yaml
</pre></div>
</div>
</li>
<li><p>Verify that the new <code class="docutils literal notranslate"><span class="pre">MachineConfig</span></code> object was added:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc<span class="w"> </span>get<span class="w"> </span>MachineConfig
</pre></div>
</div>
</li>
</ol>
</section>
</section>
<section id="add-gpu-resources-to-the-hyperconverged-cr">
<h2>Add GPU resources to the HyperConverged CR<a class="headerlink" href="#add-gpu-resources-to-the-hyperconverged-cr" title="Permalink to this heading"></a></h2>
<p>Update the <code class="docutils literal notranslate"><span class="pre">HyperConverged</span></code> Custom Resource, so that all GPU/vGPU devices in your cluster are permitted and can be assigned to OpenShift Virtualization VMs.</p>
<p>In the example below, the <strong>A10</strong> GPU device and <strong>A10-24Q</strong> vGPU device are being permitting .</p>
<ul class="simple">
<li><p>Replace the values of the:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">pciDeviceSelector</span></code> and <code class="docutils literal notranslate"><span class="pre">resourceName</span></code> under <code class="docutils literal notranslate"><span class="pre">pciHostDevices</span></code> to correspond to your GPU model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mdevNameSelector</span></code> and <code class="docutils literal notranslate"><span class="pre">resourceName</span></code> under <code class="docutils literal notranslate"><span class="pre">mediatedDevices</span></code> to correspond to your vGPU type.</p></li>
</ul>
</li>
<li><p>Set <code class="docutils literal notranslate"><span class="pre">externalResourceProvider=true</span></code> to indicate that this resource is being provided by an external device plugin, in this case the <code class="docutils literal notranslate"><span class="pre">sandbox-device-plugin</span></code> which is deployed by the GPU Operator.</p></li>
</ul>
<p>Refer to the <a class="reference external" href="https://kubevirt.io/user-guide/virtual_machines/host-devices/#listing-permitted-devices">KubeVirt user guide</a> for more information on the configuration options.</p>
<p>To find the device ID for a particular GPU, search by device name in the <a class="reference external" href="https://pci-ids.ucw.cz/v2.2/pci.ids">PCI IDs database</a>.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span>...
spec:
 configuration:
 developerConfiguration:
   featureGates:
   - GPU
 permittedHostDevices:
   pciHostDevices:
   - externalResourceProvider: true
     pciDeviceSelector: 10DE:2236
     resourceName: nvidia.com/GA102GL_A10
   mediatedDevices:
   - externalResourceProvider: true
     mdevNameSelector: NVIDIA A10-24Q
     resourceName: nvidia.com/NVIDIA_A10-24Q
...
</pre></div>
</div>
<p>A physical device that is divided into one or more virtual devices. A vGPU is a type of mediated device
(mdev); the performance of the physical GPU is divided among the virtual devices. You can assign mediated
devices to one or more virtual machines (VMs), but the number of guests must be compatible with your GPU.
Some GPUs do not support multiple guests.</p>
</section>
<section id="creating-a-virtual-machine-with-gpu">
<h2>Creating a virtual machine with GPU<a class="headerlink" href="#creating-a-virtual-machine-with-gpu" title="Permalink to this heading"></a></h2>
<p>Assign GPU devices, either passthrough or vGPU, to virtual machines.</p>
<section id="id2">
<h3>Prerequisites<a class="headerlink" href="#id2" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>The GPU devices are configured in the <code class="docutils literal notranslate"><span class="pre">HyperConverged</span></code> custom resource (CR).</p></li>
</ul>
<ol class="arabic">
<li><p>Assign the GPU device(s) to a virtual machine (VM) by editing the <code class="docutils literal notranslate"><span class="pre">spec.domain.devices.gpus</span></code> stanza of the <code class="docutils literal notranslate"><span class="pre">VirtualMachine</span></code> manifest:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span>apiVersion: kubevirt.io/v1
kind: VirtualMachine
. . . snip . . .
spec:
  domain:
    devices:
      gpus:
      - deviceName: nvidia.com/TU104GL_Tesla_T4
        name: gpu1
      - deviceName: nvidia.com/GRID_T4-1Q
        name: gpu2
. . . snip . . .
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">deviceName</span></code> The resource name associated with the GPU.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">name</span></code> A name to identify the device on the VM.</p></li>
</ul>
</li>
</ol>
</section>
</section>
<section id="vgpu-device-configuration">
<span id="vgpu-device-configuration-22-9-1-22-9-1"></span><h2>vGPU Device Configuration<a class="headerlink" href="#vgpu-device-configuration" title="Permalink to this heading"></a></h2>
<p>The vGPU Device Manager assists in creating vGPU devices on GPU worker nodes.</p>
<p>The vGPU Device Manager allows administrators to declaratively define a set of possible vGPU device configurations they would like applied to GPUs on a node.
At runtime, they then point the vGPU Device Manager at one of these configurations, and vGPU Device Manager takes care of applying it.</p>
<p>The configuration file is created as a ConfigMap, and is shared across all worker nodes.
At runtime, a node label, <code class="docutils literal notranslate"><span class="pre">nvidia.com/vgpu.config</span></code>, can be used to decide which of these configurations to actually apply to a node at any given time.
If the node is not labeled, then the <code class="docutils literal notranslate"><span class="pre">default</span></code> configuration will be used.</p>
<p>For more information on this component and how it is configured, refer to the project <a class="reference external" href="https://github.com/NVIDIA/vgpu-device-manager">README</a>.</p>
<p>By default, the GPU Operator deploys a ConfigMap for the vGPU Device Manager, containing named configurations for all <a class="reference external" href="https://docs.nvidia.com/grid/latest/grid-vgpu-user-guide/index.html#supported-gpus-grid-vgpu">vGPU types</a> supported by NVIDIA vGPU.
Users can select a specific configuration for a worker node by applying the <code class="docutils literal notranslate"><span class="pre">nvidia.com/vgpu.config</span></code> node label.</p>
<p>For example, labeling a node with <code class="docutils literal notranslate"><span class="pre">nvidia.com/vgpu.config=A10-8Q</span></code> would create 3 vGPU devices of type <strong>A10-8Q</strong> on all <strong>A10</strong> GPUs on the node (note: 3 is the maximum number of <strong>A10-8Q</strong> devices that can be created per GPU).
If the node is not labeled, the <code class="docutils literal notranslate"><span class="pre">default</span></code> configuration will be applied.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">default</span></code> configuration will create Q-series vGPU devices on all GPUs, where the amount of framebuffer memory per vGPU device is half the total GPU memory.
For example, the <code class="docutils literal notranslate"><span class="pre">default</span></code> configuration will create two <strong>A10-12Q</strong> devices on all <strong>A10</strong> GPUs, two <strong>V100-8Q</strong> devices  on all <strong>V100</strong> GPUs, and two <strong>T4-8Q</strong> devices on all <strong>T4</strong> GPUs.</p>
<p>If custom vGPU device configuration is desired, more than the default ConfigMap provides, you can create your own ConfigMap:</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc<span class="w"> </span>create<span class="w"> </span>configmap<span class="w"> </span>custom-vgpu-config<span class="w"> </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--from-file<span class="o">=</span>config.yaml<span class="o">=</span>/path/to/file
</pre></div>
</div>
</div></blockquote>
<p>And then configure the GPU Operator to use it by setting <code class="docutils literal notranslate"><span class="pre">vgpuDeviceManager.config.name=custom-vgpu-config</span></code>.</p>
<p>Apply a specific vGPU device configuration on a per-node basis by setting the <code class="docutils literal notranslate"><span class="pre">nvidia.com/vgpu.config</span></code> node label. It is recommended to set this node label prior to installing the GPU Operator if you do not want the default configuration applied.</p>
<p>Switching vGPU device configuration after one has been successfully applied assumes that no VMs with vGPU are currently running on the node. Any existing VMs will have to be shutdown/migrated first.</p>
<p>To apply a new configuration after GPU Operator install, simply update the <code class="docutils literal notranslate"><span class="pre">nvidia.com/vgpu.config</span></code> node label.</p>
<p>Let’s run through an example on a system with two <strong>A10</strong> GPUs.</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>nvidia-smi<span class="w"> </span>-L
<span class="go">GPU 0: NVIDIA A10 (UUID: GPU-ebd34bdf-1083-eaac-2aff-4b71a022f9bd)</span>
<span class="go">GPU 1: NVIDIA A10 (UUID: GPU-1795e88b-3395-b27b-dad8-0488474eec0c)</span>
</pre></div>
</div>
</div></blockquote>
<p>After installing the GPU Operator as detailed in the previous sections and without labeling the node with <code class="docutils literal notranslate"><span class="pre">nvidia.com/vgpu.config</span></code>, the <code class="docutils literal notranslate"><span class="pre">default</span></code> vGPU config get applied – four <strong>A10-12Q</strong> devices get created (two per GPU):</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc<span class="w"> </span>get<span class="w"> </span>node<span class="w"> </span>cnt-server-2<span class="w"> </span>-o<span class="w"> </span>json<span class="w"> </span><span class="p">|</span><span class="w"> </span>jq<span class="w"> </span><span class="s1">&#39;.status.allocatable | with_entries(select(.key | startswith(&quot;nvidia.com/&quot;))) | with_entries(select(.value != &quot;0&quot;))&#39;</span>
<span class="go">{</span>
<span class="go">  &quot;nvidia.com/NVIDIA_A10-12Q&quot;: &quot;4&quot;</span>
<span class="go">}</span>
</pre></div>
</div>
</div></blockquote>
<p>If instead you want to create <strong>A10-4Q</strong> devices, we can label the node like such:</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc<span class="w"> </span>label<span class="w"> </span>node<span class="w"> </span>&lt;node-name&gt;<span class="w"> </span>--overwrite<span class="w"> </span>nvidia.com/vgpu.config<span class="o">=</span>A10-4Q
</pre></div>
</div>
</div></blockquote>
<p>After the vGPU Device Manager finishes applying the new configuration, all GPU Operator pods should return to the Running state.</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>gpu-operator
<span class="go">NAME                                                          READY   STATUS    RESTARTS   AGE</span>
<span class="go">...</span>
<span class="go">nvidia-sandbox-device-plugin-daemonset-brtb6                  1/1     Running   0          10s</span>
<span class="go">nvidia-sandbox-validator-ljnwg                                1/1     Running   0          10s</span>
<span class="go">nvidia-vgpu-device-manager-8mgg8                              1/1     Running   0          30m</span>
<span class="go">nvidia-vgpu-manager-daemonset-fpplc                           1/1     Running   0          31m</span>
</pre></div>
</div>
</div></blockquote>
<p>You should now see 12 <strong>A10-4Q</strong> devices on the node, as 6 <strong>A10-4Q</strong> devices can be created per <strong>A10</strong> GPU.</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc<span class="w"> </span>get<span class="w"> </span>node<span class="w"> </span>cnt-server-2<span class="w"> </span>-o<span class="w"> </span>json<span class="w"> </span><span class="p">|</span><span class="w"> </span>jq<span class="w"> </span><span class="s1">&#39;.status.allocatable | with_entries(select(.key | startswith(&quot;nvidia.com/&quot;))) | with_entries(select(.value != &quot;0&quot;))&#39;</span>
<span class="go">{</span>
<span class="go">  &quot;nvidia.com/NVIDIA_A10-4Q&quot;: &quot;12&quot;</span>
<span class="go">}</span>
</pre></div>
</div>
</div></blockquote>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="time-slicing-gpus-in-openshift.html" class="btn btn-neutral float-left" title="Time-slicing NVIDIA GPUs in OpenShift" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="troubleshooting-gpu-ocp.html" class="btn btn-neutral float-right" title="Troubleshooting" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018-2023, NVIDIA Corporation.
      <span class="lastupdated">Last updated on 2023-04-04.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>  

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  .rst-content dl:not(.docutils) dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }
  </style>
  

</body>
</html>