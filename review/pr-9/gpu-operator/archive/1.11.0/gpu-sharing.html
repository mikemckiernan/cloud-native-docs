<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Time-Slicing GPUs in Kubernetes &mdash; NVIDIA Cloud Native Technologies  documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../../../_static/nvidia.ico"/>
    <link rel="canonical" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/archive/1.11.0/gpu-sharing.html"/>
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/sphinx_highlight.js"></script>
        <script src="../../../_static/clipboard.min.js"></script>
        <script src="../../../_static/copybutton.js"></script>
        <script src="../../../_static/js/google-analytics/google-analytics-tracker.js"></script>
        <script src="../../../_static/js/google-analytics/google-analytics-write.js"></script>
        <script src="//assets.adobedtm.com/b92787824f2e0e9b68dc2e993f9bd995339fe417/satelliteLib-7ba51e58dc61bcb0e9311aadd02a0108ab24cc6c.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="GPUDirect RDMA" href="gpu-operator-rdma.html" />
    <link rel="prev" title="GPU Operator with MIG" href="gpu-operator-mig.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" > 
            <a href="../../../contents.html">
            <img src="../../../_static/NVLogo_H_B&W.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }

    .wy-nav-content {
      max-width: 1000px;
    }
  </style>
  
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">NVIDIA Container Toolkit</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/concepts.html">Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/arch-overview.html">Architecture Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/install-guide.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/user-guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/archive.html">Archive</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">NVIDIA GPU Operator</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../about.html">About the Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../platform-support.html">Platform Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting-started.html#install-nvidia-gpu-operator">Install NVIDIA GPU Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gpu-driver-upgrades.html">GPU Driver Upgrades</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced-configurations.html">Advanced Configurations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../openshift/contents.html">GPU Operator on OpenShift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../appendix.html">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../license.html">Licenses and Contributing</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../archive.html">Archive</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id1">22.9.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id2">22.9.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id3">1.11.1</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../archive.html#id4">1.11.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id5">1.10.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id6">1.9.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id7">1.9.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id8">1.8</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Kubernetes with GPUs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../kubernetes/install-k8s.html">Install Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../kubernetes/mig-k8s.html">MIG Support in Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../kubernetes/anthos-guide.html">NVIDIA GPUs with Google Cloud’s Anthos</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Telemetry</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../gpu-telemetry/dcgm-exporter.html">DCGM-Exporter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../gpu-telemetry/dcgm-exporter.html#integrating-gpu-telemetry-into-kubernetes">Integrating GPU Telemetry into Kubernetes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multi-Instance GPU</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../mig/mig.html">Multi-Instance GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mig/mig-k8s.html">MIG Support in Kubernetes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Driver Containers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../driver-containers/overview.html">Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Playground</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../playground/dind.html">Docker-in-Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../playground/x-arch.html">Running Cross-Architecture Containers</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../contents.html">NVIDIA Cloud Native Technologies</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../contents.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../archive.html">Archive</a> &raquo;</li>
      <li>Time-Slicing GPUs in Kubernetes</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="time-slicing-gpus-in-kubernetes">
<span id="gpu-sharing-1-11-0"></span><h1>Time-Slicing GPUs in Kubernetes<a class="headerlink" href="#time-slicing-gpus-in-kubernetes" title="Permalink to this heading"></a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading"></a></h2>
<p>The latest generations of NVIDIA GPUs provide an operation mode called
Multi-Instance GPU, or MIG. MIG allows you to partition a GPU
into several smaller, predefined instances, each of which looks like a
mini-GPU that provides memory and fault isolation at the hardware layer.
You can share access to a GPU by running workloads on one of
these predefined instances instead of the full native GPU.</p>
<p>MIG support was added to Kubernetes in 2020. Refer to <a class="reference external" href="https://www.google.com/url?q=https://docs.google.com/document/d/1mdgMQ8g7WmaI_XVVRrCvHPFPOMCm5LQD5JefgAh6N8g/edit&amp;sa=D&amp;source=editors&amp;ust=1655578433019961&amp;usg=AOvVaw1F-OezvM-Svwr1lLsdQmu3">Supporting MIG in Kubernetes</a>
for details on how this works.</p>
<p>What if you don’t need the memory and fault-isolation provided by
MIG? What if you’re willing to trade the isolation provided by MIG for
the ability to share a GPU by a larger number of users. Or what if you don’t
have access to a GPU that supports MIG? Should they not be able
to provide shared access to their GPUs so long as memory and
fault-isolation are not a concern?</p>
<p>The NVIDIA GPU Operator allows oversubscription of GPUs through a set
of extended options for the <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/k8s-device-plugin">NVIDIA Kubernetes Device Plugin</a>.
Internally, GPU time-slicing is used to allow workloads that land
on oversubscribed GPUs to interleave with one another. This page covers
ways to enable this in Kubernetes using the GPU Operator.</p>
<p>This mechanism for enabling “time-sharing” of
GPUs in Kubernetes allows a system administrator to define a set of
“replicas” for a GPU, each of which can be handed out independently to a
pod to run workloads on. Unlike MIG, there is no memory or
fault-isolation between replicas, but for some workloads this is better
than not being able to share at all. Internally, GPU
time-slicing is used to multiplex workloads from
replicas of the same underlying GPU.</p>
<p>GPU time-slicing can be used with bare-metal applications, virtual machines
with GPU passthrough, and virtual machines with NVIDIA vGPU.
The following sections describe how to make use of the GPU
time-slicing feature in Kubernetes.</p>
</section>
<section id="configuration">
<h2>Configuration<a class="headerlink" href="#configuration" title="Permalink to this heading"></a></h2>
<section id="configuration-for-shared-access-to-gpus-with-gpu-time-slicing">
<h3>Configuration for Shared Access to GPUs with GPU Time-Slicing<a class="headerlink" href="#configuration-for-shared-access-to-gpus-with-gpu-time-slicing" title="Permalink to this heading"></a></h3>
<p>You can provide time-slicing configurations for the NVIDIA Kubernetes Device Plugin as a <code class="docutils literal notranslate"><span class="pre">ConfigMap</span></code>:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">sharing</span><span class="p">:</span>
<span class="w">  </span><span class="nt">timeSlicing</span><span class="p">:</span>
<span class="w">    </span><span class="nt">renameByDefault</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;bool&gt;</span>
<span class="w">    </span><span class="nt">failRequestsGreaterThanOne</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;bool&gt;</span>
<span class="w">    </span><span class="nt">resources</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;resource-name&gt;</span>
<span class="w">      </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;num-replicas&gt;</span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">...</span>
</pre></div>
</div>
<p>For each named resource under <code class="docutils literal notranslate"><span class="pre">sharing.timeSlicing.resources</span></code>, a number of
replicas can be specified for that resource type. These replicas represent
the number of shared accesses that will be granted for a GPU represented by that resource type.
If <code class="docutils literal notranslate"><span class="pre">renameByDefault=true</span></code>, then each resource will be advertised under the
name <code class="docutils literal notranslate"><span class="pre">&lt;resource-name&gt;.shared</span></code> instead of simply <code class="docutils literal notranslate"><span class="pre">&lt;resource-name&gt;</span></code>.
If <code class="docutils literal notranslate"><span class="pre">failRequestsGreaterThanOne=true</span></code>, then the plugin will fail to allocate
any shared resources to a container if they request more than one. The
container’s pod will fail with an <code class="docutils literal notranslate"><span class="pre">UnexpectedAdmissionError</span></code> and must then be manually
deleted, updated, and redeployed.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unlike with “normal” GPU requests, requesting more than one shared GPU
does not guarantee that you will get
access to a proportional amount of compute power. It only specifies that
you will get access to a GPU that is shared
by other clients, each of which has the freedom to run as many processes
on the underlying GPU as they want.
Internally, the GPU will simply give an equal share of time to
all GPU processes across all of the clients.
The <code class="docutils literal notranslate"><span class="pre">failRequestsGreaterThanOne</span></code> flag is meant to help users
understand this subtlety, by treating a request of 1 as an
access request rather than an exclusive resource request. Setting
<code class="docutils literal notranslate"><span class="pre">failRequestsGreaterThanOne=true</span></code> is recommended,
but it is set to <code class="docutils literal notranslate"><span class="pre">false</span></code> by default to retain backwards compatibility.</p>
</div>
<p>You can specify multiple configurations in a <code class="docutils literal notranslate"><span class="pre">ConfigMap</span></code> as in the following
example.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">cat &lt;&lt; EOF &gt;&gt; time-slicing-config.yaml</span>
<span class="l l-Scalar l-Scalar-Plain">apiVersion</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ConfigMap</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">time-slicing-config</span>
<span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpu-operator</span>
<span class="nt">data</span><span class="p">:</span>
<span class="w">    </span><span class="nt">a100-40gb</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|-</span>
<span class="w">        </span><span class="no">version: v1</span>
<span class="w">        </span><span class="no">sharing:</span>
<span class="w">          </span><span class="no">timeSlicing:</span>
<span class="w">            </span><span class="no">resources:</span>
<span class="w">            </span><span class="no">- name: nvidia.com/gpu</span>
<span class="w">              </span><span class="no">replicas: 8</span>
<span class="w">            </span><span class="no">- name: nvidia.com/mig-1g.5gb</span>
<span class="w">              </span><span class="no">replicas: 1</span>
<span class="w">            </span><span class="no">- name: nvidia.com/mig-2g.10gb</span>
<span class="w">              </span><span class="no">replicas: 2</span>
<span class="w">            </span><span class="no">- name: nvidia.com/mig-3g.20gb</span>
<span class="w">              </span><span class="no">replicas: 3</span>
<span class="w">            </span><span class="no">- name: nvidia.com/mig-7g.40gb</span>
<span class="w">              </span><span class="no">replicas: 7</span>
<span class="w">    </span><span class="nt">tesla-t4</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|-</span>
<span class="w">        </span><span class="no">version: v1</span>
<span class="w">        </span><span class="no">sharing:</span>
<span class="w">          </span><span class="no">timeSlicing:</span>
<span class="w">            </span><span class="no">resources:</span>
<span class="w">            </span><span class="no">- name: nvidia.com/gpu</span>
<span class="w">              </span><span class="no">replicas: 4</span>
<span class="l l-Scalar l-Scalar-Plain">EOF</span>
</pre></div>
</div>
<p>Create a <code class="docutils literal notranslate"><span class="pre">ConfigMap</span></code> in the operator namespace. In this example, it is <code class="docutils literal notranslate"><span class="pre">gpu-operator</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>create<span class="w"> </span>namespace<span class="w"> </span>gpu-operator
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>create<span class="w"> </span>-f<span class="w"> </span>time-slicing-config.yaml
</pre></div>
</div>
</section>
<section id="enabling-shared-access-to-gpus-with-the-nvidia-gpu-operator">
<h3>Enabling Shared Access to GPUs with the NVIDIA GPU Operator<a class="headerlink" href="#enabling-shared-access-to-gpus-with-the-nvidia-gpu-operator" title="Permalink to this heading"></a></h3>
<p>You can enable time-slicing with the NVIDIA GPU Operator by passing the
<code class="docutils literal notranslate"><span class="pre">devicePlugin.config.name=&lt;config-map-name&gt;</span></code> parameter,
where <code class="docutils literal notranslate"><span class="pre">&lt;config-map-name&gt;</span></code>
is the name of the <code class="docutils literal notranslate"><span class="pre">ConfigMap</span></code> created for the time-slicing
configuration as described in the previous section.</p>
<p>During fresh install of the NVIDIA GPU Operator with time-slicing enabled (e.g. <code class="docutils literal notranslate"><span class="pre">time-slicing-config</span></code>):</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>gpu-operator<span class="w"> </span>nvidia/gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>devicePlugin.config.name<span class="o">=</span>time-slicing-config
</pre></div>
</div>
<p>For dynamically enabling time-slicing with GPU Operator already installed:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>patch<span class="w"> </span>clusterpolicy/cluster-policy<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--type<span class="w"> </span>merge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>-p<span class="w"> </span><span class="s1">&#39;{&quot;spec&quot;: {&quot;devicePlugin&quot;: {&quot;config&quot;: {&quot;name&quot;: &quot;time-slicing-config&quot;}}}}&#39;</span>
</pre></div>
</div>
</section>
<section id="applying-the-default-configuration-across-the-cluster">
<h3>Applying the Default Configuration Across the Cluster<a class="headerlink" href="#applying-the-default-configuration-across-the-cluster" title="Permalink to this heading"></a></h3>
<p>The time-slicing configuration can be applied either at cluster level
or per node. By default, the GPU Operator will <strong>not</strong> apply the time-slicing
configuration to any GPU node in the cluster. The user would have to
explicitly specify it with the <code class="docutils literal notranslate"><span class="pre">devicePlugin.config.default=&lt;config-name&gt;</span></code> parameter.</p>
<p>Install the GPU Operator by passing the time-slicing <code class="docutils literal notranslate"><span class="pre">ConfigMap</span></code> name and the
<strong>default</strong> configuration (e.g. a100-40gb):</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>patch<span class="w"> </span>clusterpolicy/cluster-policy<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--type<span class="w"> </span>merge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>-p<span class="w"> </span><span class="s1">&#39;{&quot;spec&quot;: {&quot;devicePlugin&quot;: {&quot;config&quot;: {&quot;name&quot;: &quot;time-slicing-config&quot;, &quot;default&quot;: &quot;a100-40gb&quot;}}}}&#39;</span>
</pre></div>
</div>
<p>Verify that the time-slicing configuration is applied successfully to all
GPU nodes in the cluster:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>describe<span class="w"> </span>node<span class="w"> </span>&lt;node-name&gt;
<span class="go">...</span>
<span class="go">Capacity:</span>
<span class="go">nvidia.com/gpu: 8</span>
<span class="go">...</span>
<span class="go">Allocatable:</span>
<span class="go">nvidia.com/gpu: 8</span>
<span class="go">...</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In this example it is assumed that node <code class="docutils literal notranslate"><span class="pre">&lt;node-name&gt;</span></code> has one GPU.</p>
</div>
</section>
<section id="applying-a-time-slicing-configuration-per-node">
<h3>Applying a Time-Slicing Configuration Per Node<a class="headerlink" href="#applying-a-time-slicing-configuration-per-node" title="Permalink to this heading"></a></h3>
<p>To enable a time-slicing configuration per node, the user would need to
apply the <code class="docutils literal notranslate"><span class="pre">nvidia.com/device-plugin.config=&lt;config-name&gt;</span></code> node label after
installing the GPU Operator. On applying this label, the
NVIDIA Kubernetes Device Plugin will configure node GPU resources accordingly.</p>
<p>Install the GPU Operator by passing a time-slicing <code class="docutils literal notranslate"><span class="pre">ConfigMap</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>gpu-operator<span class="w"> </span>nvidia/gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>devicePlugin.config.name<span class="o">=</span>time-slicing-config
</pre></div>
</div>
<p>Label the node with the required time-slicing configuration (e.g. <code class="docutils literal notranslate"><span class="pre">a100-40gb</span></code>) in the <code class="docutils literal notranslate"><span class="pre">ConfigMap</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>label<span class="w"> </span>node<span class="w"> </span>&lt;node-name&gt;<span class="w"> </span>nvidia.com/device-plugin.config<span class="o">=</span>a100-40gb
</pre></div>
</div>
<p>Verify that the time-slicing configuration is applied successfully:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>describe<span class="w"> </span>node<span class="w"> </span>&lt;node-name&gt;
<span class="go">...</span>
<span class="go">Capacity:</span>
<span class="go">nvidia.com/gpu: 8</span>
<span class="go">...</span>
<span class="go">Allocatable:</span>
<span class="go">nvidia.com/gpu: 8</span>
<span class="go">...</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In this example it is assumed that node <code class="docutils literal notranslate"><span class="pre">&lt;node-name&gt;</span></code> has one GPU.</p>
</div>
</section>
<section id="changes-to-node-labels-by-the-gpu-feature-discovery-plugin">
<h3>Changes to Node Labels by the GPU Feature Discovery Plugin<a class="headerlink" href="#changes-to-node-labels-by-the-gpu-feature-discovery-plugin" title="Permalink to this heading"></a></h3>
<p>In addition to the standard node labels applied by the GPU Feature
Discovery Plugin (GFD), the following label
is also included when deploying
the plugin with the time-slicing configurations described above.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>nvidia.com/&lt;resource-name&gt;.replicas = &lt;num-replicas&gt;
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">&lt;num-replicas&gt;</span></code> is the factor by which each resource of <code class="docutils literal notranslate"><span class="pre">&lt;resource-name&gt;</span></code> is oversubscribed.</p>
<p>Additionally, <code class="docutils literal notranslate"><span class="pre">nvidia.com/&lt;resource-name&gt;.product</span></code> is modified as follows if <code class="docutils literal notranslate"><span class="pre">renameByDefault=false</span></code>:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>nvidia.com/&lt;resource-name&gt;.product = &lt;product name&gt;-SHARED
</pre></div>
</div>
<p>Using these labels, you can select a shared vs. non-shared GPU
in the same way as traditionally
selecting one GPU model over another. That is, the <code class="docutils literal notranslate"><span class="pre">SHARED</span></code> annotation ensures that
the <code class="docutils literal notranslate"><span class="pre">nodeSelector</span></code> can be used to attract
pods to nodes with shared GPUs.</p>
<p>Because having <code class="docutils literal notranslate"><span class="pre">renameByDefault=true</span></code> already encodes the fact that the
resource is shared on the resource name,
there is no need to annotate the product name with <code class="docutils literal notranslate"><span class="pre">SHARED</span></code>. You can already
find needed shared resources by simply requesting it in the pod specification.</p>
<p>When running with <code class="docutils literal notranslate"><span class="pre">renameByDefault=false</span></code> and <code class="docutils literal notranslate"><span class="pre">migStrategy=single</span></code>,
both the MIG profile name and the new <code class="docutils literal notranslate"><span class="pre">SHARED</span></code> annotation
are appended to the product name, like this:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>nvidia.com/gpu.product = A100-SXM4-40GB-MIG-1g.5gb-SHARED
</pre></div>
</div>
</section>
<section id="supported-resource-types">
<h3>Supported Resource Types<a class="headerlink" href="#supported-resource-types" title="Permalink to this heading"></a></h3>
<p>Currently, the only supported resource types are <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu</span></code>
and any of the resource types that emerge from configuring a node with
the mixed MIG strategy.</p>
<p>For example, the full set of time-sliceable resources on a T4 card would
be:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">nvidia.com/gpu</span>
</pre></div>
</div>
<p>And the full set of time-sliceable resources on an A100 40GB card would be:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">nvidia.com/gpu</span>
<span class="go">nvidia.com/mig-1g.5gb</span>
<span class="go">nvidia.com/mig-2g.10gb</span>
<span class="go">nvidia.com/mig-3g.20gb</span>
<span class="go">nvidia.com/mig-7g.40gb</span>
</pre></div>
</div>
<p>Likewise, on an A100 80GB card, they would be:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">nvidia.com/gpu</span>
<span class="go">nvidia.com/mig-1g.10gb</span>
<span class="go">nvidia.com/mig-2g.20gb</span>
<span class="go">nvidia.com/mig-3g.40gb</span>
<span class="go">nvidia.com/mig-7g.80gb</span>
</pre></div>
</div>
</section>
</section>
<section id="testing-gpu-time-slicing-with-the-nvidia-gpu-operator">
<h2>Testing GPU Time-Slicing with the NVIDIA GPU Operator<a class="headerlink" href="#testing-gpu-time-slicing-with-the-nvidia-gpu-operator" title="Permalink to this heading"></a></h2>
<p>This section covers a workload test scenario to validate GPU time-slicing with GPU resources.</p>
<ol class="arabic simple">
<li><p>Create a workload test file <code class="docutils literal notranslate"><span class="pre">plugin-test.yaml</span></code> as follows:</p></li>
</ol>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia-plugin-test</span>
<span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia-plugin-test</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<span class="w">  </span><span class="nt">selector</span><span class="p">:</span>
<span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span>
<span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia-plugin-test</span>
<span class="w">  </span><span class="nt">template</span><span class="p">:</span>
<span class="w">    </span><span class="nt">metadata</span><span class="p">:</span>
<span class="w">      </span><span class="nt">labels</span><span class="p">:</span>
<span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia-plugin-test</span>
<span class="w">    </span><span class="nt">spec</span><span class="p">:</span>
<span class="w">      </span><span class="nt">tolerations</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia.com/gpu</span>
<span class="w">          </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Exists</span>
<span class="w">          </span><span class="nt">effect</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NoSchedule</span>
<span class="w">      </span><span class="nt">containers</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">dcgmproftester11</span>
<span class="w">          </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia/samples:dcgmproftester-2.0.10-cuda11.0-ubuntu18.04</span>
<span class="w">          </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;/bin/sh&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;-c&quot;</span><span class="p p-Indicator">]</span>
<span class="w">          </span><span class="nt">args</span><span class="p">:</span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">while true; do /usr/bin/dcgmproftester11 --no-dcgm-validation -t 1004 -d 300; sleep 30; done</span>
<span class="w">          </span><span class="nt">resources</span><span class="p">:</span>
<span class="w">           </span><span class="nt">limits</span><span class="p">:</span>
<span class="w">             </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">          </span><span class="nt">securityContext</span><span class="p">:</span>
<span class="w">            </span><span class="nt">capabilities</span><span class="p">:</span>
<span class="w">              </span><span class="nt">add</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;SYS_ADMIN&quot;</span><span class="p p-Indicator">]</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Create a deployment with multiple replicas:</p></li>
</ol>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kubectl apply -f plugin-test.yaml</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Verify that all five replicas are running:</p></li>
</ol>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kubectl get pods</span>
<span class="go">kubectl exec &lt;driver-pod-name&gt; -n gpu-operator -- nvidia-smi</span>
</pre></div>
</div>
<p>Your output should look something like this:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">NAME                                  READY   STATUS    RESTARTS   AGE</span>
<span class="go">nvidia-plugin-test-8479c8f7c8-4tnsn   1/1     Running   0          6s</span>
<span class="go">nvidia-plugin-test-8479c8f7c8-cdgdb   1/1     Running   0          6s</span>
<span class="go">nvidia-plugin-test-8479c8f7c8-q2vn7   1/1     Running   0          6s</span>
<span class="go">nvidia-plugin-test-8479c8f7c8-t9d4b   1/1     Running   0          6s</span>
<span class="go">nvidia-plugin-test-8479c8f7c8-xggls   1/1     Running   0          6s</span>
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>&lt;driver-pod-name&gt;<span class="w"> </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--<span class="w"> </span>nvidia-smi
</pre></div>
</div>
<p>Your output should look something like this:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">+-----------------------------------------------------------------------------+</span>
<span class="go">| NVIDIA-SMI 510.73.08    Driver Version: 510.73.08    CUDA Version: 11.6     |</span>
<span class="go">|-------------------------------+----------------------+----------------------+</span>
<span class="go">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span>
<span class="go">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span>
<span class="go">|                               |                      |               MIG M. |</span>
<span class="go">|===============================+======================+======================|</span>
<span class="go">|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |</span>
<span class="go">| N/A   44C    P0    70W /  70W |   1577MiB / 15360MiB |    100%      Default |</span>
<span class="go">|                               |                      |                  N/A |</span>
<span class="go">+-------------------------------+----------------------+----------------------+</span>

<span class="go">+-----------------------------------------------------------------------------+</span>
<span class="go">| Processes:                                                                  |</span>
<span class="go">|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |</span>
<span class="go">|        ID   ID                                                   Usage      |</span>
<span class="go">|=============================================================================|</span>
<span class="go">|    0   N/A  N/A      3666      C   /usr/bin/dcgmproftester11         315MiB |</span>
<span class="go">|    0   N/A  N/A      3679      C   /usr/bin/dcgmproftester11         315MiB |</span>
<span class="go">|    0   N/A  N/A      3992      C   /usr/bin/dcgmproftester11         315MiB |</span>
<span class="go">|    0   N/A  N/A      4119      C   /usr/bin/dcgmproftester11         315MiB |</span>
<span class="go">|    0   N/A  N/A      4324      C   /usr/bin/dcgmproftester11         315MiB |</span>
<span class="go">+-----------------------------------------------------------------------------+</span>
</pre></div>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading"></a></h2>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://developer.nvidia.com/blog/improving-gpu-utilization-in-kubernetes">Blog post on GPU sharing in Kubernetes</a>.</p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/k8s-device-plugin#shared-access-to-gpus-with-cuda-time-slicing">NVIDIA Kubernetes Device Plugin</a>.</p></li>
</ol>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="gpu-operator-mig.html" class="btn btn-neutral float-left" title="GPU Operator with MIG" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="gpu-operator-rdma.html" class="btn btn-neutral float-right" title="GPUDirect RDMA" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018-2023, NVIDIA Corporation.
      <span class="lastupdated">Last updated on 2023-03-14.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>  

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  .rst-content dl:not(.docutils) dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }
  </style>
  

</body>
</html>