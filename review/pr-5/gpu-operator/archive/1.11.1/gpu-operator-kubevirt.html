<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Running KubeVirt VMs with the GPU Operator &mdash; NVIDIA Cloud Native Technologies  documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../../../_static/nvidia.ico"/>
    <link rel="canonical" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/archive/1.11.1/gpu-operator-kubevirt.html"/>
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/sphinx_highlight.js"></script>
        <script src="../../../_static/clipboard.min.js"></script>
        <script src="../../../_static/copybutton.js"></script>
        <script src="../../../_static/js/google-analytics/google-analytics-tracker.js"></script>
        <script src="../../../_static/js/google-analytics/google-analytics-write.js"></script>
        <script src="//assets.adobedtm.com/b92787824f2e0e9b68dc2e993f9bd995339fe417/satelliteLib-7ba51e58dc61bcb0e9311aadd02a0108ab24cc6c.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Appendix" href="appendix.html" />
    <link rel="prev" title="GPUDirect RDMA" href="gpu-operator-rdma.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" > 
            <a href="../../../contents.html">
            <img src="../../../_static/NVLogo_H_B&W.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }

    .wy-nav-content {
      max-width: 1000px;
    }
  </style>
  
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">NVIDIA Container Toolkit:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/concepts.html">Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/arch-overview.html">Architecture Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/install-guide.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/user-guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/archive.html">Archive</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">NVIDIA GPU Operator:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../platform-support.html">Platform Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gpu-driver-upgrades.html">GPU Driver Upgrades</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install-gpu-operator-vgpu.html">NVIDIA vGPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install-gpu-operator-nvaie.html">NVIDIA AI Enterprise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../openshift/contents.html">GPU Operator on OpenShift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gpu-operator-mig.html">GPU Operator with MIG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gpu-sharing.html">Time-Slicing GPUs in Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gpu-operator-rdma.html">GPUDirect RDMA and GPUDirect Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gpu-operator-kubevirt.html">GPU Operator with KubeVirt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../appendix.html">Appendix</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../archive.html">Archive</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id1">22.9.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id2">22.9.0</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../archive.html#id3">1.11.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id4">1.11.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id5">1.10.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id6">1.9.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id7">1.9.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id8">1.8</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Kubernetes with GPUs:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../kubernetes/install-k8s.html">Install Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../kubernetes/mig-k8s.html">MIG Support in Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../kubernetes/anthos-guide.html">NVIDIA GPUs with Google Cloud’s Anthos</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Telemetry:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../gpu-telemetry/dcgm-exporter.html">DCGM-Exporter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../gpu-telemetry/dcgm-exporter.html#integrating-gpu-telemetry-into-kubernetes">Integrating GPU Telemetry into Kubernetes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multi-Instance GPU:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../mig/mig.html">Multi-Instance GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mig/mig-k8s.html">MIG Support in Kubernetes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Driver Containers:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../driver-containers/overview.html">Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Playground:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../playground/dind.html">Docker-in-Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../playground/x-arch.html">Running Cross-Architecture Containers</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../contents.html">NVIDIA Cloud Native Technologies</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../contents.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../archive.html">Archive</a> &raquo;</li>
      <li>Running KubeVirt VMs with the GPU Operator</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="running-kubevirt-vms-with-the-gpu-operator">
<span id="gpu-operator-kubevirt-1-11-1"></span><h1>Running KubeVirt VMs with the GPU Operator<a class="headerlink" href="#running-kubevirt-vms-with-the-gpu-operator" title="Permalink to this heading"></a></h1>
<section id="introduction">
<span id="gpu-operator-kubevirt-1-11-1-introduction"></span><h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading"></a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This feature is introduced as a technical preview with the GPU Operator 1.11.0 release. This is not ready for production use. Please submit feedback and bug reports <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues">here</a>. We encourage contributions in our <a class="reference external" href="https://gitlab.com/nvidia/kubernetes/gpu-operator">Gitlab repository</a>.</p>
</div>
<p><a class="reference external" href="https://kubevirt.io/">KubeVirt</a> is a virtual machine management add-on to Kubernetes that allows you to run and manage VMs in a Kubernetes cluster. It eliminates the need to manage separate clusters for VM and container workloads, as both can now coexist in a single Kubernetes cluster.</p>
<p>Up until this point, the GPU Operator only provisioned worker nodes for running GPU-accelerated containers. Now, the GPU Operator can also be used to provision worker nodes for running GPU-accelerated VMs.</p>
<p>The prerequisites needed for running containers and VMs with GPU(s) differs, with the primary difference being the drivers required. For example, the datacenter driver is needed for containers, the vfio-pci driver is needed for GPU passthrough, and the <a class="reference external" href="https://docs.nvidia.com/grid/latest/grid-vgpu-user-guide/index.html#installing-configuring-grid-vgpu">NVIDIA vGPU Manager</a> is needed for creating vGPU devices.</p>
<p>The GPU Operator can now be configured to deploy different software components on worker nodes depending on what GPU workload is configured to run on those nodes. Consider the following example.</p>
<div class="line-block">
<div class="line">Node A is configured to run containers.</div>
<div class="line">Node B is configured to run VMs with Passthrough GPU.</div>
<div class="line">Node C is configured to run VMs with vGPU.</div>
</div>
<p>Node A receives the following software components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Datacenter</span> <span class="pre">Driver</span></code> - to install the driver</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Container</span> <span class="pre">Toolkit</span></code> - to ensure containers can properly access GPUs</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Kubernetes</span> <span class="pre">Device</span> <span class="pre">Plugin</span></code> - to discover and advertise GPU resources to kubelet</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">DCGM</span> <span class="pre">and</span> <span class="pre">DCGM</span> <span class="pre">Exporter</span></code> - to monitor the GPU(s)</p></li>
</ul>
<p>Node B receives the following software components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">VFIO</span> <span class="pre">Manager</span></code> - to load <cite>vfio-pci</cite> and bind it to all GPUs on the node</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sandbox</span> <span class="pre">Device</span> <span class="pre">Plugin</span></code> - to discover and advertise the passthrough GPUs to kubelet</p></li>
</ul>
<p>Node C receives the following software components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">vGPU</span> <span class="pre">Manager</span></code> - to install the driver</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">vGPU</span> <span class="pre">Device</span> <span class="pre">Manager</span></code> - to create vGPU devices on the node</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sandbox</span> <span class="pre">Device</span> <span class="pre">Plugin</span></code> - to discover and advertise the vGPU devices to kubelet</p></li>
</ul>
</section>
<section id="limitations">
<span id="gpu-operator-kubevirt-1-11-1-limitations"></span><h2>Limitations<a class="headerlink" href="#limitations" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p>This feature is a Technical Preview and is not ready for production use.</p></li>
<li><p>Trying out this feature requires a fresh install of GPU Operator 1.11 with necessary fields set in ClusterPolicy as detailed in this document. The instructions in this document are not valid if upgrading from 1.10 to 1.11.</p></li>
<li><p>Enabling / disabling this feature post-install is not supported.</p></li>
<li><p>MIG-backed vGPUs are not supported.</p></li>
<li><p>A GPU worker node can run GPU workloads of a particular type - containers, VMs with GPU Passthrough, and VMs with vGPU - but not a combination of any of them.</p></li>
</ul>
</section>
<section id="install-the-gpu-operator">
<h2>Install the GPU Operator<a class="headerlink" href="#install-the-gpu-operator" title="Permalink to this heading"></a></h2>
<p>To enable this functionality, install the GPU Operator and set the following parameter in <code class="docutils literal notranslate"><span class="pre">ClusterPolicy</span></code>: <code class="docutils literal notranslate"><span class="pre">sandboxWorkloads.enabled=true</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The term <code class="docutils literal notranslate"><span class="pre">sandboxing</span></code> refers to running software in a separate isolated environment, typically for added security (i.e. a virtual machine). We use the term <code class="docutils literal notranslate"><span class="pre">sandbox</span> <span class="pre">workloads</span></code> to signify workloads that run in a virtual machine, irrespective of the virtualization technology used.</p>
</div>
</section>
<section id="partition-cluster-based-the-gpu-workload">
<h2>Partition Cluster based the GPU Workload<a class="headerlink" href="#partition-cluster-based-the-gpu-workload" title="Permalink to this heading"></a></h2>
<p>When sandbox workloads are enabled (<code class="docutils literal notranslate"><span class="pre">sandboxWorkloads.enabled=true</span></code>), a worker node can run GPU workloads of a particular type – containers, VMs with GPU passthrough, or VMs with vGPU –  but not a combination of any of them. As illustrated in the <a class="reference internal" href="#gpu-operator-kubevirt-1-11-1-introduction"><span class="std std-ref">Introduction</span></a>, the GPU Operator will deploy a specific set of operands on a worker node depending on the workload type configured. For example, a node which is configured to run containers will receive the <code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Datacenter</span> <span class="pre">Driver</span></code>, while a node which is configured to run VMs with vGPU will receive the <code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">vGPU</span> <span class="pre">Manager</span></code>.</p>
<p>To set the GPU workload configuration for a worker node, apply the node label <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.workload.config=&lt;config&gt;</span></code>, where the valid config values are <code class="docutils literal notranslate"><span class="pre">container</span></code>, <code class="docutils literal notranslate"><span class="pre">vm-passthrough</span></code>, and <code class="docutils literal notranslate"><span class="pre">vm-vgpu</span></code>.</p>
<p>If the node label <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.workload.config</span></code> does not exist on the node, the GPU Operator will assume the default GPU workload configuration, <code class="docutils literal notranslate"><span class="pre">container</span></code>. To override the default GPU workload configuration, set the following value in <code class="docutils literal notranslate"><span class="pre">ClusterPolicy</span></code> during install: <code class="docutils literal notranslate"><span class="pre">sandboxWorkloads.defaultWorkload=&lt;config&gt;</span></code>.</p>
<p>Consider the following example:</p>
<p>GPU Operator is installed with the following options: <code class="docutils literal notranslate"><span class="pre">sandboxWorkloads.enabled=true</span> <span class="pre">sandboxWorkloads.defaultWorkload=container</span></code></p>
<div class="line-block">
<div class="line">Node A is <cite>not</cite> labeled with <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.workload.config</span></code>.</div>
<div class="line">Node B is labeled with <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.workload.config=vm-passthrough</span></code>.</div>
<div class="line">Node C is labeled with <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.workload.config=vm-vgpu</span></code>.</div>
</div>
<div class="line-block">
<div class="line">Node A gets provisioned for containers.</div>
<div class="line">Node B gets provisioned for GPU Passthrough.</div>
<div class="line">Node C gets provisioned for vGPU.</div>
</div>
</section>
<section id="deployment-scenarios">
<h2>Deployment Scenarios<a class="headerlink" href="#deployment-scenarios" title="Permalink to this heading"></a></h2>
<section id="running-vms-with-gpu-passthrough">
<h3>Running VMs with GPU Passthrough<a class="headerlink" href="#running-vms-with-gpu-passthrough" title="Permalink to this heading"></a></h3>
<p>This section runs through the deployment scenario of running VMs with GPU Passthrough. We will first deploy the GPU Operator, such that our worker node will be provisioned for GPU Passthrough, then we will deploy a KubeVirt VM which requests a GPU.</p>
<p>By default, to provision GPU Passthrough, the GPU Operator will deploy the following components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">VFIO</span> <span class="pre">Manager</span></code> - to load <code class="docutils literal notranslate"><span class="pre">vfio-pci</span></code> and bind it to all GPUs on the node</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sandbox</span> <span class="pre">Device</span> <span class="pre">Plugin</span></code> - to discover and advertise the passthrough GPUs to kubelet</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sandbox</span> <span class="pre">Validator</span></code> - to validate the other operands</p></li>
</ul>
<section id="id1">
<h4>Install the GPU Operator<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h4>
<p>Follow the below steps.</p>
<p>Label the worker node explicitly for GPU passthrough workloads:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>label<span class="w"> </span>node<span class="w"> </span>&lt;node-name&gt;<span class="w"> </span>--overwrite<span class="w"> </span>nvidia.com/gpu.workload.config<span class="o">=</span>vm-passthrough
</pre></div>
</div>
<p>Install the GPU Operator with sandbox workloads enabled:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>gpu-operator<span class="w"> </span>nvidia/gpu-operator<span class="w"> </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>–set<span class="w"> </span>sandboxWorkloads.enabled<span class="o">=</span><span class="nb">true</span>
</pre></div>
</div>
<p>The following operands get deployed. Ensure all pods are in a running state and all validations succeed with the <code class="docutils literal notranslate"><span class="pre">sandbox-validator</span></code> component:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>gpu-operator
<span class="go">NAME                                                          READY   STATUS    RESTARTS   AGE</span>
<span class="go">...</span>
<span class="go">nvidia-sandbox-device-plugin-daemonset-4mxsc                  1/1     Running   0          40s</span>
<span class="go">nvidia-sandbox-validator-vxj7t                                1/1     Running   0          40s</span>
<span class="go">nvidia-vfio-manager-thfwf                                     1/1     Running   0          78s</span>
</pre></div>
</div>
<p>The vfio-manager pod will bind all GPUs on the node to the vfio-pci driver:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>lspci<span class="w"> </span>--nnk<span class="w"> </span>-d<span class="w"> </span>10de:
<span class="go">3b:00.0 3D controller [0302]: NVIDIA Corporation Device [10de:2236] (rev a1)</span>
<span class="go">       Subsystem: NVIDIA Corporation Device [10de:1482]</span>
<span class="go">       Kernel driver in use: vfio-pci</span>
<span class="go">       Kernel modules: nvidiafb, nouveau</span>
<span class="go">86:00.0 3D controller [0302]: NVIDIA Corporation Device [10de:2236] (rev a1)</span>
<span class="go">       Subsystem: NVIDIA Corporation Device [10de:1482]</span>
<span class="go">       Kernel driver in use: vfio-pci</span>
<span class="go">       Kernel modules: nvidiafb, nouveau</span>
</pre></div>
</div>
<p>The sandbox-device-plugin will discover and advertise these resources to kubelet. In this example, we have two A10 GPUs:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>describe<span class="w"> </span>node<span class="w"> </span>&lt;node-name&gt;
<span class="go">...</span>
<span class="go">Capacity:</span>
<span class="go">  ...</span>
<span class="go">  nvidia.com/GA102GL_A10:         2</span>
<span class="go">  ...</span>
<span class="go">Allocatable:</span>
<span class="go">  ...</span>
<span class="go">  nvidia.com/GA102GL_A10:         2</span>
<span class="go">...</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The resource name is currently constructed by joining the <cite>device</cite> and <cite>device_name</cite> columns from the <a class="reference external" href="https://pci-ids.ucw.cz/v2.2/pci.ids">PCI IDs database</a>. For example, the entry for A10 in the database reads <code class="docutils literal notranslate"><span class="pre">2236</span>&#160; <span class="pre">GA102GL</span> <span class="pre">[A10]</span></code>, which results in a resource name <code class="docutils literal notranslate"><span class="pre">nvidia.com/GA102GL_A10</span></code>.</p>
</div>
</section>
<section id="update-the-kubevirt-cr">
<h4>Update the KubeVirt CR<a class="headerlink" href="#update-the-kubevirt-cr" title="Permalink to this heading"></a></h4>
<p>Next, we will update the KubeVirt Custom Resource, as documented in the <a class="reference external" href="https://kubevirt.io/user-guide/virtual_machines/host-devices/#listing-permitted-devices">KubeVirt user guide</a>, so that the passthrough GPUs are permitted and can be requested by a KubeVirt VM. Note, replace the values for <code class="docutils literal notranslate"><span class="pre">pciVendorSelector</span></code> and <code class="docutils literal notranslate"><span class="pre">resourceName</span></code> to correspond to your GPU model. We set <code class="docutils literal notranslate"><span class="pre">externalResourceProvider=true</span></code> to indicate that this resource is being provided by an external device plugin, in this case the <code class="docutils literal notranslate"><span class="pre">sandbox-device-plugin</span></code> which is deployed by the Operator.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To find the device ID for a particular GPU, search by device name in the <a class="reference external" href="https://pci-ids.ucw.cz/v2.2/pci.ids">PCI IDs database</a>.</p>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>edit<span class="w"> </span>kubevirt<span class="w"> </span>-n<span class="w"> </span>kubevirt
<span class="go">  ...</span>
<span class="go">  spec:</span>
<span class="go">    configuration:</span>
<span class="go">    developerConfiguration:</span>
<span class="go">      featureGates:</span>
<span class="go">      - GPU</span>
<span class="go">    permittedHostDevices:</span>
<span class="go">      pciHostDevices:</span>
<span class="go">      - externalResourceProvider: true</span>
<span class="go">        pciVendorSelector: 10DE:2236</span>
<span class="go">        resourceName: nvidia.com/GA102GL_A10</span>
<span class="go">  ...</span>
</pre></div>
</div>
</section>
<section id="create-a-vm">
<h4>Create a VM<a class="headerlink" href="#create-a-vm" title="Permalink to this heading"></a></h4>
<p>We are now ready to create a VM. Let’s create a sample VM using a simple VMI spec which requests a nvidia.com/GA102GL_A10 resource:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">---</span>
<span class="go">apiVersion: kubevirt.io/v1alpha3</span>
<span class="go">kind: VirtualMachineInstance</span>
<span class="go">metadata:</span>
<span class="go">  labels:</span>
<span class="go">    special: vmi-gpu</span>
<span class="go">  name: vmi-gpu</span>
<span class="go">spec:</span>
<span class="go">  domain:</span>
<span class="go">    devices:</span>
<span class="go">      disks:</span>
<span class="go">      - disk:</span>
<span class="go">          bus: virtio</span>
<span class="go">        name: containerdisk</span>
<span class="go">      - disk:</span>
<span class="go">          bus: virtio</span>
<span class="go">        name: cloudinitdisk</span>
<span class="go">      gpus:</span>
<span class="go">      - deviceName: nvidia.com/GA102GL_A10</span>
<span class="go">        name: gpu1</span>
<span class="go">      rng: {}</span>
<span class="go">    machine:</span>
<span class="go">      type: &quot;&quot;</span>
<span class="go">    resources:</span>
<span class="go">      requests:</span>
<span class="go">        memory: 1024M</span>
<span class="go">  terminationGracePeriodSeconds: 0</span>
<span class="go">  volumes:</span>
<span class="go">  - containerDisk:</span>
<span class="go">      image: docker.io/kubevirt/fedora-cloud-container-disk-demo:devel</span>
<span class="go">    name: containerdisk</span>
<span class="go">  - cloudInitNoCloud:</span>
<span class="go">      userData: |-</span>
<span class="gp">        #</span>cloud-config
<span class="go">        password: fedora</span>
<span class="go">        chpasswd: { expire: False }</span>
<span class="go">    name: cloudinitdisk</span>
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>vmi-gpu.yaml
<span class="go">virtualmachineinstance.kubevirt.io/vmi-gpu created</span>

<span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>vmis
<span class="go">NAME      AGE   PHASE     IP               NODENAME       READY</span>
<span class="go">vmi-gpu   13s   Running   192.168.47.210   cnt-server-2   True</span>
</pre></div>
</div>
<p>Let’s console into the VM and verify we have a GPU. Refer <a class="reference external" href="https://kubevirt.io/user-guide/operations/virtctl_client_tool/">here</a> for installing virtctl.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>./virtctl<span class="w"> </span>console<span class="w"> </span>vmi-gpu
<span class="go">Successfully connected to vmi-gpu console. The escape sequence is ^]</span>

<span class="go">vmi-gpu login: fedora</span>
<span class="go">Password:</span>
<span class="gp">[fedora@vmi-gpu ~]$ </span>sudo<span class="w"> </span>yum<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>-q<span class="w"> </span>pciutils
<span class="gp">[fedora@vmi-gpu ~]$ </span>lspci<span class="w"> </span>-nnk<span class="w"> </span>-d<span class="w"> </span>10de:
<span class="go">06:00.0 3D controller [0302]: NVIDIA Corporation GA102GL [A10] [10de:2236] (rev a1)</span>
<span class="go">       Subsystem: NVIDIA Corporation Device [10de:1482]</span>
</pre></div>
</div>
</section>
</section>
<section id="running-vms-with-vgpu">
<h3>Running VMs with vGPU<a class="headerlink" href="#running-vms-with-vgpu" title="Permalink to this heading"></a></h3>
<p>This section runs through the deployment scenario of running VMs with vGPU. We will first deploy the GPU Operator, such that our worker node will be provisioned for vGPU, then we will deploy a KubeVirt VM which requests a vGPU.</p>
<p>By default, to provision vGPU, the GPU Operator will deploy the following components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">vGPU</span> <span class="pre">Manager</span></code> - installs vGPU Manager on the node</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">vGPU</span> <span class="pre">Device</span> <span class="pre">Manager</span></code> - creates vGPU devices on the node after vGPU Manager is installed</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sandbox</span> <span class="pre">Device</span> <span class="pre">Plugin</span></code> - to discover and advertise the vGPU devices to kubelet</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sandbox</span> <span class="pre">Validator</span></code> - to validate the other operands</p></li>
</ul>
<section id="build-the-vgpu-manager-image">
<h4>Build the vGPU Manager Image<a class="headerlink" href="#build-the-vgpu-manager-image" title="Permalink to this heading"></a></h4>
<p>Building the vGPU Manager container and pushing it to a private registry is a prerequisite. To fulfill this prerequisite, follow the below steps.</p>
<p>Download the vGPU Software from the <a class="reference external" href="https://nvid.nvidia.com/dashboard/#/dashboard">NVIDIA Licensing Portal</a>.</p>
<ul class="simple">
<li><p>Login to the NVIDIA Licensing Portal and navigate to the <cite>Software Downloads</cite> section.</p></li>
<li><p>The NVIDIA vGPU Software is located in the Software Downloads section of the NVIDIA Licensing Portal.</p></li>
<li><p>The vGPU Software bundle is packaged as a zip file. Download and unzip the bundle to obtain the NVIDIA vGPU Manager for Linux (<code class="docutils literal notranslate"><span class="pre">NVIDIA-Linux-x86_64-&lt;version&gt;-vgpu-kvm.run</span></code> file)</p></li>
</ul>
<p>Next, clone the driver container repository and build the driver image with the following steps.</p>
<p>Open a terminal and clone the driver container image repository.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>git<span class="w"> </span>clone<span class="w"> </span>https://gitlab.com/nvidia/container-images/driver
<span class="gp">$ </span><span class="nb">cd</span><span class="w"> </span>driver
</pre></div>
</div>
<p>Change to the vgpu-manager directory for your OS. We use Ubuntu 20.04 as an example.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nb">cd</span><span class="w"> </span>vgpu-manager/ubuntu20.04
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For RedHat OpenShift, run <code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">vgpu-manager/rhel</span></code> to use the <code class="docutils literal notranslate"><span class="pre">rhel</span></code> folder instead.</p>
</div>
<p>Copy the NVIDIA vGPU Manager from your extracted zip file</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cp<span class="w"> </span>&lt;local-driver-download-directory&gt;/*-vgpu-kvm.run<span class="w"> </span>./
</pre></div>
</div>
<div class="line-block">
<div class="line">Set the following environment variables:</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">PRIVATE_REGISTRY</span></code> - name of private registry used to store driver image</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">VERSION</span></code> - NVIDIA vGPU Manager version downloaded from NVIDIA Software Portal</div>
<div class="line"><code class="docutils literal notranslate"><span class="pre">OS_TAG</span></code> - this must match the Guest OS version. In the below example <code class="docutils literal notranslate"><span class="pre">ubuntu20.04</span></code> is used. For RedHat OpenShift this should be set to <code class="docutils literal notranslate"><span class="pre">rhcos4.x</span></code> where x is the supported minor OCP version.</div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nb">export</span><span class="w"> </span><span class="nv">PRIVATE_REGISTRY</span><span class="o">=</span>my/private/registry<span class="w"> </span><span class="nv">VERSION</span><span class="o">=</span><span class="m">510</span>.73.06<span class="w"> </span><span class="nv">OS_TAG</span><span class="o">=</span>ubuntu20.04
</pre></div>
</div>
<p>Build the NVIDIA vGPU Manager image.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>build<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>–build-arg<span class="w"> </span><span class="nv">DRIVER_VERSION</span><span class="o">=</span><span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-t<span class="w"> </span><span class="si">${</span><span class="nv">PRIVATE_REGISTRY</span><span class="si">}</span>/vgpu-manager:<span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span>-<span class="si">${</span><span class="nv">OS_TAG</span><span class="si">}</span><span class="w"> </span>.
</pre></div>
</div>
<p>Push NVIDIA vGPU Manager image to your private registry.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>push<span class="w"> </span><span class="si">${</span><span class="nv">PRIVATE_REGISTRY</span><span class="si">}</span>/vgpu-manager:<span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span>-<span class="si">${</span><span class="nv">OS_TAG</span><span class="si">}</span>
</pre></div>
</div>
</section>
<section id="id4">
<h4>Install the GPU Operator<a class="headerlink" href="#id4" title="Permalink to this heading"></a></h4>
<p>Follow the below steps.</p>
<p>Label the worker node explicitly for vGPU workloads:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>label<span class="w"> </span>node<span class="w"> </span>&lt;node-name&gt;<span class="w"> </span>--overwrite<span class="w"> </span>nvidia.com/gpu.workload.config<span class="o">=</span>vm-vgpu
</pre></div>
</div>
<p>Create a configuration file named <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> for the vGPU Device Manager.  This file contains a list of vGPU device configurations. Each named configuration contains a list of desired vGPU types. The vGPU Device Manager reads the configuration file and applies a specific named configuration when creating vGPU devices on the node. Download the comprehensive example file as a starting point, and modify as needed:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>wget<span class="w"> </span>-O<span class="w"> </span>config.yaml<span class="w"> </span>https://raw.githubusercontent.com/NVIDIA/vgpu-device-manager/main/examples/config-example.yaml
</pre></div>
</div>
<p>Optionally, label the worker node explicitly with a vGPU devices config. More information on vGPU devices config is detailed in <a class="reference internal" href="#apply-new-vgpu-device-config-1-11-1"><span class="std std-ref">this section</span></a> below.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>label<span class="w"> </span>node<span class="w"> </span>&lt;node-name&gt;<span class="w"> </span>--overwrite<span class="w"> </span>nvidia.com/vgpu.config<span class="o">=</span>&lt;config-name&gt;
</pre></div>
</div>
<p>Create a namespace for GPU Operator:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>create<span class="w"> </span>namespace<span class="w"> </span>gpu-operator
</pre></div>
</div>
<p>Create a ConfigMap for the vGPU devices config:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>create<span class="w"> </span>cm<span class="w"> </span>vgpu-devices-config<span class="w"> </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>–from-file<span class="o">=</span>config.yaml
</pre></div>
</div>
<p>Install the GPU Operator with sandbox workloads enabled and specify the vGPU Manager image built previously:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>gpu-operator<span class="w"> </span>nvidia/gpu-operator<span class="w"> </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>–set<span class="w"> </span>sandboxWorkloads.enabled<span class="o">=</span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>–set<span class="w"> </span>vgpuManager.repository<span class="o">=</span>&lt;path<span class="w"> </span>to<span class="w"> </span>private<span class="w"> </span>repository&gt;
<span class="go">    –set vgpuManager.image=vgpu-manager</span>
<span class="go">    –set vgpuManager.version=&lt;driver version&gt;</span>
</pre></div>
</div>
<p>The following operands get deployed. Ensure all pods are in a running state and all validations succeed with the <code class="docutils literal notranslate"><span class="pre">sandbox-validator</span></code> component.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>gpu-operator
<span class="go">NAME                                                          READY   STATUS    RESTARTS   AGE</span>
<span class="go">...</span>
<span class="go">nvidia-sandbox-device-plugin-daemonset-kkdt9                  1/1     Running   0          9s</span>
<span class="go">nvidia-sandbox-validator-jcpgw                                1/1     Running   0          9s</span>
<span class="go">nvidia-vgpu-device-manager-8mgg8                              1/1     Running   0          89s</span>
<span class="go">nvidia-vgpu-manager-daemonset-fpplc                           1/1     Running   0          2m41s</span>
</pre></div>
</div>
<p>This worker node has two A10 GPUs. Assuming the node has not been labeled explicitly with <code class="docutils literal notranslate"><span class="pre">nvidia.com/vgpu.config=&lt;config-name&gt;</span></code>, the <code class="docutils literal notranslate"><span class="pre">default</span></code> configuration will be used. And since the <code class="docutils literal notranslate"><span class="pre">default</span></code> configuration in the vgpu-devices-config only lists the <strong>A10-24C</strong> vGPU type for the A10 GPU, the vgpu-device-manager should only create vGPU devices on this type.</p>
<p><strong>A10-24C</strong> is the largest vGPU type supported on the A10 GPU, and only one vGPU device can be created per physical GPU. We should see two vGPU devices created:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ls<span class="w"> </span>-l<span class="w"> </span>/sys/bus/mdev/devices
<span class="go">total 0</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:18 9adc60ea-98a7-41b6-b17b-9b3e0d210c7a -&gt; ../../../devices/pci0000:85/0000:85:02.0/0000:86:00.4/9adc60ea-98a7-41b6-b17b-9b3e0d210c7a</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:18 f9033b86-ccee-454b-8b20-dd7912d95bfd -&gt; ../../../devices/pci0000:3a/0000:3a:00.0/0000:3b:00.4/f9033b86-ccee-454b-8b20-dd7912d95bfd</span>
</pre></div>
</div>
<p>The sandbox-device-plugin will discover and advertise these resources to kubelet. In this example, we have two A10 GPUs and therefore two <strong>A10-24C</strong> vGPU devices.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>describe<span class="w"> </span>node
<span class="go">...</span>
<span class="go">Capacity:</span>
<span class="go">  ...</span>
<span class="go">  nvidia.com/NVIDIA_A10-24C:      2</span>
<span class="go">  ...</span>
<span class="go">Allocatable:</span>
<span class="go">  ...</span>
<span class="go">  nvidia.com/NVIDIA_A10-24C:      2</span>
<span class="go">...</span>
</pre></div>
</div>
</section>
<section id="id5">
<h4>Update the KubeVirt CR<a class="headerlink" href="#id5" title="Permalink to this heading"></a></h4>
<p>Next, we will update the KubeVirt Custom Resource, as documented in the <a class="reference external" href="https://kubevirt.io/user-guide/virtual_machines/host-devices/#listing-permitted-devices">KubeVirt user guide</a>, so that these vGPU devices are permitted and can be requested by a KubeVirt VM. Note, replace the values for <code class="docutils literal notranslate"><span class="pre">mdevNameSelector</span></code> and <code class="docutils literal notranslate"><span class="pre">resourceName</span></code> to correspond to your vGPU type. We set <code class="docutils literal notranslate"><span class="pre">externalResourceProvider=true</span></code> to indicate that this resource is being provided by an external device plugin, in this case the sandbox-device-plugin which is deployed by the Operator.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>edit<span class="w"> </span>kubevirt<span class="w"> </span>-n<span class="w"> </span>kubevirt
<span class="go">...</span>
<span class="go">spec:</span>
<span class="go">  certificateRotateStrategy: {}</span>
<span class="go">  configuration:</span>
<span class="go">    developerConfiguration:</span>
<span class="go">      featureGates:</span>
<span class="go">      - GPU</span>
<span class="go">    permittedHostDevices:</span>
<span class="go">      mediatedDevices:</span>
<span class="go">      - externalResourceProvider: true</span>
<span class="go">        mdevNameSelector: NVIDIA A10-24C</span>
<span class="go">        resourceName: nvidia.com/NVIDIA_A10-24C</span>
<span class="go">...</span>
</pre></div>
</div>
<p>We are now ready to create a VM. Let’s create a sample VM using a simple VMI spec which requests a <code class="docutils literal notranslate"><span class="pre">nvidia.com/NVIDIA_A10-24C</span></code> resource:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat<span class="w"> </span>vmi-vgpu.yaml
<span class="go">---</span>
<span class="go">apiVersion: kubevirt.io/v1alpha3</span>
<span class="go">kind: VirtualMachineInstance</span>
<span class="go">metadata:</span>
<span class="go">  labels:</span>
<span class="go">    special: vmi-vgpu</span>
<span class="go">  name: vmi-vgpu</span>
<span class="go">spec:</span>
<span class="go">  domain:</span>
<span class="go">    devices:</span>
<span class="go">      disks:</span>
<span class="go">      - disk:</span>
<span class="go">          bus: virtio</span>
<span class="go">        name: containerdisk</span>
<span class="go">      - disk:</span>
<span class="go">          bus: virtio</span>
<span class="go">        name: cloudinitdisk</span>
<span class="go">      gpus:</span>
<span class="go">      - deviceName: nvidia.com/NVIDIA_A10-24C</span>
<span class="go">        name: vgpu1</span>
<span class="go">      rng: {}</span>
<span class="go">    machine:</span>
<span class="go">      type: &quot;&quot;</span>
<span class="go">    resources:</span>
<span class="go">      requests:</span>
<span class="go">        memory: 1024M</span>
<span class="go">  terminationGracePeriodSeconds: 0</span>
<span class="go">  volumes:</span>
<span class="go">  - containerDisk:</span>
<span class="go">      image: docker.io/kubevirt/fedora-cloud-container-disk-demo:devel</span>
<span class="go">    name: containerdisk</span>
<span class="go">  - cloudInitNoCloud:</span>
<span class="go">      userData: |-</span>
<span class="gp">        #</span>cloud-config
<span class="go">        password: fedora</span>
<span class="go">        chpasswd: { expire: False }</span>
<span class="go">    name: cloudinitdisk</span>
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>vmi-vgpu.yaml
<span class="go">virtualmachineinstance.kubevirt.io/vmi-vgpu created</span>

<span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>vmis
<span class="go">NAME       AGE   PHASE     IP               NODENAME       READY</span>
<span class="go">vmi-vgpu   10s   Running   192.168.47.205   cnt-server-2   True</span>
</pre></div>
</div>
<p>Let’s console into the VM and verify we have a GPU. Refer <a class="reference external" href="https://docs.google.com/document/d/1mH08JNe8nj5SRKzg8llttzMJbJbDbDaLji07BG6P1c4/edit#heading=h.hwxorb7idly9">here</a> for installing virtctl.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>./virtctl<span class="w"> </span>console<span class="w"> </span>vmi-vgpu
<span class="go">Successfully connected to vmi-vgpu console. The escape sequence is ^]</span>

<span class="go">vmi-vgpu login: fedora</span>
<span class="go">Password:</span>
<span class="gp">[fedora@vmi-vgpu ~]$ </span>sudo<span class="w"> </span>yum<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>-q<span class="w"> </span>pciutils
<span class="gp">[fedora@vmi-vgpu ~]$ </span>lspci<span class="w"> </span>-nnk<span class="w"> </span>-d<span class="w"> </span>10de:
<span class="go">06:00.0 3D controller [0302]: NVIDIA Corporation GA102GL [A10] [10de:2236] (rev a1)</span>
<span class="go">       Subsystem: NVIDIA Corporation Device [10de:14d4]</span>
</pre></div>
</div>
</section>
<section id="apply-a-new-vgpu-device-configuration">
<span id="apply-new-vgpu-device-config-1-11-1"></span><h4>Apply a New vGPU Device Configuration<a class="headerlink" href="#apply-a-new-vgpu-device-configuration" title="Permalink to this heading"></a></h4>
<p>We can apply a specific vGPU device configuration on a per-node basis by setting the <code class="docutils literal notranslate"><span class="pre">nvidia.com/vgpu.config</span></code> node label. It is recommended to set this node label prior to installing the GPU Operator if you do not want the default configuration applied.</p>
<p>Switching vGPU device configuration assumes that no VMs with vGPU are currently running on the node. Any existing VMs will have to be shutdown/migrated first.</p>
<p>To apply a new configuration after GPU Operator install, simply update the node label:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>label<span class="w"> </span>node<span class="w"> </span>&lt;node-name&gt;<span class="w"> </span>--overwrite<span class="w"> </span>nvidia.com/vgpu.config<span class="o">=</span>A10-4C
</pre></div>
</div>
<p>After the vGPU Device Manager finishes applying the new configuration, all pods should return to the Running state.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>gpu-operator
<span class="go">NAME                                                          READY   STATUS    RESTARTS   AGE</span>
<span class="go">...</span>
<span class="go">nvidia-sandbox-device-plugin-daemonset-brtb6                  1/1     Running   0          10s</span>
<span class="go">nvidia-sandbox-validator-ljnwg                                1/1     Running   0          10s</span>
<span class="go">nvidia-vgpu-device-manager-8mgg8                              1/1     Running   0          30m</span>
<span class="go">nvidia-vgpu-manager-daemonset-fpplc                           1/1     Running   0          31m</span>
</pre></div>
</div>
<p>We now see 12 vGPU devices on the node, as 6 <strong>A10-4C</strong> devices can be created per A10 GPU.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ls<span class="w"> </span>-ltr<span class="w"> </span>/sys/bus/mdev/devices
<span class="go">total 0</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 87401d9a-545b-4506-b1be-d4d30f6f4a4b -&gt; ../../../devices/pci0000:3a/0000:3a:00.0/0000:3b:00.5/87401d9a-545b-4506-b1be-d4d30f6f4a4b</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 78597b11-282f-496c-a4d0-19220310039c -&gt; ../../../devices/pci0000:3a/0000:3a:00.0/0000:3b:00.4/78597b11-282f-496c-a4d0-19220310039c</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 0d093db4-2c57-40ce-a1f0-ef4d410c6db8 -&gt; ../../../devices/pci0000:3a/0000:3a:00.0/0000:3b:00.6/0d093db4-2c57-40ce-a1f0-ef4d410c6db8</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 f830dbb1-0eb5-4294-af32-c68028e2ae35 -&gt; ../../../devices/pci0000:3a/0000:3a:00.0/0000:3b:00.7/f830dbb1-0eb5-4294-af32-c68028e2ae35</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 a5a11713-e683-4372-bebf-82219c58ce24 -&gt; ../../../devices/pci0000:3a/0000:3a:00.0/0000:3b:01.1/a5a11713-e683-4372-bebf-82219c58ce24</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 1a48c902-07f1-4a19-b3b0-b89ce35ad025 -&gt; ../../../devices/pci0000:3a/0000:3a:00.0/0000:3b:01.0/1a48c902-07f1-4a19-b3b0-b89ce35ad025</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 b8de2bbe-a41a-440e-9276-f7b56dc35138 -&gt; ../../../devices/pci0000:85/0000:85:02.0/0000:86:01.1/b8de2bbe-a41a-440e-9276-f7b56dc35138</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 afd7a4bb-d638-4489-bb41-6e03fc5c75b5 -&gt; ../../../devices/pci0000:85/0000:85:02.0/0000:86:01.0/afd7a4bb-d638-4489-bb41-6e03fc5c75b5</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 98175f96-707b-4167-ada5-869110ead3ab -&gt; ../../../devices/pci0000:85/0000:85:02.0/0000:86:00.5/98175f96-707b-4167-ada5-869110ead3ab</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 6e93ea61-9068-4096-b20c-ea30a72c1238 -&gt; ../../../devices/pci0000:85/0000:85:02.0/0000:86:00.7/6e93ea61-9068-4096-b20c-ea30a72c1238</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 537ce645-32cc-46d0-b7f0-f90ead840957 -&gt; ../../../devices/pci0000:85/0000:85:02.0/0000:86:00.6/537ce645-32cc-46d0-b7f0-f90ead840957</span>
<span class="go">lrwxrwxrwx 1 root root 0 Jun  7 00:47 4eb167bc-0e15-43f3-a218-d74cc9d162ff -&gt; ../../../devices/pci0000:85/0000:85:02.0/0000:86:00.4/4eb167bc-0e15-43f3-a218-d74cc9d162ff</span>
</pre></div>
</div>
<p>Check the new vGPU resources are advertised to kubelet:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>describe<span class="w"> </span>node
<span class="go">...</span>
<span class="go">Capacity:</span>
<span class="go">  ...</span>
<span class="go">  nvidia.com/NVIDIA_A10-4C:       12</span>
<span class="go">  ...</span>
<span class="go">Allocatable:</span>
<span class="go">  ...</span>
<span class="go">  nvidia.com/NVIDIA_A10-4C:       12</span>
<span class="go">...</span>
</pre></div>
</div>
<p>Following previous instructions, we can now create a VM with an <strong>A10-4C</strong> vGPU attached.</p>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="gpu-operator-rdma.html" class="btn btn-neutral float-left" title="GPUDirect RDMA" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="appendix.html" class="btn btn-neutral float-right" title="Appendix" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018-2023, NVIDIA Corporation.
      <span class="lastupdated">Last updated on 2023-02-22.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>  

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  .rst-content dl:not(.docutils) dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }
  </style>
  

</body>
</html>