<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Time-Slicing GPUs in Kubernetes &mdash; gpu-operator 23.0.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" type="text/css" />
      <link rel="stylesheet" href="_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="_static/api-styles.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/mermaid-init.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/design-tabs.js"></script>
        <script src="_static/version.js"></script>
        <script src="_static/social-media.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="GPUDirect RDMA and GPUDirect Storage" href="gpu-operator-rdma.html" />
    <link rel="prev" title="GPU Operator with MIG" href="gpu-operator-mig.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
          </a>

<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">NVIDIA GPU Operator</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="about.html">About the Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="platform-support.html">Platform Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-driver-upgrades.html">GPU Driver Upgrades</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-vgpu.html">NVIDIA vGPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-nvaie.html">NVIDIA AI Enterprise</a></li>
<li class="toctree-l1"><a class="reference internal" href="openshift/contents.html">GPU Operator on OpenShift</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-mig.html">GPU Operator with MIG</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Time-Slicing GPUs in Kubernetes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#configuration">Configuration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#configuration-for-shared-access-to-gpus-with-gpu-time-slicing">Configuration for Shared Access to GPUs with GPU Time-Slicing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#enabling-shared-access-to-gpus-with-the-nvidia-gpu-operator">Enabling Shared Access to GPUs with the NVIDIA GPU Operator</a></li>
<li class="toctree-l3"><a class="reference internal" href="#applying-the-default-configuration-across-the-cluster">Applying the Default Configuration Across the Cluster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#applying-a-time-slicing-configuration-per-node">Applying a Time-Slicing Configuration Per Node</a></li>
<li class="toctree-l3"><a class="reference internal" href="#changes-to-node-labels-by-the-gpu-feature-discovery-plugin">Changes to Node Labels by the GPU Feature Discovery Plugin</a></li>
<li class="toctree-l3"><a class="reference internal" href="#supported-resource-types">Supported Resource Types</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#testing-gpu-time-slicing-with-the-nvidia-gpu-operator">Testing GPU Time-Slicing with the NVIDIA GPU Operator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-rdma.html">GPUDirect RDMA and GPUDirect Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-kubevirt.html">GPU Operator with KubeVirt</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix.html">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">Licenses and Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="archive.html">Archive</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Related Projects</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html">NVIDIA Container Toolkit</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/kubernetes/install-k8s.html">Kubernetes with GPUs</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-telemetry/dcgm-exporter.html">GPU Telemetry</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/mig/mig.html">Multi-Instance GPU</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/driver-containers/overview.html">Driver Containers</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">gpu-operator</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
<li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
<li>Time-Slicing GPUs in Kubernetes</li>
<li class="wy-breadcrumbs-aside">

  <span>&nbsp;</span>
</li>

  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="time-slicing-gpus-in-kubernetes">
<span id="gpu-sharing"></span><h1>Time-Slicing GPUs in Kubernetes<a class="headerlink" href="#time-slicing-gpus-in-kubernetes" title="Permalink to this headline"></a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline"></a></h2>
<p>The latest generations of NVIDIA GPUs provide an operation mode called
Multi-Instance GPU, or MIG. MIG allows you to partition a GPU
into several smaller, predefined instances, each of which looks like a
mini-GPU that provides memory and fault isolation at the hardware layer.
You can share access to a GPU by running workloads on one of
these predefined instances instead of the full native GPU.</p>
<p>MIG support was added to Kubernetes in 2020. Refer to <a class="reference external" href="https://www.google.com/url?q=https://docs.google.com/document/d/1mdgMQ8g7WmaI_XVVRrCvHPFPOMCm5LQD5JefgAh6N8g/edit&amp;sa=D&amp;source=editors&amp;ust=1655578433019961&amp;usg=AOvVaw1F-OezvM-Svwr1lLsdQmu3">Supporting MIG in Kubernetes</a>
for details on how this works.</p>
<p>What if you don’t need the memory and fault-isolation provided by
MIG? What if you’re willing to trade the isolation provided by MIG for
the ability to share a GPU by a larger number of users. Or what if you don’t
have access to a GPU that supports MIG? Should they not be able
to provide shared access to their GPUs so long as memory and
fault-isolation are not a concern?</p>
<p>The NVIDIA GPU Operator allows oversubscription of GPUs through a set
of extended options for the <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/k8s-device-plugin">NVIDIA Kubernetes Device Plugin</a>.
Internally, GPU time-slicing is used to allow workloads that land
on oversubscribed GPUs to interleave with one another. This page covers
ways to enable this in Kubernetes using the GPU Operator.</p>
<p>This mechanism for enabling “time-sharing” of
GPUs in Kubernetes allows a system administrator to define a set of
“replicas” for a GPU, each of which can be handed out independently to a
pod to run workloads on. Unlike MIG, there is no memory or
fault-isolation between replicas, but for some workloads this is better
than not being able to share at all. Internally, GPU
time-slicing is used to multiplex workloads from
replicas of the same underlying GPU.</p>
<p>GPU time-slicing can be used with bare-metal applications, virtual machines
with GPU passthrough, and virtual machines with NVIDIA vGPU.
The following sections describe how to make use of the GPU
time-slicing feature in Kubernetes.</p>
</section>
<section id="configuration">
<h2>Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline"></a></h2>
<section id="configuration-for-shared-access-to-gpus-with-gpu-time-slicing">
<h3>Configuration for Shared Access to GPUs with GPU Time-Slicing<a class="headerlink" href="#configuration-for-shared-access-to-gpus-with-gpu-time-slicing" title="Permalink to this headline"></a></h3>
<p>You can provide time-slicing configurations for the NVIDIA Kubernetes Device Plugin as a <code class="docutils literal notranslate"><span class="pre">ConfigMap</span></code>:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span><span class="w"></span>
<span class="nt">sharing</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">timeSlicing</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">renameByDefault</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;bool&gt;</span><span class="w"></span>
<span class="w">    </span><span class="nt">failRequestsGreaterThanOne</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;bool&gt;</span><span class="w"></span>
<span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;resource-name&gt;</span><span class="w"></span>
<span class="w">      </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;num-replicas&gt;</span><span class="w"></span>
<span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">...</span><span class="w"></span>
</pre></div>
</div>
<p>For each named resource under <code class="docutils literal notranslate"><span class="pre">sharing.timeSlicing.resources</span></code>, a number of
replicas can be specified for that resource type. These replicas represent
the number of shared accesses that will be granted for a GPU represented by that resource type.
If <code class="docutils literal notranslate"><span class="pre">renameByDefault=true</span></code>, then each resource will be advertised under the
name <code class="docutils literal notranslate"><span class="pre">&lt;resource-name&gt;.shared</span></code> instead of simply <code class="docutils literal notranslate"><span class="pre">&lt;resource-name&gt;</span></code>.
If <code class="docutils literal notranslate"><span class="pre">failRequestsGreaterThanOne=true</span></code>, then the plugin will fail to allocate
any shared resources to a container if they request more than one. The
container’s pod will fail with an <code class="docutils literal notranslate"><span class="pre">UnexpectedAdmissionError</span></code> and must then be manually
deleted, updated, and redeployed.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unlike with “normal” GPU requests, requesting more than one shared GPU
does not guarantee that you will get
access to a proportional amount of compute power. It only specifies that
you will get access to a GPU that is shared
by other clients, each of which has the freedom to run as many processes
on the underlying GPU as they want.
Internally, the GPU will simply give an equal share of time to
all GPU processes across all of the clients.
The <code class="docutils literal notranslate"><span class="pre">failRequestsGreaterThanOne</span></code> flag is meant to help users
understand this subtlety, by treating a request of 1 as an
access request rather than an exclusive resource request. Setting
<code class="docutils literal notranslate"><span class="pre">failRequestsGreaterThanOne=true</span></code> is recommended,
but it is set to <code class="docutils literal notranslate"><span class="pre">false</span></code> by default to retain backwards compatibility.</p>
</div>
<p>You can specify multiple configurations in a <code class="docutils literal notranslate"><span class="pre">ConfigMap</span></code> as in the following
example.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">cat &lt;&lt; EOF &gt;&gt; time-slicing-config.yaml</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">apiVersion</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ConfigMap</span><span class="w"></span>
<span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">time-slicing-config</span><span class="w"></span>
<span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpu-operator</span><span class="w"></span>
<span class="nt">data</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">a100-40gb</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|-</span><span class="w"></span>
<span class="w">        </span><span class="no">version: v1</span><span class="w"></span>
<span class="w">        </span><span class="no">sharing:</span><span class="w"></span>
<span class="w">          </span><span class="no">timeSlicing:</span><span class="w"></span>
<span class="w">            </span><span class="no">resources:</span><span class="w"></span>
<span class="w">            </span><span class="no">- name: nvidia.com/gpu</span><span class="w"></span>
<span class="w">              </span><span class="no">replicas: 8</span><span class="w"></span>
<span class="w">            </span><span class="no">- name: nvidia.com/mig-1g.5gb</span><span class="w"></span>
<span class="w">              </span><span class="no">replicas: 2</span><span class="w"></span>
<span class="w">            </span><span class="no">- name: nvidia.com/mig-2g.10gb</span><span class="w"></span>
<span class="w">              </span><span class="no">replicas: 2</span><span class="w"></span>
<span class="w">            </span><span class="no">- name: nvidia.com/mig-3g.20gb</span><span class="w"></span>
<span class="w">              </span><span class="no">replicas: 3</span><span class="w"></span>
<span class="w">            </span><span class="no">- name: nvidia.com/mig-7g.40gb</span><span class="w"></span>
<span class="w">              </span><span class="no">replicas: 7</span><span class="w"></span>
<span class="w">    </span><span class="nt">tesla-t4</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|-</span><span class="w"></span>
<span class="w">        </span><span class="no">version: v1</span><span class="w"></span>
<span class="w">        </span><span class="no">sharing:</span><span class="w"></span>
<span class="w">          </span><span class="no">timeSlicing:</span><span class="w"></span>
<span class="w">            </span><span class="no">resources:</span><span class="w"></span>
<span class="w">            </span><span class="no">- name: nvidia.com/gpu</span><span class="w"></span>
<span class="w">              </span><span class="no">replicas: 4</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">EOF</span><span class="w"></span>
</pre></div>
</div>
<p>Create a <code class="docutils literal notranslate"><span class="pre">ConfigMap</span></code> in the operator namespace. In this example, it is <code class="docutils literal notranslate"><span class="pre">gpu-operator</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl create namespace gpu-operator
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl create -f time-slicing-config.yaml
</pre></div>
</div>
</section>
<section id="enabling-shared-access-to-gpus-with-the-nvidia-gpu-operator">
<h3>Enabling Shared Access to GPUs with the NVIDIA GPU Operator<a class="headerlink" href="#enabling-shared-access-to-gpus-with-the-nvidia-gpu-operator" title="Permalink to this headline"></a></h3>
<p>You can enable time-slicing with the NVIDIA GPU Operator by passing the
<code class="docutils literal notranslate"><span class="pre">devicePlugin.config.name=&lt;config-map-name&gt;</span></code> parameter,
where <code class="docutils literal notranslate"><span class="pre">&lt;config-map-name&gt;</span></code>
is the name of the <code class="docutils literal notranslate"><span class="pre">ConfigMap</span></code> created for the time-slicing
configuration as described in the previous section.</p>
<p>During fresh install of the NVIDIA GPU Operator with time-slicing enabled (e.g. <code class="docutils literal notranslate"><span class="pre">time-slicing-config</span></code>):</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm install gpu-operator nvidia/gpu-operator <span class="se">\</span>
     -n gpu-operator <span class="se">\</span>
     --set devicePlugin.config.name<span class="o">=</span>time-slicing-config
</pre></div>
</div>
<p>For dynamically enabling time-slicing with GPU Operator already installed:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl patch clusterpolicy/cluster-policy <span class="se">\</span>
   -n gpu-operator --type merge <span class="se">\</span>
   -p <span class="s1">&#39;{&quot;spec&quot;: {&quot;devicePlugin&quot;: {&quot;config&quot;: {&quot;name&quot;: &quot;time-slicing-config&quot;}}}}&#39;</span>
</pre></div>
</div>
</section>
<section id="applying-the-default-configuration-across-the-cluster">
<h3>Applying the Default Configuration Across the Cluster<a class="headerlink" href="#applying-the-default-configuration-across-the-cluster" title="Permalink to this headline"></a></h3>
<p>The time-slicing configuration can be applied either at cluster level
or per node. By default, the GPU Operator will <strong>not</strong> apply the time-slicing
configuration to any GPU node in the cluster. The user would have to
explicitly specify it with the <code class="docutils literal notranslate"><span class="pre">devicePlugin.config.default=&lt;config-name&gt;</span></code> parameter.</p>
<p>Install the GPU Operator by passing the time-slicing <code class="docutils literal notranslate"><span class="pre">ConfigMap</span></code> name and the
<strong>default</strong> configuration (e.g. a100-40gb):</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl patch clusterpolicy/cluster-policy <span class="se">\</span>
   -n gpu-operator --type merge <span class="se">\</span>
   -p <span class="s1">&#39;{&quot;spec&quot;: {&quot;devicePlugin&quot;: {&quot;config&quot;: {&quot;name&quot;: &quot;time-slicing-config&quot;, &quot;default&quot;: &quot;a100-40gb&quot;}}}}&#39;</span>
</pre></div>
</div>
<p>Verify that the time-slicing configuration is applied successfully to all
GPU nodes in the cluster:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl describe node &lt;node-name&gt;
<span class="go">...</span>
<span class="go">Capacity:</span>
<span class="go">nvidia.com/gpu: 8</span>
<span class="go">...</span>
<span class="go">Allocatable:</span>
<span class="go">nvidia.com/gpu: 8</span>
<span class="go">...</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In this example it is assumed that node <code class="docutils literal notranslate"><span class="pre">&lt;node-name&gt;</span></code> has one GPU.</p>
</div>
</section>
<section id="applying-a-time-slicing-configuration-per-node">
<h3>Applying a Time-Slicing Configuration Per Node<a class="headerlink" href="#applying-a-time-slicing-configuration-per-node" title="Permalink to this headline"></a></h3>
<p>To enable a time-slicing configuration per node, the user would need to
apply the <code class="docutils literal notranslate"><span class="pre">nvidia.com/device-plugin.config=&lt;config-name&gt;</span></code> node label after
installing the GPU Operator. On applying this label, the
NVIDIA Kubernetes Device Plugin will configure node GPU resources accordingly.</p>
<p>Install the GPU Operator by passing a time-slicing <code class="docutils literal notranslate"><span class="pre">ConfigMap</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm install gpu-operator nvidia/gpu-operator <span class="se">\</span>
     -n gpu-operator <span class="se">\</span>
     --set devicePlugin.config.name<span class="o">=</span>time-slicing-config
</pre></div>
</div>
<p>Label the node with the required time-slicing configuration (e.g. <code class="docutils literal notranslate"><span class="pre">a100-40gb</span></code>) in the <code class="docutils literal notranslate"><span class="pre">ConfigMap</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl label node &lt;node-name&gt; nvidia.com/device-plugin.config<span class="o">=</span>a100-40gb
</pre></div>
</div>
<p>Verify that the time-slicing configuration is applied successfully:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl describe node &lt;node-name&gt;
<span class="go">...</span>
<span class="go">Capacity:</span>
<span class="go">nvidia.com/gpu: 8</span>
<span class="go">...</span>
<span class="go">Allocatable:</span>
<span class="go">nvidia.com/gpu: 8</span>
<span class="go">...</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In this example it is assumed that node <code class="docutils literal notranslate"><span class="pre">&lt;node-name&gt;</span></code> has one GPU.</p>
</div>
</section>
<section id="changes-to-node-labels-by-the-gpu-feature-discovery-plugin">
<h3>Changes to Node Labels by the GPU Feature Discovery Plugin<a class="headerlink" href="#changes-to-node-labels-by-the-gpu-feature-discovery-plugin" title="Permalink to this headline"></a></h3>
<p>In addition to the standard node labels applied by the GPU Feature
Discovery Plugin (GFD), the following label
is also included when deploying
the plugin with the time-slicing configurations described above.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>nvidia.com/&lt;resource-name&gt;.replicas = &lt;num-replicas&gt;
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">&lt;num-replicas&gt;</span></code> is the factor by which each resource of <code class="docutils literal notranslate"><span class="pre">&lt;resource-name&gt;</span></code> is oversubscribed.</p>
<p>Additionally, <code class="docutils literal notranslate"><span class="pre">nvidia.com/&lt;resource-name&gt;.product</span></code> is modified as follows if <code class="docutils literal notranslate"><span class="pre">renameByDefault=false</span></code>:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>nvidia.com/&lt;resource-name&gt;.product = &lt;product name&gt;-SHARED
</pre></div>
</div>
<p>Using these labels, you can select a shared vs. non-shared GPU
in the same way as traditionally
selecting one GPU model over another. That is, the <code class="docutils literal notranslate"><span class="pre">SHARED</span></code> annotation ensures that
the <code class="docutils literal notranslate"><span class="pre">nodeSelector</span></code> can be used to attract
pods to nodes with shared GPUs.</p>
<p>Because having <code class="docutils literal notranslate"><span class="pre">renameByDefault=true</span></code> already encodes the fact that the
resource is shared on the resource name,
there is no need to annotate the product name with <code class="docutils literal notranslate"><span class="pre">SHARED</span></code>. You can already
find needed shared resources by simply requesting it in the pod specification.</p>
<p>When running with <code class="docutils literal notranslate"><span class="pre">renameByDefault=false</span></code> and <code class="docutils literal notranslate"><span class="pre">migStrategy=single</span></code>,
both the MIG profile name and the new <code class="docutils literal notranslate"><span class="pre">SHARED</span></code> annotation
are appended to the product name, like this:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>nvidia.com/gpu.product = A100-SXM4-40GB-MIG-1g.5gb-SHARED
</pre></div>
</div>
</section>
<section id="supported-resource-types">
<h3>Supported Resource Types<a class="headerlink" href="#supported-resource-types" title="Permalink to this headline"></a></h3>
<p>Currently, the only supported resource types are <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu</span></code>
and any of the resource types that emerge from configuring a node with
the mixed MIG strategy.</p>
<p>For example, the full set of time-sliceable resources on a T4 card would
be:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">nvidia.com/gpu</span>
</pre></div>
</div>
<p>And the full set of time-sliceable resources on an A100 40GB card would be:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">nvidia.com/gpu</span>
<span class="go">nvidia.com/mig-1g.5gb</span>
<span class="go">nvidia.com/mig-2g.10gb</span>
<span class="go">nvidia.com/mig-3g.20gb</span>
<span class="go">nvidia.com/mig-7g.40gb</span>
</pre></div>
</div>
<p>Likewise, on an A100 80GB card, they would be:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">nvidia.com/gpu</span>
<span class="go">nvidia.com/mig-1g.10gb</span>
<span class="go">nvidia.com/mig-2g.20gb</span>
<span class="go">nvidia.com/mig-3g.40gb</span>
<span class="go">nvidia.com/mig-7g.80gb</span>
</pre></div>
</div>
</section>
</section>
<section id="testing-gpu-time-slicing-with-the-nvidia-gpu-operator">
<h2>Testing GPU Time-Slicing with the NVIDIA GPU Operator<a class="headerlink" href="#testing-gpu-time-slicing-with-the-nvidia-gpu-operator" title="Permalink to this headline"></a></h2>
<p>This section covers a workload test scenario to validate GPU time-slicing with GPU resources.</p>
<ol class="arabic simple">
<li><p>Create a workload test file <code class="docutils literal notranslate"><span class="pre">plugin-test.yaml</span></code> as follows:</p></li>
</ol>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">apps/v1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Deployment</span><span class="w"></span>
<span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia-plugin-test</span><span class="w"></span>
<span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia-plugin-test</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span><span class="w"></span>
<span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia-plugin-test</span><span class="w"></span>
<span class="w">  </span><span class="nt">template</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">labels</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia-plugin-test</span><span class="w"></span>
<span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">tolerations</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia.com/gpu</span><span class="w"></span>
<span class="w">          </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Exists</span><span class="w"></span>
<span class="w">          </span><span class="nt">effect</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NoSchedule</span><span class="w"></span>
<span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">dcgmproftester11</span><span class="w"></span>
<span class="w">          </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia/samples:dcgmproftester-2.0.10-cuda11.0-ubuntu18.04</span><span class="w"></span>
<span class="w">          </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;/bin/sh&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;-c&quot;</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="w">          </span><span class="nt">args</span><span class="p">:</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">while true; do /usr/bin/dcgmproftester11 --no-dcgm-validation -t 1004 -d 300; sleep 30; done</span><span class="w"></span>
<span class="w">          </span><span class="nt">resources</span><span class="p">:</span><span class="w"></span>
<span class="w">           </span><span class="nt">limits</span><span class="p">:</span><span class="w"></span>
<span class="w">             </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">          </span><span class="nt">securityContext</span><span class="p">:</span><span class="w"></span>
<span class="w">            </span><span class="nt">capabilities</span><span class="p">:</span><span class="w"></span>
<span class="w">              </span><span class="nt">add</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;SYS_ADMIN&quot;</span><span class="p p-Indicator">]</span><span class="w"></span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Create a deployment with multiple replicas:</p></li>
</ol>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kubectl apply -f plugin-test.yaml</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Verify that all five replicas are running:</p></li>
</ol>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kubectl get pods</span>
<span class="go">kubectl exec &lt;driver-pod-name&gt; -n gpu-operator -- nvidia-smi</span>
</pre></div>
</div>
<p>Your output should look something like this:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">NAME                                  READY   STATUS    RESTARTS   AGE</span>
<span class="go">nvidia-plugin-test-8479c8f7c8-4tnsn   1/1     Running   0          6s</span>
<span class="go">nvidia-plugin-test-8479c8f7c8-cdgdb   1/1     Running   0          6s</span>
<span class="go">nvidia-plugin-test-8479c8f7c8-q2vn7   1/1     Running   0          6s</span>
<span class="go">nvidia-plugin-test-8479c8f7c8-t9d4b   1/1     Running   0          6s</span>
<span class="go">nvidia-plugin-test-8479c8f7c8-xggls   1/1     Running   0          6s</span>
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl <span class="nb">exec</span> &lt;driver-pod-name&gt; -n gpu-operator -- nvidia-smi
</pre></div>
</div>
<p>Your output should look something like this:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">+-----------------------------------------------------------------------------+</span>
<span class="go">| NVIDIA-SMI 510.73.08    Driver Version: 510.73.08    CUDA Version: 11.6     |</span>
<span class="go">|-------------------------------+----------------------+----------------------+</span>
<span class="go">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span>
<span class="go">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span>
<span class="go">|                               |                      |               MIG M. |</span>
<span class="go">|===============================+======================+======================|</span>
<span class="go">|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |</span>
<span class="go">| N/A   44C    P0    70W /  70W |   1577MiB / 15360MiB |    100%      Default |</span>
<span class="go">|                               |                      |                  N/A |</span>
<span class="go">+-------------------------------+----------------------+----------------------+</span>

<span class="go">+-----------------------------------------------------------------------------+</span>
<span class="go">| Processes:                                                                  |</span>
<span class="go">|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |</span>
<span class="go">|        ID   ID                                                   Usage      |</span>
<span class="go">|=============================================================================|</span>
<span class="go">|    0   N/A  N/A      3666      C   /usr/bin/dcgmproftester11         315MiB |</span>
<span class="go">|    0   N/A  N/A      3679      C   /usr/bin/dcgmproftester11         315MiB |</span>
<span class="go">|    0   N/A  N/A      3992      C   /usr/bin/dcgmproftester11         315MiB |</span>
<span class="go">|    0   N/A  N/A      4119      C   /usr/bin/dcgmproftester11         315MiB |</span>
<span class="go">|    0   N/A  N/A      4324      C   /usr/bin/dcgmproftester11         315MiB |</span>
<span class="go">+-----------------------------------------------------------------------------+</span>
</pre></div>
</div>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline"></a></h2>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://developer.nvidia.com/blog/improving-gpu-utilization-in-kubernetes">Blog post on GPU sharing in Kubernetes</a>.</p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/k8s-device-plugin#shared-access-to-gpus-with-cuda-time-slicing">NVIDIA Kubernetes Device Plugin</a>.</p></li>
</ol>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2023, NVIDIA.
      <span class="lastupdated">Last updated on Mar 02, 2023.
      </span></p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>