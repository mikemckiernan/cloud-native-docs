<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>NVIDIA GPUs with Google Cloud’s Anthos &mdash; NVIDIA Cloud Native Technologies  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/nvidia.ico"/>
    <link rel="canonical" href="https://docs.nvidia.com/datacenter/cloud-native/kubernetes/anthos-guide.html"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/js/google-analytics/google-analytics-tracker.js"></script>
        <script src="../_static/js/google-analytics/google-analytics-write.js"></script>
        <script src="//assets.adobedtm.com/b92787824f2e0e9b68dc2e993f9bd995339fe417/satelliteLib-7ba51e58dc61bcb0e9311aadd02a0108ab24cc6c.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="DCGM-Exporter" href="../gpu-telemetry/dcgm-exporter.html" />
    <link rel="prev" title="MIG Support in Kubernetes" href="mig-k8s.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" > 
            <a href="../contents.html">
            <img src="../_static/NVLogo_H_B&W.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }

    .wy-nav-content {
      max-width: 1000px;
    }
  </style>
  
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">NVIDIA Container Toolkit:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../container-toolkit/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../container-toolkit/concepts.html">Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../container-toolkit/arch-overview.html">Architecture Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../container-toolkit/install-guide.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../container-toolkit/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../container-toolkit/user-guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../container-toolkit/release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../container-toolkit/archive.html">Archive</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">NVIDIA GPU Operator:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/getting-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/platform-support.html">Platform Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/gpu-driver-upgrades.html">GPU Driver Upgrades</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/install-gpu-operator-vgpu.html">NVIDIA vGPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/install-gpu-operator-nvaie.html">NVIDIA AI Enterprise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/openshift/contents.html">GPU Operator on OpenShift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/gpu-operator-mig.html">GPU Operator with MIG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/gpu-sharing.html">Time-Slicing GPUs in Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/gpu-operator-rdma.html">GPUDirect RDMA and GPUDirect Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/gpu-operator-kubevirt.html">GPU Operator with KubeVirt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/cdi.html">Container Device Interface Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/appendix.html">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/archive.html">Archive</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Kubernetes with GPUs:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="install-k8s.html">Install Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="mig-k8s.html">MIG Support in Kubernetes</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">NVIDIA GPUs with Google Cloud’s Anthos</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#changelog">Changelog</a></li>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#deployment-configurations">Deployment Configurations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#supported-platforms">Supported Platforms</a></li>
<li class="toctree-l2"><a class="reference internal" href="#getting-support">Getting Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="#anthos-clusters-on-bare-metal-with-nvidia-dgx-systems-and-gpu-accelerated-servers">Anthos Clusters on Bare Metal with NVIDIA DGX Systems and GPU-Accelerated Servers</a></li>
<li class="toctree-l2"><a class="reference internal" href="#anthos-clusters-with-vmware-and-nvidia-gpu-accelerated-servers">Anthos Clusters with VMware and NVIDIA GPU Accelerated Servers</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Telemetry:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gpu-telemetry/dcgm-exporter.html">DCGM-Exporter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpu-telemetry/dcgm-exporter.html#integrating-gpu-telemetry-into-kubernetes">Integrating GPU Telemetry into Kubernetes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multi-Instance GPU:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mig/mig.html">Multi-Instance GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mig/mig-k8s.html">MIG Support in Kubernetes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Driver Containers:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../driver-containers/overview.html">Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Playground:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../playground/dind.html">Docker-in-Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../playground/x-arch.html">Running Cross-Architecture Containers</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../contents.html">NVIDIA Cloud Native Technologies</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../contents.html" class="icon icon-home"></a> &raquo;</li>
      <li>NVIDIA GPUs with Google Cloud’s Anthos</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="nvidia-gpus-with-google-cloud-s-anthos">
<span id="anthos-guide"></span><h1>NVIDIA GPUs with Google Cloud’s Anthos<a class="headerlink" href="#nvidia-gpus-with-google-cloud-s-anthos" title="Permalink to this heading"></a></h1>
<section id="changelog">
<h2>Changelog<a class="headerlink" href="#changelog" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><dl class="simple">
<dt>3/22/2020 (author: PR):</dt><dd><ul>
<li><p>Fixed URLs</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>11/30/2020 (author: PR/DF):</dt><dd><ul>
<li><p>Added information on Anthos on bare metal</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>11/25/2020 (author: PR):</dt><dd><ul>
<li><p>Migrated docs to new format</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>8/14/2020 (author: PR):</dt><dd><ul>
<li><p>Initial Version</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading"></a></h2>
<p>Google Cloud’s Anthos is a modern application management platform that lets users
build, deploy, and manage applications anywhere in a secure, consistent manner.
The platform provides a consistent development and operations experience across
deployments while reducing operational overhead and improving developer productivity.
Anthos runs in hybrid and multi-cloud environments that spans <a class="reference external" href="https://cloud.google.com/kubernetes-engine">Google Cloud</a>,
<a class="reference external" href="https://cloud.google.com/anthos/docs/setup/on-premises">on-premise</a>, and is generally
available on <a class="reference external" href="https://cloud.google.com/anthos/docs/setup/public-cloud">Amazon Web Services (AWS)</a>.
Support for Anthos on Microsoft Azure is in preview. For more information on Anthos,
see the <a class="reference external" href="https://cloud.google.com/anthos">product overview</a>.</p>
<p>Systems with NVIDIA GPUs can be deployed in various configurations for use with Google Cloud’s Anthos.
The purpose of this document is to provide users with steps on getting started with using
NVIDIA GPUs with Anthos in these various configurations.</p>
</section>
<section id="deployment-configurations">
<h2>Deployment Configurations<a class="headerlink" href="#deployment-configurations" title="Permalink to this heading"></a></h2>
<p>Anthos can be deployed in different configurations. Depending on your deployment, choose one of the sections below
to get started with NVIDIA GPUs in Google Cloud’s Anthos:</p>
<ol class="arabic simple">
<li><p><a class="reference internal" href="#anthos-dgx-bm"><span class="std std-ref">Anthos Clusters on Bare Metal with NVIDIA DGX Systems and GPU-Accelerated Servers</span></a></p></li>
<li><p><a class="reference internal" href="#anthos-virt"><span class="std std-ref">Anthos Clusters with VMware and NVIDIA GPU-Accelerated Servers</span></a></p></li>
</ol>
</section>
<section id="supported-platforms">
<h2>Supported Platforms<a class="headerlink" href="#supported-platforms" title="Permalink to this heading"></a></h2>
<section id="gpus">
<h3>GPUs<a class="headerlink" href="#gpus" title="Permalink to this heading"></a></h3>
<p>The following GPUs are supported:</p>
<ul class="simple">
<li><p>NVIDIA A100, T4 and V100</p></li>
</ul>
</section>
<section id="dgx-systems">
<h3>DGX Systems<a class="headerlink" href="#dgx-systems" title="Permalink to this heading"></a></h3>
<p>The following NVIDIA DGX systems are supported:</p>
<ul class="simple">
<li><p>NVIDIA DGX A100</p></li>
<li><p>NVIDIA DGX-2 and DGX-1 (Volta)</p></li>
</ul>
</section>
<section id="linux-distributions">
<h3>Linux Distributions<a class="headerlink" href="#linux-distributions" title="Permalink to this heading"></a></h3>
<p>The following Linux distributions are supported:</p>
<ul class="simple">
<li><p>Ubuntu 18.04.z, 20.04.z LTS</p></li>
</ul>
<p>For more information on the Anthos Ready platforms, visit this <a class="reference external" href="https://cloud.google.com/anthos/docs/resources/partner-platforms#nvidia">page</a>.</p>
</section>
</section>
<section id="getting-support">
<h2>Getting Support<a class="headerlink" href="#getting-support" title="Permalink to this heading"></a></h2>
<p>For support issues related to using GPUs with Anthos, please <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/new">open a ticket</a>
on the NVIDIA GPU Operator GitHub project. Your feedback is appreciated.</p>
<p>DGX customers can visit the <a class="reference external" href="https://www.nvidia.com/en-us/data-center/dgx-systems/support/">NVIDIA DGX Systems Support Portal</a>.</p>
</section>
<section id="anthos-clusters-on-bare-metal-with-nvidia-dgx-systems-and-gpu-accelerated-servers">
<span id="anthos-dgx-bm"></span><h2>Anthos Clusters on Bare Metal with NVIDIA DGX Systems and GPU-Accelerated Servers<a class="headerlink" href="#anthos-clusters-on-bare-metal-with-nvidia-dgx-systems-and-gpu-accelerated-servers" title="Permalink to this heading"></a></h2>
<p>Anthos on bare metal with DGX A100 or NVIDIA GPU-accelerated servers systems enables a consistent development and operational experience across deployments,
while reducing expensive overhead and improving developer productivity. Refer to the Anthos <a class="reference external" href="https://cloud.google.com/anthos/gke/docs">documentation</a> for
more information on Anthos cluster environments.</p>
<section id="installation-flow">
<h3>Installation Flow<a class="headerlink" href="#installation-flow" title="Permalink to this heading"></a></h3>
<p>The basic steps described in this document follows this workflow:</p>
<ol class="arabic simple">
<li><p>Configure nodes</p>
<ul class="simple">
<li><p>Ensure each node (including the control plane) meets the pre-requisites, including time synchronization, correct versions of Docker and other conditions.</p></li>
</ul>
</li>
<li><p>Configure networking (Optional)</p>
<ul class="simple">
<li><p>Ensure network connectivity between control plane and nodes - ideally the VIPs, control plane and the nodes in the cluster are in the same network subnet.</p></li>
</ul>
</li>
<li><p>Configure an admin workstation and set up Anthos to create the cluster</p>
<ul class="simple">
<li><p>Set up the cluster using Anthos on bare-metal</p></li>
</ul>
</li>
<li><p>Setup NVIDIA software on GPU nodes</p>
<ul class="simple">
<li><p>Set up the NVIDIA software components on the GPU nodes to ensure that your cluster can run CUDA applications.</p></li>
</ul>
</li>
</ol>
<p>At the end of the installation flow, you should have a user cluster with GPU-enabled nodes that you can use to deploy applications.</p>
</section>
<section id="configure-nodes">
<h3>Configure Nodes<a class="headerlink" href="#configure-nodes" title="Permalink to this heading"></a></h3>
<p>These steps are required on each node in the cluster (including the control plane).</p>
<section id="time-synchronization">
<h4>Time Synchronization<a class="headerlink" href="#time-synchronization" title="Permalink to this heading"></a></h4>
<ul>
<li><p>Ensure <code class="docutils literal notranslate"><span class="pre">apparmor</span></code> is stopped:</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>apparmor-utils<span class="w"> </span>policycoreutils
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>systemctl<span class="w"> </span>--now<span class="w"> </span><span class="nb">enable</span><span class="w"> </span>apparmor<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>systemctl<span class="w"> </span>stop<span class="w"> </span>apparmor
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Synchronize the time on each node:</p>
<blockquote>
<div><ul>
<li><p>Check the current time</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>timedatectl
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">               Local time: Fri 2020-11-20 10:38:06 PST</span>
<span class="go">           Universal time: Fri 2020-11-20 18:38:06 UTC</span>
<span class="go">                 RTC time: Fri 2020-11-20 18:38:08</span>
<span class="go">                Time zone: US/Pacific (PST, -0800)</span>
<span class="go">System clock synchronized: no</span>
<span class="go">              NTP service: active</span>
<span class="go">          RTC in local TZ: no</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Configure the NTP server in <code class="docutils literal notranslate"><span class="pre">/etc/systemd/timesyncd.conf</span></code>:</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">NTP=time.google.com</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Adjust the system clock:</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>timedatectl<span class="w"> </span>set-local-rtc<span class="w"> </span><span class="m">0</span><span class="w"> </span>--adjust-system-clock
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Restart the service</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>systemctl<span class="w"> </span>restart<span class="w"> </span>systemd-timesyncd.service
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Verify the synchronization with the time server</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>timedatectl
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">               Local time: Fri 2020-11-20 11:03:22 PST</span>
<span class="go">           Universal time: Fri 2020-11-20 19:03:22 UTC</span>
<span class="go">                 RTC time: Fri 2020-11-20 19:03:22</span>
<span class="go">                Time zone: US/Pacific (PST, -0800)</span>
<span class="go">System clock synchronized: yes</span>
<span class="go">              NTP service: active</span>
<span class="go">          RTC in local TZ: no</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
</ul>
</section>
<section id="test-network-connectivity">
<h4>Test Network Connectivity<a class="headerlink" href="#test-network-connectivity" title="Permalink to this heading"></a></h4>
<ul>
<li><p>Ensure you can <code class="docutils literal notranslate"><span class="pre">nslookup</span></code> on <em>hostname</em></p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>systemctl<span class="w"> </span>restart<span class="w"> </span>systemd-resolved<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>ping<span class="w"> </span>us.archive.ubuntu.com
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">ping: us.archive.ubuntu.com: Temporary failure in name resolution</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Check the nameserver in <code class="docutils literal notranslate"><span class="pre">resolve.conf</span></code></p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat<span class="w"> </span>&lt;&lt;EOF<span class="w"> </span>&gt;<span class="w"> </span>/etc/resolv.conf
<span class="go">nameserver 8.8.8.8</span>
<span class="go">EOF</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>And re-test <code class="docutils literal notranslate"><span class="pre">ping</span></code></p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ping<span class="w"> </span>us.archive.ubuntu.com

<span class="go">PING us.archive.ubuntu.com (91.189.91.38) 56(84) bytes of data.</span>
<span class="go">64 bytes from banjo.canonical.com (91.189.91.38): icmp_seq=1 ttl=49 time=73.4 ms</span>
<span class="go">64 bytes from banjo.canonical.com (91.189.91.38): icmp_seq=2 ttl=49 time=73.3 ms</span>
<span class="go">64 bytes from banjo.canonical.com (91.189.91.38): icmp_seq=3 ttl=49 time=73.4 ms</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</section>
<section id="install-docker">
<h4>Install Docker<a class="headerlink" href="#install-docker" title="Permalink to this heading"></a></h4>
<p>Follow these steps to install Docker. On DGX systems, Docker may already be installed using the <code class="docutils literal notranslate"><span class="pre">docker-ce</span></code> package.
In this case, use <code class="docutils literal notranslate"><span class="pre">docker.io</span></code> as the base installation package for Docker to ensure a successful cluster setup with
Anthos.</p>
<ul>
<li><p>Stop services using docker:</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>systemctl<span class="w"> </span>stop<span class="w"> </span>kubelet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>systemctl<span class="w"> </span>stop<span class="w"> </span>docker<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>systemctl<span class="w"> </span>stop<span class="w"> </span>containerd<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>systemctl<span class="w"> </span>stop<span class="w"> </span>containerd.io
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Purge the existing packages of Docker and <code class="docutils literal notranslate"><span class="pre">nvidia-docker2</span></code> if any:</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>systemctl<span class="w"> </span>stop<span class="w"> </span>run-docker-netns-default.mount<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>systemctl<span class="w"> </span>stop<span class="w"> </span>docker.haproxy
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>dpkg<span class="w"> </span>-r<span class="w"> </span>nv-docker-options<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>dpkg<span class="w"> </span>--purge<span class="w"> </span>nv-docker-options<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>dpkg<span class="w"> </span>-r<span class="w"> </span>nvidia-docker2<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>dpkg<span class="w"> </span>--purge<span class="w"> </span>nvidia-docker2<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>dpkg<span class="w"> </span>-r<span class="w"> </span>docker-ce<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>dpkg<span class="w"> </span>--purge<span class="w"> </span>docker-ce<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>dpkg<span class="w"> </span>-r<span class="w"> </span>docker-ce-cli<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>dpkg<span class="w"> </span>-r<span class="w"> </span>containerd<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>dpkg<span class="w"> </span>--purge<span class="w"> </span>containerd<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>dpkg<span class="w"> </span>-r<span class="w"> </span>containerd.io<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>dpkg<span class="w"> </span>--purge
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Re-install Docker</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>apt-get<span class="w"> </span>update<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>apt-transport-https<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>ca-certificates<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>curl<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>software-properties-common<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>inetutils-traceroute<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>conntrack
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>curl<span class="w"> </span>-fsSL<span class="w"> </span>https://download.docker.com/linux/ubuntu/gpg<span class="w"> </span><span class="p">|</span><span class="w"> </span>apt-key<span class="w"> </span>add<span class="w"> </span>-
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>add-apt-repository<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="s2">&quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \</span>
<span class="gp">   $</span><span class="s2">(lsb_release -cs) stable&quot;</span>
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>apt-get<span class="w"> </span>update<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>docker.io
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>systemctl<span class="w"> </span>--now<span class="w"> </span><span class="nb">enable</span><span class="w"> </span>docker
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
<section id="install-nvidia-docker2-on-gpu-nodes">
<h5>Install <code class="docutils literal notranslate"><span class="pre">nvidia-docker2</span></code> on GPU Nodes<a class="headerlink" href="#install-nvidia-docker2-on-gpu-nodes" title="Permalink to this heading"></a></h5>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This step should be performed on the GPU nodes only</p>
</div>
<p>For DGX systems, re-install <code class="docutils literal notranslate"><span class="pre">nvidia-docker2</span></code> from the DGX repositories:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>nvidia-docker2
</pre></div>
</div>
<p>Since Kubernetes does not support the <code class="docutils literal notranslate"><span class="pre">--gpus</span></code> option with Docker yet, the <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> runtime should
be setup as the default container runtime for Docker on the GPU node. This can be done by adding the
<code class="docutils literal notranslate"><span class="pre">default-runtime</span></code> line into the Docker daemon config file, which is usually located on the system
at <code class="docutils literal notranslate"><span class="pre">/etc/docker/daemon.json</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go">   &quot;default-runtime&quot;: &quot;nvidia&quot;,</span>
<span class="go">   &quot;runtimes&quot;: {</span>
<span class="go">        &quot;nvidia&quot;: {</span>
<span class="go">            &quot;path&quot;: &quot;/usr/bin/nvidia-container-runtime&quot;,</span>
<span class="go">            &quot;runtimeArgs&quot;: []</span>
<span class="go">      }</span>
<span class="go">   }</span>
<span class="go">}</span>
</pre></div>
</div>
<p>Restart the Docker daemon to complete the installation after setting the default runtime:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>docker
</pre></div>
</div>
<p>For non-DGX systems, refer to the NVIDIA Container Toolkit <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker">installation guide</a>
to setup <code class="docutils literal notranslate"><span class="pre">nvidia-docker2</span></code>.</p>
</section>
</section>
</section>
<section id="configure-networking-optional">
<h3>Configure Networking (Optional)<a class="headerlink" href="#configure-networking-optional" title="Permalink to this heading"></a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The following steps are provided as a reference for configuring the network so that the control plane and the
nodes are on the same subnet by using tunnels and DNAT. If the nodes in your cluster are on the same subnet,
then you may skip this step.</p>
<p>In the example below:</p>
<ul class="simple">
<li><p>The control plane is at <code class="docutils literal notranslate"><span class="pre">10.117.29.41</span></code></p></li>
<li><p>The GPU node or admin workstation is at <code class="docutils literal notranslate"><span class="pre">10.110.20.149</span></code></p></li>
<li><p>The control plane VIP is <code class="docutils literal notranslate"><span class="pre">10.0.0.8</span></code></p></li>
</ul>
<p>If the machines are on a different subnet than each other or the control plane VIP then tunnel routes
can be used to establish connectivity.</p>
<p>There are two scenarios to consider:</p>
<ol class="arabic simple">
<li><p>If the machines are on the same subnet, but the VIP is on a different subnet, then add the correct
IP route (using <code class="docutils literal notranslate"><span class="pre">ip</span> <span class="pre">route</span> <span class="pre">add</span> <span class="pre">10.0.0.8</span> <span class="pre">via</span> <span class="pre">&lt;contro-plane-ip&gt;</span></code> from the GPU node or admin-workstation</p></li>
<li><p>If the machines and VIP are on different subnets, then a tunnel is also needed to enable the above
route command to succeed where <code class="docutils literal notranslate"><span class="pre">&lt;control-plane-ip&gt;</span></code> is the control plane tunnel <code class="docutils literal notranslate"><span class="pre">192.168.210.1</span></code>.</p></li>
</ol>
</div>
<section id="control-plane">
<h4>Control Plane<a class="headerlink" href="#control-plane" title="Permalink to this heading"></a></h4>
<p>Setup tunneling:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ip<span class="w"> </span>tunnel<span class="w"> </span>add<span class="w"> </span>tun0<span class="w"> </span>mode<span class="w"> </span>ipip<span class="w"> </span><span class="nb">local</span><span class="w"> </span><span class="m">10</span>.117.29.41<span class="w"> </span>remote<span class="w"> </span><span class="m">10</span>.110.20.149
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ip<span class="w"> </span>addr<span class="w"> </span>add<span class="w"> </span><span class="m">192</span>.168.200.1/24<span class="w"> </span>dev<span class="w"> </span>tun0
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ip<span class="w"> </span>link<span class="w"> </span><span class="nb">set</span><span class="w"> </span>tun0<span class="w"> </span>up
</pre></div>
</div>
<p>Update DNAT to support the control plane VIP over the tunnel:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>iptables<span class="w"> </span>-t<span class="w"> </span>nat<span class="w"> </span>-I<span class="w"> </span>PREROUTING<span class="w">  </span>-p<span class="w"> </span>udp<span class="w"> </span>-d<span class="w"> </span><span class="m">192</span>.168.210.1<span class="w">  </span>--dport<span class="w"> </span><span class="m">6081</span><span class="w"> </span>-j<span class="w"> </span>DNAT<span class="w"> </span>--to-destination<span class="w"> </span><span class="m">10</span>.117.29.41
</pre></div>
</div>
</section>
<section id="gpu-node-or-admin-workstation">
<h4>GPU Node or Admin Workstation<a class="headerlink" href="#gpu-node-or-admin-workstation" title="Permalink to this heading"></a></h4>
<p>Establish connectivity with the control plane:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ip<span class="w"> </span>tunnel<span class="w"> </span>add<span class="w"> </span>tun1<span class="w"> </span>mode<span class="w"> </span>ipip<span class="w"> </span><span class="nb">local</span><span class="w"> </span><span class="m">10</span>.110.20.149<span class="w">  </span>remote<span class="w"> </span><span class="m">10</span>.117.29.41
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ip<span class="w"> </span>addr<span class="w"> </span>add<span class="w"> </span><span class="m">192</span>.168.210.2/24<span class="w"> </span>dev<span class="w"> </span>tun1
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ip<span class="w"> </span>link<span class="w"> </span><span class="nb">set</span><span class="w"> </span>tun1<span class="w"> </span>up
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ip<span class="w"> </span>route<span class="w"> </span>add<span class="w"> </span><span class="m">10</span>.0.0.8/32<span class="w"> </span>via<span class="w"> </span><span class="m">192</span>.168.210.1
</pre></div>
</div>
<p>Setup DNAT:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>iptables<span class="w"> </span>-t<span class="w"> </span>nat<span class="w"> </span>-I<span class="w"> </span>OUTPUT<span class="w"> </span>-p<span class="w"> </span>udp<span class="w"> </span>-d<span class="w"> </span><span class="m">10</span>.117.29.41<span class="w">  </span>--dport<span class="w"> </span><span class="m">6081</span><span class="w"> </span>-j<span class="w"> </span>DNAT<span class="w"> </span>--to-destination<span class="w"> </span><span class="m">192</span>.168.210.1
</pre></div>
</div>
</section>
</section>
<section id="configure-admin-workstation">
<h3>Configure Admin Workstation<a class="headerlink" href="#configure-admin-workstation" title="Permalink to this heading"></a></h3>
<p>Configure the admin workstation prior to setting up the cluster.</p>
<p>Download the Google Cloud SDK:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>wget<span class="w"> </span>https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-314.0.0-linux-x86_64.tar.gz<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>tar<span class="w"> </span>-xf<span class="w"> </span>google-cloud-sdk-314.0.0-linux-x86_64.tar.gz
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>google-cloud-sdk/install.sh
</pre></div>
</div>
<p>Install the Anthos authentication components:</p>
<blockquote>
<div><p>$ gcloud components install anthos-auth</p>
</div></blockquote>
<p>See the <a class="reference external" href="https://cloud.google.com/anthos/gke/docs/on-prem/how-to/install-overview-basic">Anthos installtion overview</a>
for detailed instructions for installing Anthos in an on-premise environment and setup your cluster.</p>
</section>
<section id="setup-nvidia-software-on-gpu-nodes">
<h3>Setup NVIDIA Software on GPU Nodes<a class="headerlink" href="#setup-nvidia-software-on-gpu-nodes" title="Permalink to this heading"></a></h3>
<p>Once the Anthos cluster has been set up, you can proceed to deploy the NVIDIA software components on the GPU nodes.</p>
<section id="nvidia-drivers">
<h4>NVIDIA Drivers<a class="headerlink" href="#nvidia-drivers" title="Permalink to this heading"></a></h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>DGX systems include the NVIDIA drivers. This step can be skipped.</p>
</div>
<p>For complete instructions on setting up NVIDIA drivers, visit the quickstart
guide at <a class="reference external" href="https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html">https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html</a>.
The guide covers a number of pre-installation requirements and steps on supported Linux
distributions for a successful install of the driver.</p>
</section>
<section id="nvidia-device-plugin">
<h4>NVIDIA Device Plugin<a class="headerlink" href="#nvidia-device-plugin" title="Permalink to this heading"></a></h4>
<p>To use GPUs in Kubernetes, the <a class="reference external" href="https://github.com/NVIDIA/k8s-device-plugin/">NVIDIA Device Plugin</a> is required.
The NVIDIA Device Plugin is a daemonset that automatically enumerates the number of GPUs on each node of the cluster
and allows pods to be run on GPUs.</p>
<p>The preferred method to deploy the device plugin is as a daemonset using <code class="docutils literal notranslate"><span class="pre">helm</span></code>.</p>
<p>Add the <code class="docutils literal notranslate"><span class="pre">nvidia-device-plugin</span></code> <code class="docutils literal notranslate"><span class="pre">helm</span></code> repository:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>repo<span class="w"> </span>add<span class="w"> </span>nvdp<span class="w"> </span>https://nvidia.github.io/k8s-device-plugin<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>helm<span class="w"> </span>repo<span class="w"> </span>update
</pre></div>
</div>
<p>Deploy the device plugin:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--generate-name<span class="w"> </span>nvdp/nvidia-device-plugin
</pre></div>
</div>
<p>For more user configurable options while deploying the daemonset, refer to the device plugin
<a class="reference external" href="https://github.com/NVIDIA/k8s-device-plugin/#deployment-via-helm">README</a></p>
</section>
<section id="node-feature-discovery">
<h4>Node Feature Discovery<a class="headerlink" href="#node-feature-discovery" title="Permalink to this heading"></a></h4>
<p>For detecting the hardware configuration and system configuration, we will deploy the <a class="reference external" href="https://github.com/kubernetes-sigs/node-feature-discovery">Node Feature Discovery</a>
add-on:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>https://raw.githubusercontent.com/kubernetes-sigs/node-feature-discovery/v0.6.0/nfd-master.yaml.template
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>https://raw.githubusercontent.com/kubernetes-sigs/node-feature-discovery/v0.6.0/nfd-worker-daemonset.yaml.template
</pre></div>
</div>
<p>See the <a class="reference external" href="https://kubernetes-sigs.github.io/node-feature-discovery">NFD documentation</a> for more information on NFD.</p>
</section>
</section>
</section>
<section id="anthos-clusters-with-vmware-and-nvidia-gpu-accelerated-servers">
<span id="anthos-virt"></span><h2>Anthos Clusters with VMware and NVIDIA GPU Accelerated Servers<a class="headerlink" href="#anthos-clusters-with-vmware-and-nvidia-gpu-accelerated-servers" title="Permalink to this heading"></a></h2>
<p>Anthos running on-premise has requirements for which vSphere versions are supported along with network and storage requirements.
Please see the Anthos version compatibility matrix for more information:
<a class="reference external" href="https://cloud.google.com/anthos/gke/docs/on-prem/versioning-and-upgrades#version_compatibility_matrix.">https://cloud.google.com/anthos/gke/docs/on-prem/versioning-and-upgrades#version_compatibility_matrix</a>.</p>
<p>This guide assumes that the user already has an installed Anthos on-premise cluster in a vSphere environment. Please see
<a class="reference external" href="https://cloud.google.com/anthos/gke/docs/on-prem/how-to/install-overview-basic">https://cloud.google.com/anthos/gke/docs/on-prem/how-to/install-overview-basic</a>
for detailed instructions for installing Anthos in an on-premise environment.</p>
<p>Kubernetes provides access to special hardware resources such as NVIDIA GPUs, NICs,
Infiniband adapters and other devices through the <a class="reference external" href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">device plugin framework</a>.
However, configuring and managing nodes with these hardware resources requires
configuration of multiple software components such as drivers, container runtimes
or other libraries which are difficult and prone to errors. The <a class="reference external" href="https://github.com/NVIDIA/gpu-operator">NVIDIA GPU Operator</a>
uses the operator framework within Kubernetes to automate the management of all NVIDIA
software components needed to provision GPUs.</p>
<p>In the VMware vSphere configuration, Anthos uses the NVIDIA GPU Operator to configure GPU nodes in the Kubernetes cluster
so that the nodes can be used to schedule CUDA applications. The GPU Operator itself is
deployed using Helm. The rest of this section provides users with steps on getting
started.</p>
<section id="configuring-pcie-passthrough">
<h3>Configuring PCIe Passthrough<a class="headerlink" href="#configuring-pcie-passthrough" title="Permalink to this heading"></a></h3>
<p>For the GPU to be accessible to the VM, first you must enable <a class="reference external" href="https://kb.vmware.com/s/article/1010789">PCI Passthrough</a>
on the ESXi host. This can be done from the vSphere client. This will require a reboot
of the ESXi host to complete the process and therefore the host should be put into
maintenance mode and any VMs running on the ESXi host evacuated to another.
If you only have a single ESXi host, then the VMs will need to be restarted after the reboot.</p>
<p>From the vSphere client, select an ESXi host from the Inventory of VMware vSphere Client.
In the Configure tab, click Hardware &gt; PCI Devices. This will show you the
passthrough-enabled devices (you will most likely find none at this time).</p>
<a class="reference internal image-reference" href="../_images/image01.png"><img alt="../_images/image01.png" src="../_images/image01.png" style="width: 800px;" /></a>
<p>Click CONFIGURE PASSTHROUGH to launch the Edit PCI Device Availability window. Look for the GPU device and
select the checkbox next to it (the GPU device will be recognizable as having NVIDIA Corporation in the Vendor Name view).
Select the GPU devices (you may have more than one) and click OK.</p>
<a class="reference internal image-reference" href="../_images/image02.png"><img alt="../_images/image02.png" src="../_images/image02.png" style="width: 800px;" /></a>
<p>At this point, the GPU(s) will appear as Available (pending). You will need to select Reboot This Host and complete the reboot before proceeding to the next step.</p>
<a class="reference internal image-reference" href="../_images/image03.png"><img alt="../_images/image03.png" src="../_images/image03.png" style="width: 800px;" /></a>
<p>It is a VMware best practice to reboot an ESXi host only when it is in maintenance mode and after all the VMs have been migrated to other hosts.
If you have only 1 ESXi host, then you can reboot without migrating the VMs, though shutting them down gracefully first is always a good idea.</p>
<a class="reference internal image-reference" href="../_images/image04.png"><img alt="../_images/image04.png" src="../_images/image04.png" style="width: 400px;" /></a>
<p>Once the server has rebooted. Make sure to remove maintenance mode (if it was used) or restart the VMs that needed to be stopped (when only a single ESXi host is used).</p>
</section>
<section id="adding-gpus-to-a-node">
<h3>Adding GPUs to a Node<a class="headerlink" href="#adding-gpus-to-a-node" title="Permalink to this heading"></a></h3>
<section id="creating-a-node-pool-for-the-gpu-node">
<h4>Creating a Node Pool for the GPU Node<a class="headerlink" href="#creating-a-node-pool-for-the-gpu-node" title="Permalink to this heading"></a></h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is an optional step.</p>
</div>
<p>Node Pools are a good way to specify pools of Kubernetes worker nodes which may have different or unique attributes. In this case, we have the opportunity to
create a node pool which contains workers that manually have a GPU assigned to it. See <a class="reference external" href="https://cloud.google.com/anthos/gke/docs/on-prem/how-to/managing-node-pools?hl=en">here</a>
for more information regarding node pools with Anthos on-premise.</p>
<p>First, edit your user cluster config.yaml file on the admin workstation and add an additional node pool:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">- name: user-cluster1-gpu</span>
<span class="go">  cpus: 4</span>
<span class="go">  memoryMB: 8192</span>
<span class="go">  replicas: 1</span>
<span class="go">  labels:</span>
<span class="go">    hardware: gpu</span>
</pre></div>
</div>
<p>After adding the node pool to your configuration, use the <code class="docutils literal notranslate"><span class="pre">gkectl</span></code> update command push the change:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>gkectl<span class="w"> </span>update<span class="w"> </span>cluster<span class="w"> </span>--kubeconfig<span class="w"> </span><span class="o">[</span>ADMIN_CLUSTER_KUBECONFIG<span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--config<span class="w"> </span><span class="o">[</span>USER_CLUSTER_KUBECONFIG<span class="o">]</span>
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">Reading config with version &quot;v1&quot;</span>
<span class="go">Update summary for cluster user-cluster1-bundledlb:</span>
<span class="go">   Node pool(s) to be created: [user-cluster1-gpu]</span>
<span class="go">Do you want to continue? [Y/n]: Y</span>
<span class="go">Updating cluster &quot;user-cluster1-bundledlb&quot;...</span>
<span class="go">Creating node MachineDeployment(s) in user cluster...  DONE</span>
<span class="go">Done updating the user cluster</span>
</pre></div>
</div>
</section>
<section id="add-gpus-to-nodes-in-vsphere">
<h4>Add GPUs to Nodes in vSphere<a class="headerlink" href="#add-gpus-to-nodes-in-vsphere" title="Permalink to this heading"></a></h4>
<p>Select an existing user-cluster node to add a GPU to (if you created a node pool
with the previous step then you would choose a node from that pool). Make sure that
this VM is on the host with the GPU (if you have vMotion enabled this could be as
simple as right clicking on the VM and selecting <strong>Migrate</strong>).</p>
<p>To configure a PCI device on a virtual machine, from the Inventory in vSphere Client,
right-click the virtual machine and select <strong>Power-&gt;Power Off</strong>.</p>
<a class="reference internal image-reference" href="../_images/image05.png"><img alt="../_images/image05.png" src="../_images/image05.png" style="width: 800px;" /></a>
<p>After the VM is powered off, right-click the virtual machine and click <strong>Edit Settings</strong>.</p>
<a class="reference internal image-reference" href="../_images/image06.png"><img alt="../_images/image06.png" src="../_images/image06.png" style="width: 400px;" /></a>
<p>Within the Edit Settings window, click <strong>ADD NEW DEVICE</strong>.</p>
<a class="reference internal image-reference" href="../_images/image07.png"><img alt="../_images/image07.png" src="../_images/image07.png" style="width: 800px;" /></a>
<p>Choose PCI Device from the dropdown.</p>
<a class="reference internal image-reference" href="../_images/image08.png"><img alt="../_images/image08.png" src="../_images/image08.png" style="width: 400px;" /></a>
<p>You may need to select the GPU or if it’s the only device available it may be automatically
selected for you. If you don’t see the GPU, it’s possible your VM is not currently on the
ESXi host with the passthrough device configured.</p>
<a class="reference internal image-reference" href="../_images/image09.png"><img alt="../_images/image09.png" src="../_images/image09.png" style="width: 800px;" /></a>
<p>Expand the <strong>Memory</strong> section and make sure to select the option for Reserve all <strong>Guest Memory (All locked)</strong>.</p>
<a class="reference internal image-reference" href="../_images/image10.png"><img alt="../_images/image10.png" src="../_images/image10.png" style="width: 800px;" /></a>
<p>Click <strong>OK</strong>.</p>
<p>Before the VM can be started, the VM/Host Rule for VM anti-affinity must be deleted.
(Note that this step may not be necessary if your cluster’s <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> contains <code class="docutils literal notranslate"><span class="pre">antiAffinityGroups.enabled:</span> <span class="pre">False</span></code>).
From the vSphere Inventory list, click on the cluster then the <strong>Configure</strong> tab and then
under <strong>Configuration</strong> select <strong>VM/Host Rules</strong>. Select the rule containing your node and delete it.</p>
<a class="reference internal image-reference" href="../_images/image11.png"><img alt="../_images/image11.png" src="../_images/image11.png" style="width: 800px;" /></a>
<p>Now you can power on the VM, right click on the VM and select <strong>Power&gt;Power On</strong>.</p>
<a class="reference internal image-reference" href="../_images/image12.png"><img alt="../_images/image12.png" src="../_images/image12.png" style="width: 800px;" /></a>
<p>If vSphere presents you with <strong>Power On Recommendations</strong> then select <strong>OK</strong>.</p>
<a class="reference internal image-reference" href="../_images/image13.png"><img alt="../_images/image13.png" src="../_images/image13.png" style="width: 800px;" /></a>
<p>The following steps should be performed from your Admin Workstation or other Linux system which has the ability to use <code class="docutils literal notranslate"><span class="pre">kubectl</span></code> to work with the cluster.</p>
</section>
</section>
<section id="install-nvidia-gpu-operator">
<span id="install-gpu-operator"></span><h3>Install NVIDIA GPU Operator<a class="headerlink" href="#install-nvidia-gpu-operator" title="Permalink to this heading"></a></h3>
<section id="install-helm">
<h4>Install Helm<a class="headerlink" href="#install-helm" title="Permalink to this heading"></a></h4>
<p>The preferred method to deploy the GPU Operator is using <code class="docutils literal notranslate"><span class="pre">helm</span></code>.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>curl<span class="w"> </span>-fsSL<span class="w"> </span>-o<span class="w"> </span>get_helm.sh<span class="w"> </span>https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>chmod<span class="w"> </span><span class="m">700</span><span class="w"> </span>get_helm.sh<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>./get_helm.sh
</pre></div>
</div>
<p>Now, add the NVIDIA Helm repository:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>repo<span class="w"> </span>add<span class="w"> </span>nvidia<span class="w"> </span>https://helm.ngc.nvidia.com/nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="o">&amp;&amp;</span><span class="w"> </span>helm<span class="w"> </span>repo<span class="w"> </span>update
</pre></div>
</div>
</section>
<section id="install-the-gpu-operator">
<h4>Install the GPU Operator<a class="headerlink" href="#install-the-gpu-operator" title="Permalink to this heading"></a></h4>
<p>The GPU Operator Helm chart offers a number of customizable options that can be configured depending on your environment.</p>
<div class="align-default"><img height="120" src="../_images/blockdiag-9ab121b0af6e54db8e70aef1d4d49cd964af7cd7.png" width="640" /></div>
<section id="chart-customization-options">
<span id="gpu-operator-helm-chart-options"></span><h5>Chart Customization Options<a class="headerlink" href="#chart-customization-options" title="Permalink to this heading"></a></h5>
<p>The following options are available when using the Helm chart. These options can be used with <code class="docutils literal notranslate"><span class="pre">--set</span></code> when installing via Helm.</p>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 50%" />
<col style="width: 30%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">cdi.enabled</span></code></p></td>
<td><p>When set to <code class="docutils literal notranslate"><span class="pre">true</span></code>, the Operator installs two additional runtime classes,
nvidia-cdi and nvidia-legacy, and enables the use of the Container Device Interface (CDI)
for making GPUs accessible to containers.
Using CDI aligns the Operator with the recent efforts to standardize how complex devices like GPUs
are exposed to containerized environments.</p>
<p>Pods can specify <code class="docutils literal notranslate"><span class="pre">spec.runtimeClassName</span></code> as <code class="docutils literal notranslate"><span class="pre">nvidia-cdi</span></code> to use the functionality or
specify <code class="docutils literal notranslate"><span class="pre">nvidia-legacy</span></code> to prevent using CDI to perform device injection.</p>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">cdi.default</span></code></p></td>
<td><p>When set to <code class="docutils literal notranslate"><span class="pre">true</span></code>, the container runtime uses CDI to perform device injection by default.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">driver.enabled</span></code></p></td>
<td><p>By default, the Operator deploys NVIDIA drivers as a container on the system.
Set this value to <code class="docutils literal notranslate"><span class="pre">false</span></code> when using the Operator on systems with pre-installed drivers.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">true</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">driver.repository</span></code></p></td>
<td><p>The images are downloaded from NGC. Specify another image repository when using
custom driver images.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nvcr.io/nvidia</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">driver.rdma.enabled</span></code></p></td>
<td><p>Controls whether the driver daemonset should build and load the <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> kernel module.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">driver.rdma.useHostMofed</span></code></p></td>
<td><p>Indicate if MOFED is directly pre-installed on the host. This is used to build and load <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> kernel module.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">driver.version</span></code></p></td>
<td><p>Version of the NVIDIA datacenter driver supported by the Operator.</p></td>
<td><p>Depends on the version of the Operator. See the Component Matrix
for more information on supported drivers.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">mig.strategy</span></code></p></td>
<td><p>Controls the strategy to be used with MIG on supported NVIDIA GPUs. Options
are either <code class="docutils literal notranslate"><span class="pre">mixed</span></code> or <code class="docutils literal notranslate"><span class="pre">single</span></code>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">single</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">migManager.enabled</span></code></p></td>
<td><p>The MIG manager watches for changes to the MIG geometry and applies reconfiguration as needed. By
default, the MIG manager only runs on nodes with GPUs that support MIG (for e.g. A100).</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">true</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">nfd.enabled</span></code></p></td>
<td><p>Deploys Node Feature Discovery plugin as a daemonset.
Set this variable to <code class="docutils literal notranslate"><span class="pre">false</span></code> if NFD is already running in the cluster.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">true</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">operator.annotations</span></code></p></td>
<td><p>Map of custom annotations that will be added to all GPU Operator managed pods.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">{}</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">operator.defaultRuntime</span></code></p></td>
<td><p><strong>DEPRECATED as of v1.9</strong></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">docker</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">operator.labels</span></code></p></td>
<td><p>Map of custom labels that will be added to all GPU Operator managed pods.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">{}</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">psp.enabled</span></code></p></td>
<td><p>The GPU operator deploys <code class="docutils literal notranslate"><span class="pre">PodSecurityPolicies</span></code> if enabled.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">toolkit.enabled</span></code></p></td>
<td><p>By default, the Operator deploys the NVIDIA Container Toolkit (<code class="docutils literal notranslate"><span class="pre">nvidia-docker2</span></code> stack)
as a container on the system. Set this value to <code class="docutils literal notranslate"><span class="pre">false</span></code> when using the Operator on systems
with pre-installed NVIDIA runtimes.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">true</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="namespace">
<h5>Namespace<a class="headerlink" href="#namespace" title="Permalink to this heading"></a></h5>
<p>Prior to GPU Operator v1.9, the operator was installed in the <code class="docutils literal notranslate"><span class="pre">default</span></code> namespace while all operands were
installed in the <code class="docutils literal notranslate"><span class="pre">gpu-operator-resources</span></code> namespace.</p>
<p>Starting with GPU Operator v1.9, both the operator and operands get installed in the same namespace.
The namespace is configurable and is determined during installation. For example, to install the GPU Operator
in the <code class="docutils literal notranslate"><span class="pre">gpu-operator</span></code> namespace:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--create-namespace<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>nvidia/gpu-operator
</pre></div>
</div>
<p>If a namespace is not specified during installation, all GPU Operator components will be installed in the
<code class="docutils literal notranslate"><span class="pre">default</span></code> namespace.</p>
</section>
<section id="operands">
<h5>Operands<a class="headerlink" href="#operands" title="Permalink to this heading"></a></h5>
<p>By default, the GPU Operator operands are deployed on all GPU worker nodes in the cluster.
GPU worker nodes are identified by the presence of the label <code class="docutils literal notranslate"><span class="pre">feature.node.kubernetes.io/pci-10de.present=true</span></code>,
where <code class="docutils literal notranslate"><span class="pre">0x10de</span></code> is the PCI vendor ID assigned to NVIDIA.</p>
<p>To disable operands from getting deployed on a GPU worker node, label the node with <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.deploy.operands=false</span></code>.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>label<span class="w"> </span>nodes<span class="w"> </span><span class="nv">$NODE</span><span class="w"> </span>nvidia.com/gpu.deploy.operands<span class="o">=</span><span class="nb">false</span>
</pre></div>
</div>
</section>
<section id="common-deployment-scenarios">
<h5>Common Deployment Scenarios<a class="headerlink" href="#common-deployment-scenarios" title="Permalink to this heading"></a></h5>
<p>In this section, we present some common deployment recipes when using the Helm chart to install the GPU Operator.</p>
<section id="bare-metal-passthrough-with-default-configurations-on-ubuntu">
<h6>Bare-metal/Passthrough with default configurations on Ubuntu<a class="headerlink" href="#bare-metal-passthrough-with-default-configurations-on-ubuntu" title="Permalink to this heading"></a></h6>
<p>In this scenario, the default configuration options are used:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--create-namespace<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>nvidia/gpu-operator
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>For installing on Secure Boot systems or using Precompiled modules refer to <a class="reference internal" href="../gpu-operator/install-precompiled-signed-drivers.html#install-precompiled-signed-drivers"><span class="std std-ref">Installing Precompiled and Canonical Signed Drivers on Ubuntu 20.04 and 22.04</span></a>.</p></li>
</ul>
</div>
</section>
<section id="bare-metal-passthrough-with-default-configurations-on-red-hat-enterprise-linux">
<h6>Bare-metal/Passthrough with default configurations on Red Hat Enterprise Linux<a class="headerlink" href="#bare-metal-passthrough-with-default-configurations-on-red-hat-enterprise-linux" title="Permalink to this heading"></a></h6>
<p>In this scenario, the default configuration options are used:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--create-namespace<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>nvidia/gpu-operator
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>When using RHEL8 with Kubernetes, SELinux has to be enabled (either in permissive or enforcing mode) for use with the GPU Operator. Additionally, network restricted environments are not supported.</p></li>
</ul>
</div>
</section>
<section id="bare-metal-passthrough-with-default-configurations-on-centos">
<h6>Bare-metal/Passthrough with default configurations on CentOS<a class="headerlink" href="#bare-metal-passthrough-with-default-configurations-on-centos" title="Permalink to this heading"></a></h6>
<p>In this scenario, the CentOS toolkit image is used:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--create-namespace<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>nvidia/gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>toolkit.version<span class="o">=</span><span class="m">1</span>.7.1-centos7
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>For CentOS 8 systems, use <cite>toolkit.version=1.7.1-centos8</cite>.</p></li>
<li><p>Replace <cite>1.7.1</cite> toolkit version used here with the latest one available <a class="reference external" href="https://ngc.nvidia.com/catalog/containers/nvidia:k8s:container-toolkit/tags">here</a>.</p></li>
</ul>
</div>
</section>
<hr class="docutils" />
<section id="nvidia-vgpu">
<h6>NVIDIA vGPU<a class="headerlink" href="#nvidia-vgpu" title="Permalink to this heading"></a></h6>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The GPU Operator with NVIDIA vGPUs requires additional steps to build a private driver image prior to install.
Refer to the document <a class="reference internal" href="../gpu-operator/install-gpu-operator-vgpu.html#install-gpu-operator-vgpu"><span class="std std-ref">NVIDIA vGPU</span></a> for detailed instructions on the workflow and required values of
the variables used in this command.</p>
</div>
<p>The command below will install the GPU Operator with its default configuration for vGPU:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--create-namespace<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>nvidia/gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>driver.repository<span class="o">=</span><span class="nv">$PRIVATE_REGISTRY</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>driver.version<span class="o">=</span><span class="nv">$VERSION</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>driver.imagePullSecrets<span class="o">={</span><span class="nv">$REGISTRY_SECRET_NAME</span><span class="o">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>driver.licensingConfig.configMapName<span class="o">=</span>licensing-config
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="nvidia-ai-enterprise">
<h6>NVIDIA AI Enterprise<a class="headerlink" href="#nvidia-ai-enterprise" title="Permalink to this heading"></a></h6>
<p>Refer to <a class="reference internal" href="../gpu-operator/install-gpu-operator-nvaie.html#install-gpu-operator-nvaie"><span class="std std-ref">GPU Operator with NVIDIA AI Enterprise</span></a>.</p>
</section>
<hr class="docutils" />
<section id="bare-metal-passthrough-with-pre-installed-nvidia-drivers">
<h6>Bare-metal/Passthrough with pre-installed NVIDIA drivers<a class="headerlink" href="#bare-metal-passthrough-with-pre-installed-nvidia-drivers" title="Permalink to this heading"></a></h6>
<p>In this example, the user has already pre-installed NVIDIA drivers as part of the system image:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--create-namespace<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>nvidia/gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>driver.enabled<span class="o">=</span><span class="nb">false</span>
</pre></div>
</div>
<hr class="docutils" />
</section>
<section id="bare-metal-passthrough-with-pre-installed-drivers-and-nvidia-container-toolkit">
<span id="preinstalled-drivers-and-toolkit"></span><h6>Bare-metal/Passthrough with pre-installed drivers and NVIDIA Container Toolkit<a class="headerlink" href="#bare-metal-passthrough-with-pre-installed-drivers-and-nvidia-container-toolkit" title="Permalink to this heading"></a></h6>
<p>In this example, the user has already pre-installed the NVIDIA drivers and NVIDIA Container Toolkit (<code class="docutils literal notranslate"><span class="pre">nvidia-docker2</span></code>)
as part of the system image.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>These steps should be followed when using the GPU Operator v1.9+ on DGX A100 systems with DGX OS 5.1+.</p>
</div>
<p>Before installing the operator, ensure that the following configurations are modified depending on the container runtime configured in your cluster.</p>
<p>Docker:</p>
<blockquote>
<div><ul>
<li><p>Update the Docker configuration to add <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> as the default runtime. The <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> runtime should
be setup as the default container runtime for Docker on GPU nodes. This can be done by adding the
<code class="docutils literal notranslate"><span class="pre">default-runtime</span></code> line into the Docker daemon config file, which is usually located on the system
at <code class="docutils literal notranslate"><span class="pre">/etc/docker/daemon.json</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go">    &quot;default-runtime&quot;: &quot;nvidia&quot;,</span>
<span class="go">    &quot;runtimes&quot;: {</span>
<span class="go">        &quot;nvidia&quot;: {</span>
<span class="go">            &quot;path&quot;: &quot;/usr/bin/nvidia-container-runtime&quot;,</span>
<span class="go">            &quot;runtimeArgs&quot;: []</span>
<span class="go">      }</span>
<span class="go">    }</span>
<span class="go">}</span>
</pre></div>
</div>
<p>Restart the Docker daemon to complete the installation after setting the default runtime:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>docker
</pre></div>
</div>
</li>
</ul>
</div></blockquote>
<p>Containerd:</p>
<blockquote>
<div><ul>
<li><p>Update <code class="docutils literal notranslate"><span class="pre">containerd</span></code> to use <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> as the default runtime and add <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> runtime configuration.
This can be done by adding below config to <code class="docutils literal notranslate"><span class="pre">/etc/containerd/config.toml</span></code> and restarting <code class="docutils literal notranslate"><span class="pre">containerd</span></code> service.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">version = 2</span>
<span class="go">[plugins]</span>
<span class="go">  [plugins.&quot;io.containerd.grpc.v1.cri&quot;]</span>
<span class="go">    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd]</span>
<span class="go">      default_runtime_name = &quot;nvidia&quot;</span>

<span class="go">      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes]</span>
<span class="go">        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.nvidia]</span>
<span class="go">          privileged_without_host_devices = false</span>
<span class="go">          runtime_engine = &quot;&quot;</span>
<span class="go">          runtime_root = &quot;&quot;</span>
<span class="go">          runtime_type = &quot;io.containerd.runc.v2&quot;</span>
<span class="go">          [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.nvidia.options]</span>
<span class="go">            BinaryName = &quot;/usr/bin/nvidia-container-runtime&quot;</span>
</pre></div>
</div>
<p>Restart the Containerd daemon to complete the installation after setting the default runtime:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>containerd
</pre></div>
</div>
</li>
</ul>
</div></blockquote>
<p>Install the GPU operator with the following options:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--create-namespace<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>nvidia/gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--set<span class="w"> </span>driver.enabled<span class="o">=</span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">      </span>--set<span class="w"> </span>toolkit.enabled<span class="o">=</span><span class="nb">false</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="bare-metal-passthrough-with-pre-installed-nvidia-container-toolkit-but-no-drivers">
<h6>Bare-metal/Passthrough with pre-installed NVIDIA Container Toolkit (but no drivers)<a class="headerlink" href="#bare-metal-passthrough-with-pre-installed-nvidia-container-toolkit-but-no-drivers" title="Permalink to this heading"></a></h6>
<p>In this example, the user has already pre-installed the NVIDIA Container Toolkit (<code class="docutils literal notranslate"><span class="pre">nvidia-docker2</span></code>) as part of the system image.</p>
<p>Before installing the operator, ensure that the following configurations are modified depending on the container runtime configured in your cluster.</p>
<p>Docker:</p>
<blockquote>
<div><ul>
<li><p>Update the Docker configuration to add <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> as the default runtime. The <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> runtime should
be setup as the default container runtime for Docker on GPU nodes. This can be done by adding the
<code class="docutils literal notranslate"><span class="pre">default-runtime</span></code> line into the Docker daemon config file, which is usually located on the system
at <code class="docutils literal notranslate"><span class="pre">/etc/docker/daemon.json</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go">    &quot;default-runtime&quot;: &quot;nvidia&quot;,</span>
<span class="go">    &quot;runtimes&quot;: {</span>
<span class="go">        &quot;nvidia&quot;: {</span>
<span class="go">            &quot;path&quot;: &quot;/usr/bin/nvidia-container-runtime&quot;,</span>
<span class="go">            &quot;runtimeArgs&quot;: []</span>
<span class="go">      }</span>
<span class="go">    }</span>
<span class="go">}</span>
</pre></div>
</div>
<p>Restart the Docker daemon to complete the installation after setting the default runtime:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>docker
</pre></div>
</div>
</li>
</ul>
</div></blockquote>
<p>Containerd:</p>
<blockquote>
<div><ul>
<li><p>Update <code class="docutils literal notranslate"><span class="pre">containerd</span></code> to use <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> as the default runtime and add <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> runtime configuration.
This can be done by adding below config to <code class="docutils literal notranslate"><span class="pre">/etc/containerd/config.toml</span></code> and restarting <code class="docutils literal notranslate"><span class="pre">containerd</span></code> service.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">version = 2</span>
<span class="go">[plugins]</span>
<span class="go">  [plugins.&quot;io.containerd.grpc.v1.cri&quot;]</span>
<span class="go">    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd]</span>
<span class="go">      default_runtime_name = &quot;nvidia&quot;</span>

<span class="go">      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes]</span>
<span class="go">        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.nvidia]</span>
<span class="go">          privileged_without_host_devices = false</span>
<span class="go">          runtime_engine = &quot;&quot;</span>
<span class="go">          runtime_root = &quot;&quot;</span>
<span class="go">          runtime_type = &quot;io.containerd.runc.v2&quot;</span>
<span class="go">          [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.nvidia.options]</span>
<span class="go">            BinaryName = &quot;/usr/bin/nvidia-container-runtime&quot;</span>
</pre></div>
</div>
<p>Restart the Containerd daemon to complete the installation after setting the default runtime:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>containerd
</pre></div>
</div>
</li>
</ul>
</div></blockquote>
<p>Configure toolkit to use the <code class="docutils literal notranslate"><span class="pre">root</span></code> directory of the driver installation as <code class="docutils literal notranslate"><span class="pre">/run/nvidia/driver</span></code>, which is the path mounted by driver container.</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo<span class="w"> </span>sed<span class="w"> </span>-i<span class="w"> </span><span class="s1">&#39;s/^#root/root/&#39;</span><span class="w"> </span>/etc/nvidia-container-runtime/config.toml
</pre></div>
</div>
</div></blockquote>
<p>Once these steps are complete, now install the GPU operator with the following options (which will provision a driver):</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--create-namespace<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>nvidia/gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>toolkit.enabled<span class="o">=</span><span class="nb">false</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="custom-driver-image-based-off-a-specific-driver-version">
<h6>Custom driver image (based off a specific driver version)<a class="headerlink" href="#custom-driver-image-based-off-a-specific-driver-version" title="Permalink to this heading"></a></h6>
<p>If you want to use custom driver container images (for e.g. using 465.27), then
you would need to build a new driver container image. Follow these steps:</p>
<ul>
<li><p>Rebuild the driver container by specifying the <code class="docutils literal notranslate"><span class="pre">$DRIVER_VERSION</span></code> argument when building the Docker image. For
reference, the driver container Dockerfiles are available on the Git repo <a class="reference external" href="https://gitlab.com/nvidia/container-images/driver">here</a></p></li>
<li><p>Build the container using the appropriate Dockerfile. For example:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker<span class="w"> </span>build<span class="w"> </span>--pull<span class="w"> </span>-t<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--build-arg<span class="w"> </span><span class="nv">DRIVER_VERSION</span><span class="o">=</span><span class="m">455</span>.28<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>nvidia/driver:455.28-ubuntu20.04<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--file<span class="w"> </span>Dockerfile<span class="w"> </span>.
</pre></div>
</div>
<p>Ensure that the driver container is tagged as shown in the example by using the <code class="docutils literal notranslate"><span class="pre">driver:&lt;version&gt;-&lt;os&gt;</span></code> schema.</p>
</li>
<li><p>Specify the new driver image and repository by overriding the defaults in
the Helm install command. For example:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--create-namespace<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>nvidia/gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>driver.repository<span class="o">=</span>docker.io/nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>driver.version<span class="o">=</span><span class="s2">&quot;465.27&quot;</span>
</pre></div>
</div>
</li>
</ul>
<p>Note that these instructions are provided for reference and evaluation purposes.
Not using the standard releases of the GPU Operator from NVIDIA would mean limited
support for such custom configurations.</p>
</section>
<hr class="docutils" id="custom-runtime-options" />
<section id="custom-configuration-for-runtime-containerd">
<h6>Custom configuration for runtime <code class="docutils literal notranslate"><span class="pre">containerd</span></code><a class="headerlink" href="#custom-configuration-for-runtime-containerd" title="Permalink to this heading"></a></h6>
<p>When <cite>containerd</cite> is the container runtime used, the following configuration
options are used with the container-toolkit deployed with GPU Operator:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">toolkit</span><span class="p">:</span>
<span class="w">   </span><span class="nt">env</span><span class="p">:</span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CONTAINERD_CONFIG</span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/etc/containerd/config.toml</span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CONTAINERD_SOCKET</span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/run/containerd/containerd.sock</span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CONTAINERD_RUNTIME_CLASS</span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia</span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CONTAINERD_SET_AS_DEFAULT</span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
<p>These options are defined as follows:</p>
<blockquote>
<div><ul>
<li><dl>
<dt><strong>CONTAINERD_CONFIG</strong><span class="classifier">The path on the host to the <code class="docutils literal notranslate"><span class="pre">containerd</span></code> config</span></dt><dd><p>you would like to have updated with support for the <code class="docutils literal notranslate"><span class="pre">nvidia-container-runtime</span></code>.
By default this will point to <code class="docutils literal notranslate"><span class="pre">/etc/containerd/config.toml</span></code> (the default
location for <code class="docutils literal notranslate"><span class="pre">containerd</span></code>). It should be customized if your <code class="docutils literal notranslate"><span class="pre">containerd</span></code>
installation is not in the default location.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>CONTAINERD_SOCKET</strong><span class="classifier">The path on the host to the socket file used to</span></dt><dd><p>communicate with <code class="docutils literal notranslate"><span class="pre">containerd</span></code>. The operator will use this to send a
<code class="docutils literal notranslate"><span class="pre">SIGHUP</span></code> signal to the <code class="docutils literal notranslate"><span class="pre">containerd</span></code> daemon to reload its config. By
default this will point to <code class="docutils literal notranslate"><span class="pre">/run/containerd/containerd.sock</span></code>
(the default location for <code class="docutils literal notranslate"><span class="pre">containerd</span></code>). It should be customized if
your <code class="docutils literal notranslate"><span class="pre">containerd</span></code> installation is not in the default location.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>CONTAINERD_RUNTIME_CLASS</strong><span class="classifier">The name of the</span></dt><dd><p><a class="reference external" href="https://kubernetes.io/docs/concepts/containers/runtime-class">Runtime Class</a>
you would like to associate with the <code class="docutils literal notranslate"><span class="pre">nvidia-container-runtime</span></code>.
Pods launched with a <code class="docutils literal notranslate"><span class="pre">runtimeClassName</span></code> equal to CONTAINERD_RUNTIME_CLASS
will always run with the <code class="docutils literal notranslate"><span class="pre">nvidia-container-runtime</span></code>. The default
CONTAINERD_RUNTIME_CLASS is <code class="docutils literal notranslate"><span class="pre">nvidia</span></code>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>CONTAINERD_SET_AS_DEFAULT</strong><span class="classifier">A flag indicating whether you want to set</span></dt><dd><p><code class="docutils literal notranslate"><span class="pre">nvidia-container-runtime</span></code> as the default runtime used to launch all
containers. When set to false, only containers in pods with a <code class="docutils literal notranslate"><span class="pre">runtimeClassName</span></code>
equal to CONTAINERD_RUNTIME_CLASS will be run with the <code class="docutils literal notranslate"><span class="pre">nvidia-container-runtime</span></code>.
The default value is <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p>For using with RKE2 (Rancher Kubernetes Engine 2) or K3s following settings needs to be set in <cite>ClusterPolicy</cite>.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">toolkit</span><span class="p">:</span>
<span class="w">   </span><span class="nt">env</span><span class="p">:</span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CONTAINERD_CONFIG</span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl</span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CONTAINERD_SOCKET</span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/run/k3s/containerd/containerd.sock</span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CONTAINERD_RUNTIME_CLASS</span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia</span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CONTAINERD_SET_AS_DEFAULT</span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;true&quot;</span>
</pre></div>
</div>
<p>These options can be passed to GPU Operator during install time as below.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">helm install -n gpu-operator --create-namespace \</span>
<span class="go">  nvidia/gpu-operator $HELM_OPTIONS \</span>
<span class="go">    --set toolkit.env[0].name=CONTAINERD_CONFIG \</span>
<span class="go">    --set toolkit.env[0].value=/var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl \</span>
<span class="go">    --set toolkit.env[1].name=CONTAINERD_SOCKET \</span>
<span class="go">    --set toolkit.env[1].value=/run/k3s/containerd/containerd.sock \</span>
<span class="go">    --set toolkit.env[2].name=CONTAINERD_RUNTIME_CLASS \</span>
<span class="go">    --set toolkit.env[2].value=nvidia \</span>
<span class="go">    --set toolkit.env[3].name=CONTAINERD_SET_AS_DEFAULT \</span>
<span class="go">    --set-string toolkit.env[3].value=true</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="proxy-environments">
<h6>Proxy Environments<a class="headerlink" href="#proxy-environments" title="Permalink to this heading"></a></h6>
<p>Refer to the section <a class="reference internal" href="../gpu-operator/install-gpu-operator-proxy.html#install-gpu-operator-proxy"><span class="std std-ref">Install GPU Operator in Proxy Environments</span></a> for more information on how to install the Operator on clusters
behind a HTTP proxy.</p>
</section>
<hr class="docutils" />
<section id="air-gapped-environments">
<h6>Air-gapped Environments<a class="headerlink" href="#air-gapped-environments" title="Permalink to this heading"></a></h6>
<p>Refer to the section <a class="reference internal" href="../gpu-operator/install-gpu-operator-air-gapped.html#install-gpu-operator-air-gapped"><span class="std std-ref">Install GPU Operator in Air-gapped Environments</span></a> for more information on how to install the Operator
in air-gapped environments.</p>
</section>
<hr class="docutils" />
<section id="multi-instance-gpu-mig">
<h6>Multi-Instance GPU (MIG)<a class="headerlink" href="#multi-instance-gpu-mig" title="Permalink to this heading"></a></h6>
<p>Refer to the document <a class="reference internal" href="../gpu-operator/gpu-operator-mig.html#install-gpu-operator-mig"><span class="std std-ref">GPU Operator with MIG</span></a> for more information on how use the Operator with Multi-Instance GPU (MIG)
on NVIDIA Ampere products. For guidance on configuring MIG support for the <strong>NVIDIA GPU Operator</strong> in an OpenShift Container Platform cluster, see the <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/openshift/mig-ocp.html">user guide</a>.</p>
</section>
<hr class="docutils" />
<section id="kubevirt-openshift-virtualization">
<h6>KubeVirt / OpenShift Virtualization<a class="headerlink" href="#kubevirt-openshift-virtualization" title="Permalink to this heading"></a></h6>
<p>Refer to the document <a class="reference internal" href="../gpu-operator/gpu-operator-kubevirt.html#gpu-operator-kubevirt"><span class="std std-ref">GPU Operator with KubeVirt</span></a> for more information on how to use the GPU Operator to provision GPU nodes for running KubeVirt virtual machines with access to GPU.
For guidance on using the GPU Operator with OpenShift Virtualization, refer to the document <span class="xref std std-ref">nvidia-gpu-operator-openshift-virtualization-vgpu-enablement</span>.</p>
</section>
<section id="outdated-kernels">
<h6>Outdated Kernels<a class="headerlink" href="#outdated-kernels" title="Permalink to this heading"></a></h6>
<p>Refer to the section <a class="reference internal" href="../gpu-operator/install-gpu-operator-outdated-kernels.html#install-gpu-operator-outdated-kernels"><span class="std std-ref">Considerations when Installing with Outdated Kernels in Cluster</span></a> for more information on how to install the Operator successfully
when nodes in the cluster are not running the latest kernel</p>
</section>
</section>
<hr class="docutils" />
<section id="verify-gpu-operator-install">
<h5>Verify GPU Operator Install<a class="headerlink" href="#verify-gpu-operator-install" title="Permalink to this heading"></a></h5>
<p>Once the Helm chart is installed, check the status of the pods to ensure all the containers are running and the validation is complete:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>gpu-operator
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">NAME                                                          READY   STATUS      RESTARTS   AGE</span>
<span class="go">gpu-feature-discovery-crrsq                                   1/1     Running     0          60s</span>
<span class="go">gpu-operator-7fb75556c7-x8spj                                 1/1     Running     0          5m13s</span>
<span class="go">gpu-operator-node-feature-discovery-master-58d884d5cc-w7q7b   1/1     Running     0          5m13s</span>
<span class="go">gpu-operator-node-feature-discovery-worker-6rht2              1/1     Running     0          5m13s</span>
<span class="go">gpu-operator-node-feature-discovery-worker-9r8js              1/1     Running     0          5m13s</span>
<span class="go">nvidia-container-toolkit-daemonset-lhgqf                      1/1     Running     0          4m53s</span>
<span class="go">nvidia-cuda-validator-rhvbb                                   0/1     Completed   0          54s</span>
<span class="go">nvidia-dcgm-5jqzg                                             1/1     Running     0          60s</span>
<span class="go">nvidia-dcgm-exporter-h964h                                    1/1     Running     0          60s</span>
<span class="go">nvidia-device-plugin-daemonset-d9ntc                          1/1     Running     0          60s</span>
<span class="go">nvidia-device-plugin-validator-cm2fd                          0/1     Completed   0          48s</span>
<span class="go">nvidia-driver-daemonset-5xj6g                                 1/1     Running     0          4m53s</span>
<span class="go">nvidia-mig-manager-89z9b                                      1/1     Running     0          4m53s</span>
<span class="go">nvidia-operator-validator-bwx99                               1/1     Running     0          58s</span>
</pre></div>
</div>
<p>We can now proceed to running some sample GPU workloads to verify that the Operator (and its components) are working correctly.</p>
</section>
</section>
</section>
<section id="running-gpu-applications">
<h3>Running GPU Applications<a class="headerlink" href="#running-gpu-applications" title="Permalink to this heading"></a></h3>
<section id="jupyter-notebooks">
<h4>Jupyter Notebooks<a class="headerlink" href="#jupyter-notebooks" title="Permalink to this heading"></a></h4>
<p>This section of the guide walks through how to run a sample Jupyter notebook on the Kubernetes cluster.</p>
<ol class="arabic">
<li><p>Create a yaml file for the pod and service for the notebook:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nv">LOADBALANCERIP</span><span class="o">=</span>&lt;ip<span class="w"> </span>address<span class="w"> </span>to<span class="w"> </span>be<span class="w"> </span>used<span class="w"> </span>to<span class="w"> </span>expose<span class="w"> </span>the<span class="w"> </span>service&gt;
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat<span class="w"> </span>&lt;&lt;<span class="w"> </span>EOF<span class="w"> </span><span class="p">|</span><span class="w"> </span>kubectl<span class="w"> </span>create<span class="w"> </span>-f<span class="w"> </span>-
<span class="go">apiVersion: v1</span>
<span class="go">kind: Service</span>
<span class="go">metadata:</span>
<span class="go">  name: tf-notebook</span>
<span class="go">  labels:</span>
<span class="go">    app: tf-notebook</span>
<span class="go">spec:</span>
<span class="go">  type: LoadBalancer</span>
<span class="go">  loadBalancerIP: $LOADBALANCERIP</span>
<span class="go">  ports:</span>
<span class="go">  - port: 80</span>
<span class="go">    name: http</span>
<span class="go">    targetPort: 8888</span>
<span class="go">    nodePort: 30001</span>
<span class="go">  selector:</span>
<span class="go">    app: tf-notebook</span>
<span class="go">---</span>
<span class="go">apiVersion: v1</span>
<span class="go">kind: Pod</span>
<span class="go">metadata:</span>
<span class="go">  name: tf-notebook</span>
<span class="go">  labels:</span>
<span class="go">    app: tf-notebook</span>
<span class="go">spec:</span>
<span class="go">  securityContext:</span>
<span class="go">    fsGroup: 0</span>
<span class="go">  containers:</span>
<span class="go">  - name: tf-notebook</span>
<span class="go">    image: tensorflow/tensorflow:latest-gpu-jupyter</span>
<span class="go">    resources:</span>
<span class="go">      limits:</span>
<span class="go">        nvidia.com/gpu: 1</span>
<span class="go">    ports:</span>
<span class="go">    - containerPort: 8888</span>
<span class="go">      name: notebook</span>
<span class="go">EOF</span>
</pre></div>
</div>
</li>
<li><p>View the logs of the tf-notebook pod to obtain the token:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>logs<span class="w"> </span>tf-notebook
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">[I 19:07:43.061 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret</span>
<span class="go">[I 19:07:43.423 NotebookApp] Serving notebooks from local directory: /tf</span>
<span class="go">[I 19:07:43.423 NotebookApp] The Jupyter Notebook is running at:</span>
<span class="go">[I 19:07:43.423 NotebookApp] http://tf-notebook:8888/?token=fc5d8b9d6f29d5ddad62e8c731f83fc8e90a2d817588d772</span>
<span class="go">[I 19:07:43.423 NotebookApp]  or http://127.0.0.1:8888/?token=fc5d8b9d6f29d5ddad62e8c731f83fc8e90a2d817588d772</span>
<span class="go">[I 19:07:43.423 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).</span>
<span class="go">[C 19:07:43.429 NotebookApp]</span>

<span class="go">   To access the notebook, open this file in a browser:</span>
<span class="go">      file:///root/.local/share/jupyter/runtime/nbserver-1-open.html</span>
<span class="go">   Or copy and paste one of these URLs:</span>
<span class="go">      http://tf-notebook:8888/?token=fc5d8b9d6f29d5ddad62e8c731f83fc8e90a2d817588d772</span>
<span class="go">   or http://127.0.0.1:8888/?token=fc5d8b9d6f29d5ddad62e8c731f83fc8e90a2d817588d772</span>
<span class="go">[I 19:08:24.180 NotebookApp] 302 GET / (172.16.20.30) 0.61ms</span>
<span class="go">[I 19:08:24.182 NotebookApp] 302 GET /tree? (172.16.20.30) 0.57ms</span>
</pre></div>
</div>
</li>
<li><p>From a web browser, navigate to <code class="docutils literal notranslate"><span class="pre">http://&lt;LOADBALANCERIP&gt;</span></code> and enter the token where prompted to login:
Depending on your environment you may not have web browser access to the exposed service. You may be able to use
<a class="reference external" href="https://www.ssh.com/ssh/tunneling/example">SSH Port Forwarding/Tunneling</a> to achieve this.</p>
<a class="reference internal image-reference" href="../_images/image14.png"><img alt="../_images/image14.png" src="../_images/image14.png" style="width: 800px;" /></a>
</li>
<li><p>Once logged in, navigate click on the tenserflow-tutorials folder and then on the first file, <strong>classification.ipynb</strong>:</p>
<a class="reference internal image-reference" href="../_images/image15.png"><img alt="../_images/image15.png" src="../_images/image15.png" style="width: 800px;" /></a>
</li>
<li><p>This will launch a new tab with the Notebook loaded. You can now run through the Notebook by clicking on the <strong>Run</strong>
button. The notebook will step through each section and execute the code as you go. Continue pressing <strong>Run</strong> until you
reach the end of the notebook and observe the execution of the classification program.</p>
<a class="reference internal image-reference" href="../_images/image16.png"><img alt="../_images/image16.png" src="../_images/image16.png" style="width: 800px;" /></a>
</li>
<li><p>Once the notebook is complete you can check the logs of the <code class="docutils literal notranslate"><span class="pre">tf-notebook</span></code> pod to confirm it was using the GPU:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">=========snip===============</span>
<span class="go">[I 19:17:58.116 NotebookApp] Saving file at /tensorflow-tutorials/classification.ipynb</span>
<span class="go">2020-05-21 19:21:01.422482: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1</span>
<span class="go">2020-05-21 19:21:01.436767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span>
<span class="go">2020-05-21 19:21:01.437469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:</span>
<span class="go">pciBusID: 0000:13:00.0 name: Tesla P4 computeCapability: 6.1</span>
<span class="go">coreClock: 1.1135GHz coreCount: 20 deviceMemorySize: 7.43GiB deviceMemoryBandwidth: 178.99GiB/s</span>
<span class="go">2020-05-21 19:21:01.438477: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1</span>
<span class="go">2020-05-21 19:21:01.462370: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10</span>
<span class="go">2020-05-21 19:21:01.475269: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10</span>
<span class="go">2020-05-21 19:21:01.478104: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10</span>
<span class="go">2020-05-21 19:21:01.501057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10</span>
<span class="go">2020-05-21 19:21:01.503901: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10</span>
<span class="go">2020-05-21 19:21:01.544763: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7</span>
<span class="go">2020-05-21 19:21:01.545022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span>
<span class="go">2020-05-21 19:21:01.545746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span>
<span class="go">2020-05-21 19:21:01.546356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0</span>
<span class="go">2020-05-21 19:21:01.546705: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA</span>
<span class="go">2020-05-21 19:21:01.558283: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2194840000 Hz</span>
<span class="go">2020-05-21 19:21:01.558919: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f6f2c000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:</span>
<span class="go">2020-05-21 19:21:01.558982: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version</span>
<span class="go">2020-05-21 19:21:01.645786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span>
<span class="go">2020-05-21 19:21:01.646387: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x53ab350 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:</span>
<span class="go">2020-05-21 19:21:01.646430: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P4, Compute Capability 6.1</span>
<span class="go">2020-05-21 19:21:01.647005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span>
<span class="go">2020-05-21 19:21:01.647444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:</span>
<span class="go">pciBusID: 0000:13:00.0 name: Tesla P4 computeCapability: 6.1</span>
<span class="go">coreClock: 1.1135GHz coreCount: 20 deviceMemorySize: 7.43GiB deviceMemoryBandwidth: 178.99GiB/s</span>
<span class="go">2020-05-21 19:21:01.647523: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1</span>
<span class="go">2020-05-21 19:21:01.647570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10</span>
<span class="go">2020-05-21 19:21:01.647611: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10</span>
<span class="go">2020-05-21 19:21:01.647647: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10</span>
<span class="go">2020-05-21 19:21:01.647683: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10</span>
<span class="go">2020-05-21 19:21:01.647722: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10</span>
<span class="go">2020-05-21 19:21:01.647758: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7</span>
<span class="go">2020-05-21 19:21:01.647847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span>
<span class="go">2020-05-21 19:21:01.648311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span>
<span class="go">2020-05-21 19:21:01.648720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0</span>
<span class="go">2020-05-21 19:21:01.649158: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1</span>
<span class="go">2020-05-21 19:21:01.650302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:</span>
<span class="go">2020-05-21 19:21:01.650362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0</span>
<span class="go">2020-05-21 19:21:01.650392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N</span>
<span class="go">2020-05-21 19:21:01.650860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span>
<span class="go">2020-05-21 19:21:01.651341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span>
<span class="go">2020-05-21 19:21:01.651773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7048 MB memory) -&gt; physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:13:00.0, compute capability: 6.1)</span>
<span class="go">2020-05-21 19:21:03.601093: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10</span>
<span class="go">[I 19:21:58.132 NotebookApp] Saving file at /tensorflow-tutorials/classification.ipynb</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>
<section id="uninstall-and-cleanup">
<h3>Uninstall and Cleanup<a class="headerlink" href="#uninstall-and-cleanup" title="Permalink to this heading"></a></h3>
<p>You can remove the <code class="docutils literal notranslate"><span class="pre">tf-notebook</span></code> and service with the following commands:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>delete<span class="w"> </span>pod<span class="w"> </span>tf-notebook
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>delete<span class="w"> </span>svc<span class="w"> </span>tf-notebook
</pre></div>
</div>
<p>You can remove the GPU operator with the command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>uninstall<span class="w"> </span><span class="k">$(</span>helm<span class="w"> </span>list<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>gpu-operator<span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;{print $1}&#39;</span><span class="k">)</span>
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">release &quot;gpu-operator-1590086955&quot; uninstalled</span>
</pre></div>
</div>
<p>You can now stop the VM, remove the PCI device, remove the memory reservation, and restart the VM.</p>
<p>You do not need to remove the PCI passthrough device from the host.</p>
</section>
<section id="known-issues">
<h3>Known Issues<a class="headerlink" href="#known-issues" title="Permalink to this heading"></a></h3>
<p>This section outlines some known issues with using Google Cloud’s Anthos with NVIDIA GPUs.</p>
<ol class="arabic">
<li><p>Attaching a GPU to a Anthos on-prem worker node requires manually editing the VM from vSphere.
These changes will not survive an Anthos on-prem upgrade process. When the node with the GPU is
deleted as part of the update process, the new VM replacing it will not have the GPU added.
The GPU must be added back to a new VM manually again. While the NVIDIA GPU seems to be able to
handle that event gracefully, the workload backed by the GPU may need to be initiated again manually.</p></li>
<li><p>Attaching a VM to the GPU means that the VM can no longer be migrated to another ESXi host. The VM
will essentially be pinned to the ESXi host which hosts the GPU. vMotion and VMware HA features cannot be used.</p></li>
<li><p>VMs that use a PCI Passthrough device require that their full memory allocation be locked. This will cause a
<strong>Virtual machine memory usage</strong> alarm on the VM which can safely be ignored.</p>
<a class="reference internal image-reference" href="../_images/image17.png"><img alt="../_images/image17.png" src="../_images/image17.png" style="width: 800px;" /></a>
</li>
</ol>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mig-k8s.html" class="btn btn-neutral float-left" title="MIG Support in Kubernetes" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../gpu-telemetry/dcgm-exporter.html" class="btn btn-neutral float-right" title="DCGM-Exporter" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018-2023, NVIDIA Corporation.
      <span class="lastupdated">Last updated on 2023-04-02.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>  

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  .rst-content dl:not(.docutils) dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }
  </style>
  

</body>
</html>