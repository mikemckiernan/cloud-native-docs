<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>GPUDirect RDMA and GPUDirect Storage &mdash; NVIDIA Cloud Native Technologies  documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../../../_static/nvidia.ico"/>
    <link rel="canonical" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/archive/22.9.2/gpu-operator-rdma.html"/>
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/sphinx_highlight.js"></script>
        <script src="../../../_static/clipboard.min.js"></script>
        <script src="../../../_static/copybutton.js"></script>
        <script src="../../../_static/tabs.js"></script>
        <script src="../../../_static/js/google-analytics/google-analytics-tracker.js"></script>
        <script src="../../../_static/js/google-analytics/google-analytics-write.js"></script>
        <script src="//assets.adobedtm.com/b92787824f2e0e9b68dc2e993f9bd995339fe417/satelliteLib-7ba51e58dc61bcb0e9311aadd02a0108ab24cc6c.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="GPU Operator with KubeVirt" href="gpu-operator-kubevirt.html" />
    <link rel="prev" title="Time-Slicing GPUs in Kubernetes" href="gpu-sharing.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" > 
            <a href="../../../contents.html">
            <img src="../../../_static/NVLogo_H_B&W.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }

    .wy-nav-content {
      max-width: 1000px;
    }
  </style>
  
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">NVIDIA Container Toolkit:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/concepts.html">Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/arch-overview.html">Architecture Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/install-guide.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/user-guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../container-toolkit/archive.html">Archive</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">NVIDIA GPU Operator:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../platform-support.html">Platform Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gpu-driver-upgrades.html">GPU Driver Upgrades</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install-gpu-operator-vgpu.html">NVIDIA vGPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install-gpu-operator-nvaie.html">NVIDIA AI Enterprise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../openshift/contents.html">GPU Operator on OpenShift</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gpu-operator-mig.html">GPU Operator with MIG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gpu-sharing.html">Time-Slicing GPUs in Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gpu-operator-rdma.html">GPUDirect RDMA and GPUDirect Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gpu-operator-kubevirt.html">GPU Operator with KubeVirt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../appendix.html">Advanced Configurations</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../archive.html">Archive</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../archive.html#id1">22.9.2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id2">22.9.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id3">22.9.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id4">1.11.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id5">1.11.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id6">1.10.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id7">1.9.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id8">1.9.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../archive.html#id9">1.8</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Kubernetes with GPUs:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../kubernetes/install-k8s.html">Install Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../kubernetes/mig-k8s.html">MIG Support in Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../kubernetes/anthos-guide.html">NVIDIA GPUs with Google Cloud’s Anthos</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GPU Telemetry:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../gpu-telemetry/dcgm-exporter.html">DCGM-Exporter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../gpu-telemetry/dcgm-exporter.html#integrating-gpu-telemetry-into-kubernetes">Integrating GPU Telemetry into Kubernetes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multi-Instance GPU:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../mig/mig.html">Multi-Instance GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mig/mig-k8s.html">MIG Support in Kubernetes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Driver Containers:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../driver-containers/overview.html">Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Playground:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../playground/dind.html">Docker-in-Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../playground/x-arch.html">Running Cross-Architecture Containers</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../contents.html">NVIDIA Cloud Native Technologies</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../contents.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../archive.html">Archive</a> &raquo;</li>
      <li>GPUDirect RDMA and GPUDirect Storage</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="gpudirect-rdma-and-gpudirect-storage">
<span id="operator-rdma-22-9-2"></span><h1>GPUDirect RDMA and GPUDirect Storage<a class="headerlink" href="#gpudirect-rdma-and-gpudirect-storage" title="Permalink to this heading"></a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#about-gpudirect-rdma-and-gpudirect-storage" id="id6">About GPUDirect RDMA and GPUDirect Storage</a></p></li>
<li><p><a class="reference internal" href="#configuring-gpudirect-rdma-using-nvidia-peermem" id="id7">Configuring GPUDirect RDMA Using nvidia-peermem</a></p>
<ul>
<li><p><a class="reference internal" href="#platform-support" id="id8">Platform Support</a></p></li>
<li><p><a class="reference internal" href="#prerequisites" id="id9">Prerequisites</a></p></li>
<li><p><a class="reference internal" href="#installing-the-gpu-operator-and-enabling-gpudirect-rdma" id="id10">Installing the GPU Operator and Enabling GPUDirect RDMA</a></p></li>
<li><p><a class="reference internal" href="#verifying-the-installation-of-gpudirect-with-rdma" id="id11">Verifying the Installation of GPUDirect with RDMA</a></p></li>
<li><p><a class="reference internal" href="#verifying-the-installation-by-performing-a-data-transfer" id="id12">Verifying the Installation by Performing a Data Transfer</a></p></li>
<li><p><a class="reference internal" href="#related-information" id="id13">Related Information</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#using-gpudirect-storage" id="id14">Using GPUDirect Storage</a></p>
<ul>
<li><p><a class="reference internal" href="#id2" id="id15">Platform Support</a></p></li>
<li><p><a class="reference internal" href="#id3" id="id16">Prerequisites</a></p></li>
<li><p><a class="reference internal" href="#installation" id="id17">Installation</a></p></li>
<li><p><a class="reference internal" href="#verification" id="id18">Verification</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#further-reading" id="id19">Further Reading</a></p></li>
</ul>
</div>
<section id="about-gpudirect-rdma-and-gpudirect-storage">
<h2>About GPUDirect RDMA and GPUDirect Storage<a class="headerlink" href="#about-gpudirect-rdma-and-gpudirect-storage" title="Permalink to this heading"></a></h2>
<p><a class="reference external" href="https://docs.nvidia.com/cuda/gpudirect-rdma/index.html">GPUDirect RDMA</a> is a technology in NVIDIA GPUs that enables direct
data exchange between GPUs and a third-party peer device using PCI Express. The third-party devices could be network interfaces
such as NVIDIA ConnectX SmartNICs or BlueField DPUs, or video acquisition adapters.</p>
<p>GPUDirect Storage (<a class="reference external" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html">GDS</a>) enables a direct data path between local or remote storage, like NFS server or NVMe/NVMe over Fabric (NVMe-oF), and GPU memory.
GDS leverages direct memory access (DMA) transfers between GPU memory and storage, which avoids a bounce buffer through the CPU. This direct path increases system bandwidth and decreases the latency and utilization load on the CPU.</p>
<p>To support GPUDirect RDMA, a userspace CUDA APIs and kernel mode drivers are required. Starting with
<a class="reference external" href="https://docs.nvidia.com/cuda/gpudirect-rdma/index.html#new-in-cuda-114">CUDA 11.4 and R470 drivers</a>, a
new kernel module <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> is included in the standard NVIDIA driver installers (e.g. <code class="docutils literal notranslate"><span class="pre">.run</span></code>). The
kernel module provides Mellanox Infiniband-based HCAs direct peer-to-peer read and write access to the GPU’s memory.</p>
<p>In conjunction with the <a class="reference external" href="https://github.com/Mellanox/network-operator">Network Operator</a>, the GPU Operator can be used to
set up the networking related components such as Mellanox drivers, <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> and Kubernetes device plugins to enable
workloads to take advantage of GPUDirect RDMA and GPUDirect Storage. Refer to the Network Operator <a class="reference external" href="https://docs.nvidia.com/networking/display/COKAN10">documentation</a>
on installing the Network Operator.</p>
</section>
<section id="configuring-gpudirect-rdma-using-nvidia-peermem">
<h2>Configuring GPUDirect RDMA Using nvidia-peermem<a class="headerlink" href="#configuring-gpudirect-rdma-using-nvidia-peermem" title="Permalink to this heading"></a></h2>
<section id="platform-support">
<h3>Platform Support<a class="headerlink" href="#platform-support" title="Permalink to this heading"></a></h3>
<p>The following platforms are supported for GPUDirect with RDMA:</p>
<ul class="simple">
<li><p>Kubernetes on bare metal and on vSphere VMs with GPU passthrough and vGPU.</p></li>
<li><p>VMware vSphere with Tanzu.</p></li>
<li><p>For Red Hat Openshift on bare metal and on vSphere VMs with GPU passthrough and vGPU configurations,
see the <a class="reference internal" href="openshift/nvaie-with-ocp.html#nvaie-ocp-22-9-2-22-9-2"><span class="std std-ref">NVIDIA AI Enterprise with OpenShift</span></a> information.</p></li>
</ul>
<p>For information about the supported versions, see <a class="reference internal" href="../../platform-support.html#support-for-gpudirect-rdma"><span class="std std-ref">Support for GPUDirect RDMA</span></a> on the platform support page.</p>
</section>
<section id="prerequisites">
<h3>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this heading"></a></h3>
<ul>
<li><p>Make sure that <a class="reference external" href="https://github.com/Mellanox/ofed-docker">MOFED</a> drivers are installed either through <a class="reference external" href="https://github.com/Mellanox/network-operator">Network Operator</a> or directly on the host.</p></li>
<li><p>For installations on VMWare vSphere, refer to the following additional prerequisites:</p>
<ul>
<li><p>Make sure the Mellanox network interface controller and the NVIDIA GPU are in the same PCIe IO root complex.</p></li>
<li><p>Enable the following PCI options:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">pciPassthru.allowP2P</span> <span class="pre">=</span> <span class="pre">true</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pciPassthru.RelaxACSforP2P</span> <span class="pre">=</span> <span class="pre">true</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pciPassthru.use64bitMMIO</span> <span class="pre">=</span> <span class="pre">true</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pciPassthru.64bitMMIOSizeGB</span> <span class="pre">=</span> <span class="pre">128</span></code></p></li>
</ul>
<p>For information about configuring the settings, refer to the
<a class="reference external" href="https://core.vmware.com/resource/deploy-ai-ready-vsphere-7#vm-settings-A">Deploy an AI-Ready Enterprise Platform on vSphere 7</a>
document from VMWare.</p>
</li>
</ul>
</li>
</ul>
</section>
<section id="installing-the-gpu-operator-and-enabling-gpudirect-rdma">
<h3>Installing the GPU Operator and Enabling GPUDirect RDMA<a class="headerlink" href="#installing-the-gpu-operator-and-enabling-gpudirect-rdma" title="Permalink to this heading"></a></h3>
<p>If the MOFED drivers were installed with the Network Operator, run the following command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--create-namespace<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>nvidia/gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>driver.rdma.enabled<span class="o">=</span><span class="nb">true</span>
</pre></div>
</div>
<p>If the MOFED drivers were installed directly on host, run the following command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--create-namespace<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>nvidia/gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>driver.rdma.enabled<span class="o">=</span><span class="nb">true</span><span class="w"> </span>--set<span class="w"> </span>driver.rdma.useHostMofed<span class="o">=</span><span class="nb">true</span>
</pre></div>
</div>
</section>
<section id="verifying-the-installation-of-gpudirect-with-rdma">
<h3>Verifying the Installation of GPUDirect with RDMA<a class="headerlink" href="#verifying-the-installation-of-gpudirect-with-rdma" title="Permalink to this heading"></a></h3>
<p>During the installation, the NVIDIA driver daemonset runs an <cite>init container</cite> to wait on the Mellanox OFED (MOFED) drivers to be ready.
This init container checks for Mellanox NICs on the node and ensures that the necessary kernel symbols are exported MOFED kernel drivers.
After the verfication is complete by the init container, the nvidia-peermem-ctr container is started inside each driver pod.</p>
<ol class="arabic">
<li><p>Confirm that the pod template for the driver daemonset includes the mofed-validation init container and
the nvidia-driver-ctr and nvidia-peermem-ctr containers:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>describe<span class="w"> </span>ds<span class="w"> </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>nvidia-driver-daemonset
</pre></div>
</div>
<p><em>Example Output</em></p>
<p>The following partial output omits the init containers and containers that are common to all installations.</p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">...</span>
<span class="go"> Init Containers:</span>
<span class="go">  mofed-validation:</span>
<span class="go">  Container ID:  containerd://5a36c66b43f676df616e25ba7ae0c81aeaa517308f28ec44e474b2f699218de3</span>
<span class="go">  Image:         nvcr.io/nvidia/cloud-native/gpu-operator-validator:v1.8.1</span>
<span class="go">  Image ID:      nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:7a70e95fd19c3425cd4394f4b47bbf2119a70bd22d67d72e485b4d730853262c</span>
<span class="go">...</span>
<span class="go"> Containers:</span>
<span class="go">  nvidia-driver-ctr:</span>
<span class="go">  Container ID:  containerd://199a760946c55c3d7254fa0ebe6a6557dd231179057d4909e26c0e6aec49ab0f</span>
<span class="go">  Image:         nvcr.io/nvaie/vgpu-guest-driver:470.63.01-ubuntu20.04</span>
<span class="go">  Image ID:      nvcr.io/nvaie/vgpu-guest-driver@sha256:a1b7d2c8e1bad9bb72d257ddfc5cec341e790901e7574ba2c32acaddaaa94625</span>
<span class="go">...</span>
<span class="go">  nvidia-peermem-ctr:</span>
<span class="go">  Container ID:  containerd://0742d86f6017bf0c304b549ebd8caad58084a4185a1225b2c9a7f5c4a171054d</span>
<span class="go">  Image:         nvcr.io/nvaie/vgpu-guest-driver:470.63.01-ubuntu20.04</span>
<span class="go">  Image ID:      nvcr.io/nvaie/vgpu-guest-driver@sha256:a1b7d2c8e1bad9bb72d257ddfc5cec341e790901e7574ba2c32acaddaaa94625</span>
<span class="go">...</span>
</pre></div>
</div>
</li>
<li><p>Confirm that the nvidia-peermem-ctr container successfully loaded the nvidia-peermem kernel module:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>logs<span class="w"> </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>ds/nvidia-driver-daemonset<span class="w"> </span>-c<span class="w"> </span>nvidia-peermem-ctr
</pre></div>
</div>
<p>Alternatively, run <code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">logs</span> <span class="pre">-n</span> <span class="pre">gpu-operator</span> <span class="pre">nvidia-driver-daemonset-xxxxx</span> <span class="pre">-c</span> <span class="pre">nvidia-peermem-ctr</span></code> for each pod in the daemonset.</p>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">waiting for mellanox ofed and nvidia drivers to be installed</span>
<span class="go">waiting for mellanox ofed and nvidia drivers to be installed</span>
<span class="go">successfully loaded nvidia-peermem module</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="verifying-the-installation-by-performing-a-data-transfer">
<h3>Verifying the Installation by Performing a Data Transfer<a class="headerlink" href="#verifying-the-installation-by-performing-a-data-transfer" title="Permalink to this heading"></a></h3>
<p>You can perform the following steps to verify that GPUDirect with RDMA is configured
correctly and that pods can perform RDMA data transfers.</p>
<ol class="arabic">
<li><p>Get the network interface name of the Infiniband device on the host:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span>-n<span class="w"> </span>network-operator<span class="w"> </span>mofed-ubuntu22.04-ds-xxxxx<span class="w"> </span>--<span class="w"> </span>ibdev2netdev
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">mlx5_0 port 1 ==&gt; ens64np1 (Up)</span>
</pre></div>
</div>
</li>
<li><p>Configure a secondary network on the device using MACVLAN:</p>
<ul>
<li><p>Create a file, such as <code class="docutils literal notranslate"><span class="pre">demo-macvlannetwork.yaml</span></code>, with contents like the following example:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mellanox.com/v1alpha1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">MacvlanNetwork</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">demo-macvlannetwork</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">networkNamespace</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;default&quot;</span>
<span class="hll"><span class="nt">master</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;ens64np1&quot;</span>
</span><span class="nt">mode</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;bridge&quot;</span>
<span class="nt">mtu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1500</span>
<span class="nt">ipam</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">  </span><span class="no">{</span>
<span class="w">    </span><span class="no">&quot;type&quot;: &quot;whereabouts&quot;,</span>
<span class="w">    </span><span class="no">&quot;range&quot;: &quot;192.168.2.225/28&quot;,</span>
<span class="w">    </span><span class="no">&quot;exclude&quot;: [</span>
<span class="w">      </span><span class="no">&quot;192.168.2.229/30&quot;,</span>
<span class="w">      </span><span class="no">&quot;192.168.2.236/32&quot;</span>
<span class="w">    </span><span class="no">]</span>
<span class="w">  </span><span class="no">}</span>
</pre></div>
</div>
<p>Replace <code class="docutils literal notranslate"><span class="pre">ens64np1</span></code> with the the network interface name reported by the <code class="docutils literal notranslate"><span class="pre">ibdev2netdev</span></code> command
from the preceding step.</p>
</li>
<li><p>Apply the manifest:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>demo-macvlannetwork.yaml
</pre></div>
</div>
</li>
<li><p>Confirm that the additional network is ready:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>macvlannetworks<span class="w"> </span>demo-macvlannetwork
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME                  STATUS   AGE</span>
<span class="go">demo-macvlannetwork   ready    2023-03-10T18:22:28Z</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Start two pods that run the <code class="docutils literal notranslate"><span class="pre">mellanox/cuda-perftest</span></code> container on two different nodes in the cluster.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-0-0-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-0-0-0" name="0-0" role="tab" tabindex="0">demo-pod-1</button><button aria-controls="panel-0-0-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-1" name="0-1" role="tab" tabindex="-1">demo-pod-2</button></div><div aria-labelledby="tab-0-0-0" class="sphinx-tabs-panel" id="panel-0-0-0" name="0-0" role="tabpanel" tabindex="0"><ul>
<li><p>Create a file, such as <code class="docutils literal notranslate"><span class="pre">demo-pod-1.yaml</span></code>, for the first pod with contents like the following:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="hll"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">demo-pod-1</span>
</span><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span>
<span class="w">    </span><span class="nt">k8s.v1.cni.cncf.io/networks</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">demo-macvlannetwork</span>
<span class="w">    </span><span class="c1"># If a network with static IPAM is used replace network annotation with the below.</span>
<span class="w">    </span><span class="c1"># k8s.v1.cni.cncf.io/networks: &#39;[</span>
<span class="w">    </span><span class="c1">#   { &quot;name&quot;: &quot;rdma-net&quot;,</span>
<span class="w">    </span><span class="c1">#     &quot;ips&quot;: [&quot;192.168.111.101/24&quot;],</span>
<span class="w">    </span><span class="c1">#     &quot;gateway&quot;: [&quot;192.168.111.1&quot;]</span>
<span class="w">    </span><span class="c1">#   }</span>
<span class="w">    </span><span class="c1"># ]&#39;</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">nodeSelector</span><span class="p">:</span>
<span class="w">    </span><span class="c1"># Note: Replace hostname or remove selector altogether</span>
<span class="hll"><span class="w">    </span><span class="nt">kubernetes.io/hostname</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvnode1</span>
</span><span class="w">  </span><span class="nt">restartPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">OnFailure</span>
<span class="w">  </span><span class="nt">containers</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mellanox/cuda-perftest</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">rdma-gpu-test-ctr</span>
<span class="w">    </span><span class="nt">securityContext</span><span class="p">:</span>
<span class="w">      </span><span class="nt">capabilities</span><span class="p">:</span>
<span class="w">        </span><span class="nt">add</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="w"> </span><span class="s">&quot;IPC_LOCK&quot;</span><span class="w"> </span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="nt">resources</span><span class="p">:</span>
<span class="w">      </span><span class="nt">limits</span><span class="p">:</span>
<span class="w">        </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">        </span><span class="nt">rdma/rdma_shared_device_a</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">      </span><span class="nt">requests</span><span class="p">:</span>
<span class="w">        </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">        </span><span class="nt">rdma/rdma_shared_device_a</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
</pre></div>
</div>
</li>
<li><p>Apply the manifest:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>demo-pod-1.yaml
</pre></div>
</div>
</li>
</ul>
</div><div aria-labelledby="tab-0-0-1" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-1" name="0-1" role="tabpanel" tabindex="0"><ul>
<li><p>Create a file, such as <code class="docutils literal notranslate"><span class="pre">demo-pod-2.yaml</span></code>, for the second pod with contents like the following:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="hll"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">demo-pod-2</span>
</span><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span>
<span class="w">    </span><span class="nt">k8s.v1.cni.cncf.io/networks</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">demo-macvlannetwork</span>
<span class="w">    </span><span class="c1"># If a network with static IPAM is used replace network annotation with the below.</span>
<span class="w">    </span><span class="c1"># k8s.v1.cni.cncf.io/networks: &#39;[</span>
<span class="w">    </span><span class="c1">#   { &quot;name&quot;: &quot;rdma-net&quot;,</span>
<span class="w">    </span><span class="c1">#     &quot;ips&quot;: [&quot;192.168.111.101/24&quot;],</span>
<span class="w">    </span><span class="c1">#     &quot;gateway&quot;: [&quot;192.168.111.1&quot;]</span>
<span class="w">    </span><span class="c1">#   }</span>
<span class="w">    </span><span class="c1"># ]&#39;</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">nodeSelector</span><span class="p">:</span>
<span class="w">    </span><span class="c1"># Note: Replace hostname or remove selector altogether</span>
<span class="hll"><span class="w">    </span><span class="nt">kubernetes.io/hostname</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvnode2</span>
</span><span class="w">  </span><span class="nt">restartPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">OnFailure</span>
<span class="w">  </span><span class="nt">containers</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mellanox/cuda-perftest</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">rdma-gpu-test-ctr</span>
<span class="w">    </span><span class="nt">securityContext</span><span class="p">:</span>
<span class="w">      </span><span class="nt">capabilities</span><span class="p">:</span>
<span class="w">        </span><span class="nt">add</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="w"> </span><span class="s">&quot;IPC_LOCK&quot;</span><span class="w"> </span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="nt">resources</span><span class="p">:</span>
<span class="w">      </span><span class="nt">limits</span><span class="p">:</span>
<span class="w">        </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">        </span><span class="nt">rdma/rdma_shared_device_a</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">      </span><span class="nt">requests</span><span class="p">:</span>
<span class="w">        </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">        </span><span class="nt">rdma/rdma_shared_device_a</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
</pre></div>
</div>
</li>
<li><p>Apply the manifest:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>demo-pod-2.yaml
</pre></div>
</div>
</li>
</ul>
</div></div>
</li>
<li><p>Get the IP addresses of the pods:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-o<span class="w"> </span>wide
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME         READY   STATUS    RESTARTS   AGE    IP              NODE      NOMINATED NODE   READINESS GATES</span>
<span class="go">demo-pod-1   1/1     Running   0          3d4h   192.168.38.90   nvnode1   &lt;none&gt;           &lt;none&gt;</span>
<span class="go">demo-pod-2   1/1     Running   0          3d4h   192.168.47.89   nvnode2   &lt;none&gt;           &lt;none&gt;</span>
</pre></div>
</div>
</li>
<li><p>From one terminal, open a shell in the container on the first pod and start the performance test server:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span>demo-pod-1<span class="w"> </span>--<span class="w"> </span>ib_write_bw<span class="w"> </span>-d<span class="w"> </span>mlx5_0<span class="w"> </span>-a<span class="w"> </span>-F<span class="w"> </span>--report_gbits<span class="w"> </span>-q<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">************************************</span>
<span class="go">* Waiting for client to connect... *</span>
<span class="go">************************************</span>
</pre></div>
</div>
</li>
<li><p>From another terminal, open a shell in the container on the second pod and run the performance client:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span>demo-pod-2<span class="w"> </span>--<span class="w"> </span>ib_write_bw<span class="w"> </span>-d<span class="w"> </span>mlx5_0<span class="w"> </span>-a<span class="w"> </span>-F<span class="w"> </span>--report_gbits<span class="w"> </span>-q<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">192</span>.168.38.90
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go"> ---------------------------------------------------------------------------------------</span>
<span class="go">                    RDMA_Write BW Test</span>
<span class="go"> Dual-port       : OFF          Device         : mlx5_0</span>
<span class="go"> Number of qps   : 1            Transport type : IB</span>
<span class="go"> Connection type : RC           Using SRQ      : OFF</span>
<span class="go"> PCIe relax order: ON</span>
<span class="go"> ibv_wr* API     : ON</span>
<span class="go"> TX depth        : 128</span>
<span class="go"> CQ Moderation   : 100</span>
<span class="go"> Mtu             : 1024[B]</span>
<span class="go"> Link type       : Ethernet</span>
<span class="go"> GID index       : 5</span>
<span class="go"> Max inline data : 0[B]</span>
<span class="go"> rdma_cm QPs     : OFF</span>
<span class="go"> Data ex. method : Ethernet</span>
<span class="go">---------------------------------------------------------------------------------------</span>
<span class="go"> local address: LID 0000 QPN 0x01ac PSN 0xc76db1 RKey 0x23beb2 VAddr 0x007f26a2c8b000</span>
<span class="go"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:226</span>
<span class="go"> remote address: LID 0000 QPN 0x01a9 PSN 0x2f722 RKey 0x23beaf VAddr 0x007f820b24f000</span>
<span class="go"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:225</span>
<span class="go">---------------------------------------------------------------------------------------</span>
<span class="go"> #bytes     #iterations    BW peak[Gb/sec]    BW average[Gb/sec]   MsgRate[Mpps]</span>
<span class="go"> 2          5000             0.11               0.11               6.897101</span>
<span class="go"> 4          5000             0.22               0.22               6.995646</span>
<span class="go"> 8          5000             0.45               0.45               7.014752</span>
<span class="go"> 16         5000             0.90               0.90               7.017509</span>
<span class="go"> 32         5000             1.80               1.80               7.020162</span>
<span class="go"> 64         5000             3.59               3.59               7.007110</span>
<span class="go"> 128        5000             7.19               7.18               7.009540</span>
<span class="go"> 256        5000             15.06              14.98              7.313517</span>
<span class="go"> 512        5000             30.04              29.73              7.259329</span>
<span class="go"> 1024       5000             59.65              58.81              7.178529</span>
<span class="go"> 2048       5000             91.53              91.47              5.582931</span>
<span class="go"> 4096       5000             92.13              92.06              2.809574</span>
<span class="go"> 8192       5000             92.35              92.31              1.408535</span>
<span class="go"> 16384      5000             92.46              92.46              0.705381</span>
<span class="go"> 32768      5000             92.36              92.35              0.352302</span>
<span class="go"> 65536      5000             92.39              92.38              0.176196</span>
<span class="go"> 131072     5000             92.42              92.41              0.088131</span>
<span class="go"> 262144     5000             92.45              92.44              0.044080</span>
<span class="go"> 524288     5000             92.42              92.42              0.022034</span>
<span class="go"> 1048576    5000             92.40              92.40              0.011015</span>
<span class="go"> 2097152    5000             92.40              92.39              0.005507</span>
<span class="go"> 4194304    5000             92.40              92.39              0.002753</span>
<span class="go"> 8388608    5000             92.39              92.39              0.001377</span>
<span class="go">---------------------------------------------------------------------------------------</span>
</pre></div>
</div>
<p>The command output indicates that the data transfer rate was approximately 92 Gbps.</p>
</li>
<li><p>Delete the pods:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>delete<span class="w"> </span>-f<span class="w"> </span>demo-pod-1.yaml<span class="w"> </span>-f<span class="w"> </span>demo-pod-2.yaml
</pre></div>
</div>
</li>
<li><p>Delete the secondary network:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>delete<span class="w"> </span>-f<span class="w"> </span>demo-macvlannetworks.yaml
</pre></div>
</div>
</li>
</ol>
</section>
<section id="related-information">
<h3>Related Information<a class="headerlink" href="#related-information" title="Permalink to this heading"></a></h3>
<p>For more information about nvidia-peermem, refer to
<a class="reference external" href="https://docs.nvidia.com/cuda/gpudirect-rdma/index.html#using-nvidia-peermem">Using nvidia-peermem</a>
in the NVIDIA CUDA documentation.</p>
</section>
</section>
<section id="using-gpudirect-storage">
<h2>Using GPUDirect Storage<a class="headerlink" href="#using-gpudirect-storage" title="Permalink to this heading"></a></h2>
<section id="id2">
<h3>Platform Support<a class="headerlink" href="#id2" title="Permalink to this heading"></a></h3>
<p>See <a class="reference internal" href="../../platform-support.html#support-for-gpudirect-storage"><span class="std std-ref">Support for GPUDirect Storage</span></a> on the platform support page.</p>
</section>
<section id="id3">
<h3>Prerequisites<a class="headerlink" href="#id3" title="Permalink to this heading"></a></h3>
<p>Make sure that <a class="reference external" href="https://github.com/Mellanox/ofed-docker">MOFED</a> drivers are installed through <a class="reference external" href="https://github.com/Mellanox/network-operator">Network Operator</a>.</p>
</section>
<section id="installation">
<h3>Installation<a class="headerlink" href="#installation" title="Permalink to this heading"></a></h3>
<p>The following section is applicable to the following configurations and describe how to deploy the GPU Operator using the Helm Chart:</p>
<ul class="simple">
<li><p>Kubernetes on bare metal and on vSphere VMs with GPU passthrough and vGPU.</p></li>
</ul>
<p>Starting with v22.9.1, the GPU Operator provides an option to load the <code class="docutils literal notranslate"><span class="pre">nvidia-fs</span></code> kernel module during the bootstrap of the NVIDIA driver daemonset.
Please refer to below install commands based on Mellanox OFED (MOFED) drivers are installed through Network-Operator.</p>
<p>MOFED drivers installed with Network-Operator:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm<span class="w"> </span>install<span class="w"> </span>--wait<span class="w"> </span>--generate-name<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>-n<span class="w"> </span>gpu-operator<span class="w"> </span>--create-namespace<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>nvidia/gpu-operator<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--set<span class="w"> </span>driver.rdma.enabled<span class="o">=</span><span class="nb">true</span>
<span class="go">     --set gds.enabled=true</span>
</pre></div>
</div>
<p>For detailed information on how to deploy Network Operator and GPU Operator for GPU Direct Storage, please use this <a class="reference external" href="https://docs.nvidia.com/ai-enterprise/deployment-guide-bare-metal/0.1.0/gds-overview.html">link</a>.</p>
</section>
<section id="verification">
<h3>Verification<a class="headerlink" href="#verification" title="Permalink to this heading"></a></h3>
<p>During the installation, an <cite>initContainer</cite> is used with the driver daemonset to wait on the Mellanox OFED (MOFED) drivers to be ready.
This initContainer checks for Mellanox NICs on the node and ensures that the necessary kernel symbols are exported MOFED kernel drivers.
Once everything is in place, the containers nvidia-peermem-ctr and nvidia-fs-ctr will be instantiated inside the driver daemonset.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>get<span class="w"> </span>pod<span class="w"> </span>-n<span class="w"> </span>gpu-operator

<span class="go">gpu-operator   gpu-feature-discovery-pktzg                                       1/1     Running     0          11m</span>
<span class="go">gpu-operator   gpu-operator-1672257888-node-feature-discovery-master-7ccb7txmc   1/1     Running     0          12m</span>
<span class="go">gpu-operator   gpu-operator-1672257888-node-feature-discovery-worker-bqhrl       1/1     Running     0          11m</span>
<span class="go">gpu-operator   gpu-operator-6f64c86bc-zjqdh                                      1/1     Running     0          12m</span>
<span class="go">gpu-operator   nvidia-container-toolkit-daemonset-rgwqg                          1/1     Running     0          11m</span>
<span class="go">gpu-operator   nvidia-cuda-validator-8whvt                                       0/1     Completed   0          8m50s</span>
<span class="go">gpu-operator   nvidia-dcgm-exporter-pt9q9                                        1/1     Running     0          11m</span>
<span class="go">gpu-operator   nvidia-device-plugin-daemonset-472fc                              1/1     Running     0          11m</span>
<span class="go">gpu-operator   nvidia-device-plugin-validator-29nhc                              0/1     Completed   0          8m34s</span>
<span class="go">gpu-operator   nvidia-driver-daemonset-j9vw6                                     3/3     Running     0          12m</span>
<span class="go">gpu-operator   nvidia-mig-manager-mtjcw                                          1/1     Running     0          7m35s</span>
<span class="go">gpu-operator   nvidia-operator-validator-b8nz2                                   1/1     Running     0          11m</span>
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl<span class="w"> </span>describe<span class="w"> </span>pod<span class="w"> </span>-n<span class="w"> </span>&lt;Operator<span class="w"> </span>Namespace&gt;<span class="w"> </span>nvidia-driver-daemonset-xxxx
<span class="go">&lt;snip&gt;</span>
<span class="go"> Init Containers:</span>
<span class="go">  mofed-validation:</span>
<span class="go">   Container ID:  containerd://a31a8c16ce7596073fef7cb106da94c452fdff111879e7fc3ec58b9cef83856a</span>
<span class="go">   Image:         nvcr.io/nvidia/cloud-native/gpu-operator-validator:v22.9.1</span>
<span class="go">   Image ID:      nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:18c9ea88ae06d479e6657b8a4126a8ee3f4300a40c16ddc29fb7ab3763d46005</span>

<span class="go"> &lt;snip&gt;</span>
<span class="go"> Containers:</span>
<span class="go">  nvidia-driver-ctr:</span>
<span class="go">   Container ID:  containerd://7cf162e4ee4af865c0be2023d61fbbf68c828d396207e7eab2506f9c2a5238a4</span>
<span class="go">   Image:         nvcr.io/nvidia/driver:525.60.13-ubuntu20.04</span>
<span class="go">   Image ID:      nvcr.io/nvidia/driver@sha256:0ee0c585fa720f177734b3295a073f402d75986c1fe018ae68bd73fe9c21b8d8</span>


<span class="go">  &lt;snip&gt;</span>
<span class="go">  nvidia-peermem-ctr:</span>
<span class="go">   Container ID:  containerd://5c71c9f8ccb719728a0503500abecfb5423e8088f474d686ee34b5fe3746c28e</span>
<span class="go">   Image:         nvcr.io/nvidia/driver:525.60.13-ubuntu20.04</span>
<span class="go">   Image ID:      nvcr.io/nvidia/driver@sha256:0ee0c585fa720f177734b3295a073f402d75986c1fe018ae68bd73fe9c21b8d8</span>

<span class="go">  &lt;snip&gt;</span>
<span class="go">  nvidia-fs-ctr:</span>
<span class="go">   Container ID:  containerd://f5c597d59e1cf8747aa20b8c229a6f6edd3ed588b9d24860209ba0cc009c0850</span>
<span class="go">   Image:         nvcr.io/nvidia/cloud-native/nvidia-fs:2.14.13-ubuntu20.04</span>
<span class="go">   Image ID:      nvcr.io/nvidia/cloud-native/nvidia-fs@sha256:109485365f68caeaee1edee0f3f4d722fe5b5d7071811fc81c630c8a840b847b</span>

<span class="go"> &lt;snip&gt;</span>
</pre></div>
</div>
<p>Lastly, verify that NVIDIA kernel modules have been successfully loaded on the worker node:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>lsmod<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>nvidia

<span class="go">nvidia_fs             245760  0</span>
<span class="go">nvidia_peermem         16384  0</span>
<span class="go">nvidia_modeset       1159168  0</span>
<span class="go">nvidia_uvm           1048576  0</span>
<span class="go">nvidia              39059456  115 nvidia_uvm,nvidia_peermem,nvidia_modeset</span>
<span class="go">ib_core               319488  9 rdma_cm,ib_ipoib,nvidia_peermem,iw_cm,ib_umad,rdma_ucm,ib_uverbs,mlx5_ib,ib_cm</span>
<span class="go">drm                   491520  6 drm_kms_helper,drm_vram_helper,nvidia,mgag200,ttm</span>
</pre></div>
</div>
</section>
</section>
<section id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Permalink to this heading"></a></h2>
<p>Refer to the following resources for more information:</p>
<blockquote>
<div><ul class="simple">
<li><p>GPUDirect RDMA: <a class="reference external" href="https://docs.nvidia.com/cuda/gpudirect-rdma/index.html">https://docs.nvidia.com/cuda/gpudirect-rdma/index.html</a></p></li>
<li><p>NVIDIA Network Operator: <a class="reference external" href="https://github.com/Mellanox/network-operator">https://github.com/Mellanox/network-operator</a></p></li>
<li><p>Blog post on deploying the Network Operator: <a class="reference external" href="https://developer.nvidia.com/blog/deploying-gpudirect-rdma-on-egx-stack-with-the-network-operator/">https://developer.nvidia.com/blog/deploying-gpudirect-rdma-on-egx-stack-with-the-network-operator/</a></p></li>
</ul>
</div></blockquote>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="gpu-sharing.html" class="btn btn-neutral float-left" title="Time-Slicing GPUs in Kubernetes" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="gpu-operator-kubevirt.html" class="btn btn-neutral float-right" title="GPU Operator with KubeVirt" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018-2023, NVIDIA Corporation.
      <span class="lastupdated">Last updated on 2023-04-04.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>  

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  .rst-content dl:not(.docutils) dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }
  </style>
  

</body>
</html>